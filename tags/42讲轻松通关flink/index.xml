<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>42讲轻松通关Flink on 枕霞惜友</title>
    <link>https://xiaohao890809.github.io/tags/42%E8%AE%B2%E8%BD%BB%E6%9D%BE%E9%80%9A%E5%85%B3flink/</link>
    <description>Recent content in 42讲轻松通关Flink on 枕霞惜友</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Jul 2020 10:41:48 +0000</lastBuildDate>
    
	<atom:link href="https://xiaohao890809.github.io/tags/42%E8%AE%B2%E8%BD%BB%E6%9D%BE%E9%80%9A%E5%85%B3flink/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>第29讲：项目背景和实时处理系统架构设计</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-30/</link>
      <pubDate>Mon, 20 Jul 2020 10:41:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-30/</guid>
      <description>本从这一课时开始我们进入“Flink 实时统计 PV、UV”项目的学习。本课时先介绍实时统计项目的背景、架构设计和技术选型。
背景 PV（Page View，网站的浏览量）即页面的浏览次数，一般用来衡量网站用户访问的网页数量。我们可以简单地认为，一个用户每次打开一个页面便会记录一次 PV，也就是说，PV 是指页面刷新的次数，刷新一次页面，记录一次 PV。
在互联网项目中，PV 的度量方法是指发起一次请求（Request）从浏览器到网络服务器，网络服务器在接收到请求后会将对应的网页返回给访问者，这个过程就是一次 PV。
UV（Unique Visitor，独立访客次数）是一天内访问某个站点的人数。一天内同一个用户不管有多少次访问网站，那么只记录一次 UV。一般来讲，会通过 IP 或者 Cookie 来进行判断。
如果网站的一个页面有 100 次访问，其中有一些访问者多次访问和点击，那么页面的 UV 一定是小于 100 的。
PV 和 UV 是我们业务中十分常见的场景，对于这种需求一般是通过什么样的技术架构实现呢？
技术选型和整体架构 在互联网常见的后端应用中，技术选型可能多种多样，比如基于 Spring、Dubbo、Spring Cloud 等，但是基本的处理框架大同小异。最常见的数据就是日志数据，如果是 Web 应用，那么我们的日志数据就是用户的访问日志或者用户的点击日志等。
从上图中可以看到有几个关键的处理步骤：
 日志数据如何上报 日志数据的清洗 实时计算程序 前端展示  所以基于以上的业务处理流程，我们常用的实时处理技术选型和架构如下图所示：
在上图的架构图中，涉及几个关键的技术选型和处理步骤，我们下面一一进行讲解。
日志数据埋点 “埋点”是数据采集领域中的一个术语，特别是采集用户行为数据，针对用户的某些事件和行为进行捕捉、处理并发送的过程。
埋点数据是十分重要的数据，也可用来进行：
 流量监控、页面访问的热点分析 构建用户的行为数据，获取用户整个访问的路径和习惯 可以根据用户的访问数据进行偏好分析、个性化推荐 AB 测试 ……  数据埋点的方式多种多样，可以在页面中使用 JavaScript 埋点开发，也可以简单地通过 Nginx 代理服务器进行日志的规范化。
当前大型网站的架构一般都是通过 Nginx 等服务器做网管和代理，Nginx 服务器本身支持日志格式的定制化。
服务端也可以先定义好全局的规范化日志，然后再进行采集，可以更加精细化的定制，满足精细化分析的需求。
我们在本案例中的场景是计算 PV 和 UV 数据，所以数据来源最有可能的是 Nginx 定制化的日志格式，例如：</description>
    </item>
    
    <item>
      <title>第28讲：TopN 热门商品功能实现</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-29/</link>
      <pubDate>Mon, 20 Jul 2020 10:40:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-29/</guid>
      <description>本课时主要讲解 Flink 中的 TopN 功能的设计和实现。
TopN 在我们的业务场景中是十分常见的需求，比如电商场景中求热门商品的销售额、微博每天的热门话题 TopN、贴吧中每天发帖最多的贴吧排名等。TopN 可以进行分组排序，也可以按照需要全局排序，比如若要计算用户下单总金额的 Top 10 时，就需要进行全局排序，然而当我们计算每个城市的 Top10 时就需要将订单按照城市进行分组然后再进行计算。
下面我们就详细讲解 TopN 的设计和实现。
整体设计 我们下面使用订单数据进行讲解，整体的数据流向如下图所示：
订单数据由业务系统产生并发送到 Kafka 中，我们的 Flink 代码会消费 Kafka 中的数据，并进行计算后写入 Redis，然后前端就可以通过读取 Redis 中的数据进行展示了。
订单设计 简化后的订单数据如下，主要包含：下单用户 ID、商品 ID、用户所在城市名称、订单金额和下单时间。
public class OrderDetail { private Long userId; //下单用户id  private Long itemId; //商品id  private String citeName;//用户所在城市  private Double price;//订单金额  private Long timeStamp;//下单时间 } 我们采用 Event-Time 来作为 Flink 程序的时间特征，并且设置 Checkpoint 时间周期为 60 秒。
StreamExecutionEnvironment env = StreamExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>第27讲：Flink Redis Sink 实现</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-28/</link>
      <pubDate>Mon, 20 Jul 2020 10:39:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-28/</guid>
      <description>我们在第 12 课时“Flink 常用的 Source 和 Connector”中提过 Flink 提供了比较丰富的用来连接第三方的连接器，可以在官网中找到 Flink 支持的各种各样的连接器。
此外，Flink 还会基于 Apache Bahir 发布一些 Connector，其中就有我们非常熟悉的 Redis。很多人在 Flink 项目中访问 Redis 的方法都是自己进行实现的，我们也可以使用 Bahir 实现的 Redis 连接器。
事实上，使用 Redis Sink 常用的方法有很多，比如自定义 Sink 函数、依赖开源的 Redis Sink 实现等。
下面我们就分别介绍常用的 Redis Sink 实现。
自定义 Redis Sink  REmote DIctionary Server（Redis）是一个由 Salvatore Sanfilippo 写的 key-value 存储系统。Redis 是一个开源的使用 ANSI C 语言编写、遵守 BSD 协议、支持网络、可基于内存亦可持久化的日志型、Key-Value 数据库，并提供多种语言的 API。
  它通常被称为数据结构服务器，因为值（value）可以是字符串（String）、哈希（Hash）、列表（List）、集合（Sets）和有序集合（Sorted Sets）等类型。
 如果你对 Redis 不熟悉，可以参考官网上的说明。 点击这里下载一个稳定版本的 Redis，我在本地安装的是 2.8.5 版本。使用下面命令进行安装：</description>
    </item>
    
    <item>
      <title>第26讲：Flink 中的聚合函数和累加器的设计和使用</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-27/</link>
      <pubDate>Mon, 20 Jul 2020 10:38:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-27/</guid>
      <description>我们在第 08 课时中提到了 Flink 所支持的窗口和时间类型，并且在第 25 课时中详细讲解了如何设置时间戳提取器和水印发射器。
实际的业务中，我们在使用窗口的过程中一定是基于窗口进行的聚合计算。例如，计算窗口内的 UV、PV 等，那么 Flink 支持哪些基于窗口的聚合函数？累加器又该如何实现呢？
Flink 支持的窗口函数 我们在定义完窗口以后，需要指定窗口上进行的计算。目前 Flink 支持的窗口函数包含 3 种：
 ReduceFunction 增量聚合 AggregateFunction 增量聚合 ProcessWindowFunction 全量聚合  最后还有一种 FlodFunction，但是在 Flink 1.9 版本后已经废弃，推荐使用 AggregateFunction 代替。
下面我们详细讲解以上 3 种窗口聚合函数的定义和使用。
ReduceFunction ReduceFunction 基于两个类型一致的输入进行增量聚合，我们可以自定义 ReduceFunction 来增量聚合窗口内的数据。
可以这样定义自己的 ReduceFunction，覆写 reduce 方法：
DataStream&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt; input = ...; input .keyBy(&amp;lt;key selector&amp;gt;) .window(&amp;lt;window assigner&amp;gt;) .reduce(new ReduceFunction&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt; { public Tuple2&amp;lt;String, Long&amp;gt; reduce(Tuple2&amp;lt;String, Long&amp;gt; v1, Tuple2&amp;lt;String, Long&amp;gt; v2) { return new Tuple2&amp;lt;&amp;gt;(v1.</description>
    </item>
    
    <item>
      <title>第25讲：Flink 中 watermark 的定义和使用</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-26/</link>
      <pubDate>Mon, 20 Jul 2020 10:37:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-26/</guid>
      <description>第 08 课时我们提过窗口和时间的概念，Flink 框架支持事件时间、摄入时间和处理时间三种。Watermark（水印）的出现是用于处理数据从 Source 产生，再到转换和输出，在这个过程中由于网络和反压的原因导致了消息乱序问题。
那么在实际的开发过程中，如何正确地使用 Watermark 呢？
使用 Watermark 必知必会 Watermark 和事件时间 事件时间（Event Time）是数据产生的时间，这个时间一般在数据中自带，由消息的生产者生成。例如，我们的上游是 Kafka 消息，那么每个生成的消息中自带一个时间戳代表该条数据的产生时间，这个时间是固定的，从数据的诞生开始就一直携带。所以，我们在处理消息乱序的情况时，会用 EventTime 和 Watermark 进行配合使用。
我们只需要一行代码，就可以在代码中指定 Flink 系统使用的时间类型为 EventTime：
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); 那么为什么不用处理时间（Processing Time）和摄入时间（Ingestion Time）呢？
处理时间（Processing Time）指的是数据被 Flink 框架处理时机器的系统时间，这个时间本身存在不确定性，比如因为网络延迟等原因。
摄入时间（Ingestion Time）理论上处于事件时间（Event Time）和处理时间（Processing Time）之间，可以用来防止 Flink 内部处理数据发生乱序的情况，但是无法解决数据进入 Flink 之前的乱序行为。
所以我们一般都会用 EventTime、WaterMark 和窗口配合使用来解决消息的乱序和延迟问题。
水印的本质是时间戳 水印的本质是一个一个的时间戳，这个时间戳存在 DataStream 的数据流中，Watermark 的生成方式有两种：
 AssignerWithPeriodicWatermarks 生成周期水印，周期默认的时间是 200ms； AssignerWithPunctuatedWatermarks 按需生成水印。  当 Flink 系统中出现了一个 Watermark T，那么就意味着 EventTime &amp;lt;= T 的数据都已经到达。当 Wartermark T 通过窗口后，后续到来的迟到数据就会被丢弃。
窗口触发和乱序时间 Flink 在用时间 + 窗口 + 水印来解决实际生产中的数据乱序问题，有如下的触发条件：</description>
    </item>
    
    <item>
      <title>第24讲：Flink 消费 Kafka 数据业务开发</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-25/</link>
      <pubDate>Mon, 20 Jul 2020 10:36:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-25/</guid>
      <description>在上一课时中我们提过在实时计算的场景下，绝大多数的数据源都是消息系统，而 Kafka 从众多的消息中间件中脱颖而出，主要是因为高吞吐、低延迟的特点；同时也讲了 Flink 作为生产者像 Kafka 写入数据的方式和代码实现。这一课时我们将从以下几个方面介绍 Flink 消费 Kafka 中的数据方式和源码实现。
Flink 如何消费 Kafka Flink 在和 Kafka 对接的过程中，跟 Kafka 的版本是强相关的。上一课时也提到了，我们在使用 Kafka 连接器时需要引用相对应的 Jar 包依赖，对于某些连接器比如 Kafka 是有版本要求的，一定要去官方网站找到对应的依赖版本。
我们本地的 Kafka 版本是 2.1.0，所以需要对应的类是 FlinkKafkaConsumer。首先需要在 pom.xml 中引入 jar 包依赖：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-connector-kafka_2.11&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.10.0&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 下面将对 Flink 消费 Kafka 数据的方式进行分类讲解。
消费单个 Topic 上一课时我们在本地搭建了 Kafka 环境，并且手动创建了名为 test 的 Topic，然后向名为 test 的 Topic 中写入了数据。
那么现在我们要消费这个 Topic 中的数据，该怎么做呢？
public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>第23讲：Mock Kafka 消息并发送</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-24/</link>
      <pubDate>Mon, 20 Jul 2020 10:35:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-24/</guid>
      <description>本课时主要讲解 Kafka 的一些核心概念，以及模拟消息并发送。
大数据消息中间件的王者——Kafka 在上一课时中提过在实时计算的场景下，我们绝大多数的数据源都是消息系统。所以，一个强大的消息中间件来支撑高达几十万的 QPS，以及海量数据存储就显得极其重要。
Kafka 从众多的消息中间件中脱颖而出，主要是因为高吞吐、低延迟的特点；另外基于 Kafka 的生态越来越完善，各个实时处理框架包括 Flink 在消息处理上都会优先进行支持。在第 14 课时“Flink Exactly-once 实现原理解析”中提到 Flink 和 Kafka 结合实现端到端精确一次语义的原理。
Kafka 从众多的消息中间件中脱颖而出，已经成为大数据生态系统中必不可少的一员，主要的特性包括：
 高吞吐 低延迟 高容错 可靠性 生态丰富  为了接下来更好地理解和使用 Kafka，我们首先来看一下 Kafka 中的核心概念和基本入门。
Kafka 核心概念 Kafka 是一个消息队列，生产者向消息队列中写入数据，消费者从队列中获取数据并进行消费。作为一个企业级的消息中间件，Kafka 会支持庞大的业务，不同的业务会有多个队列，我们用 Topic 来给队列命名，在使用 Kafka 时必须指定 Topic。
我们可以认为一个 Topic 就是一个队列，每个 Topic 又会被分成多个 Partition，这样做是为了横向扩展，提高吞吐量。
Kafka 中每个 Partition 都对应一个 Broker，一个 Broker 可以管理多个 Partition。举个例子，假如 Kafka 的某个 Topic 有 10 个 Partition、2 个 Broker，那么每个 Broker 就会管理 5 个 Partition。我们可以把 Partition 简单理解为一个文件，在接收生产者的数据时，需要将数据动态追加到 Partition 上。</description>
    </item>
    
    <item>
      <title>第22讲：项目背景和整体架构设计</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-23/</link>
      <pubDate>Mon, 20 Jul 2020 10:34:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-23/</guid>
      <description>从这一课时开始我们进入实战课程的学习。本项目是一个模拟实时电商数据大屏，本课时先介绍该项目的背景、架构设计和技术选型。
背景 我们在第 01 课时“Flink 的应用场景和架构模型”中提到过，Flink 应用最广的一个场景便是实时计算大屏。每年的双十一、618 电商大促等，各大公司的实时数据战报和数据大屏是一道亮丽的风景线。
实时大屏对数据有非常高的稳定性和精确性要求，特别是面向公众第三方的数据大屏，同时要求高吞吐、低延迟、极高的稳定性和绝对零误差。随时电商大促的成交记录一次次被刷新，背后是下单、支付、发货高达几万甚至十几万的峰值 QPS。
在面向实际运营的数据大屏中，需要提供高达几十种维度的数据，每秒的数据量高达千万甚至亿级别，这对于我们的实时计算架构提出了相当高的要求。那么我们的大屏背后的实时处理在这种数据量规模如何才能达到高吞吐、低延迟、极高的稳定性和绝对零误差的呢？
技术选型和整体架构 在上图的架构图中，涉及几个关键的技术选型，我们下面一一进行讲解。
业务库 Binlog 同步利器——Canal 我们的实时计算架构一般是基于业务数据进行的，但无论是实时计算大屏还是常规的数据分析报表，都不能影响业务的正常进行，所以这里需要引入消息中间件或增量同步框架 Canal。
我们生产环境中的业务数据绝大多数都是基于 MySQL 的，所以需要一个能够实时监控 MySQL 业务数据变化的工具。Canal 是阿里巴巴开源的数据库 Binlog 日志解析框架，主要用途是基于 MySQL 数据库增量日志解析，提供增量数据订阅和消费。
Canal 的原理也非常简单，它会伪装成一个数据库的从库，来读取 Binlog 并进行解析。关于 Canal 的更多资料，你可以参考这里。
解耦和海量数据支持——Kafka 在实时大屏的技术架构下，我们的数据源绝大多数情况下都是消息。我们需要一个强大的消息中间件来支撑高达几十万 QPS，同时支持海量数据存储。
首先，我们为什么需要引入消息中间件？主要是下面三个目的：
 同步变异步 应用解耦 流量削峰  在我们的架构中，为了和业务数据互相隔离，需要使用消息中间件进行解耦从而互不影响。另外在双十一等大促场景下，交易峰值通常出现在某一个时间段，这个时间段系统压力陡增，数据量暴涨，消息中间件还起到了削峰的作用。
为什么选择 Kafka？
Kafka 是最初由 Linkedin 公司开发，是一个分布式、高吞吐、多分区的消息中间件。Kafka 经过长时间的迭代和实践检验，因为其独特的优点已经成为目前主流的分布式消息引擎，经常被用作企业的消息总线、实时数据存储等。
Kafka 从众多的消息中间件中脱颖而出，主要是因为高吞吐、低延迟的特点；另外基于 Kafka 的生态越来越完善，各个实时处理框架包括 Flink 在消息处理上都会优先进行支持。在第 14 课时“Flink Exactly-once 实现原理解析”中提到了 Flink 和 Kafka 结合实现端到端精确一次语义的原理。
Kafka 作为大数据生态系统中已经必不可少的一员，主要的特性如下所示。
 高吞吐：可以满足每秒百万级别消息的生产和消费，并且可以通过横向扩展，保证数据处理能力可以得到线性扩展。 低延迟：以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间复杂度的访问性能。 高容错：Kafka 允许集群的节点出现失败。 可靠性：消息可以根据策略进行磁盘的持久化，并且读写效率都很高。 生态丰富：Kafka 周边生态极其丰富，与各个实时处理框架结合紧密。  实时计算服务——Flink Flink 在当前的架构中主要承担了消息消费、维表关联、消息发送等，我们在之前的课程中多次提到过 Flink 的优势，主要包括：</description>
    </item>
    
    <item>
      <title>第21讲：Flink 在实时计算平台和实时数据仓库中的作用</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-22/</link>
      <pubDate>Mon, 20 Jul 2020 10:33:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-22/</guid>
      <description>基于 Flink 的实时计算平台 大部分公司随着业务场景的不断丰富，同时在业界经过多年的实践检验，基于 Hadoop 的离线存储体系已经足够成熟。但是离线计算天然时效性不强，一般都是隔天级别的滞后，业务数据随着实践的推移，本身的价值就会逐渐减少。越来越多的场景需要使用实时计算，在这种背景下实时计算平台的需求应运而生。
架构选型 我们在第 03 课时“Flink 的编程模型与其他框架比较”中，提到过 Flink 自身独有的优势。
首先在架构上，Flink 采用了经典的主从模式，DataFlow Graph 与 Storm 形成的拓扑 Topology 结构类似，Flink 程序启动后，会根据用户的代码处理成 Stream Graph，然后优化成为 JobGraph，JobManager 会根据 JobGraph 生成 ExecutionGraph。ExecutionGraph 才是 Flink 真正能执行的数据结构，当很多个 ExecutionGraph 分布在集群中，就会形成一张网状的拓扑结构。
其次在容错方面，针对以前的 Spark Streaming 任务，我们可以配置对应的 checkpoint，也就是保存点（检查点）。当任务出现 failover 的时候，会从 checkpoint 重新加载，使得数据不丢失。但是这个过程会导致原来的数据重复处理，不能做到“只处理一次”的语义。Flink 基于两阶段提交实现了端到端的一次处理语义。
在任务的反压上，Flink 没有使用任何复杂的机制来解决反压问题，Flink 在数据传输过程中使用了分布式阻塞队列。我们知道在一个阻塞队列中，当队列满了以后发送者会被天然阻塞住，这种阻塞功能相当于给这个阻塞队列提供了反压的能力。
这些优势和特性，使得 Flink 在实时计算平台的搭建上占有一席之地。
实时计算平台整体架构 一般的实时计算平台的构成大都是以下几部分构成。
 实时数据收集层  在实际业务中，大量的实时计算都是基于消息系统进行的数据收集和投递，这都离不开强大的消息中间件。目前业界使用最广的是 Kafka，另外一些重要的业务数据还会用到其他消息系统比如 RocketMQ 等。Kafka 因为高吞吐、低延迟的特性，特别适合大数量量、高 QPS 下的业务场景，而 RocketMQ 则在事务消息、一致性上有独特的优势。
 实时计算层  Flink 在计算层同时支持流式及批量分析应用，这就是我们所说的批流一体。Flink 承担了数据的实时采集、实时计算和下游发送的角色。随着 Blink 的开源和一些其他实时产品的开源，支持可视化、SQL 化的开发模式已经越来越普及。</description>
    </item>
    
    <item>
      <title>第20讲：Flink 高级应用之海量数据高效去重</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-21/</link>
      <pubDate>Mon, 20 Jul 2020 10:32:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-21/</guid>
      <description>本课时我们主要讲解 Flink 中的海量数据高效去重。
消除重复数据是我们在实际业务中经常遇到的一类问题。在大数据领域，重复数据的删除有助于减少存储所需要的存储容量。而且在一些特定的业务场景中，重复数据是不可接受的，例如，精确统计网站一天的用户数量、在事实表中统计每天发出的快递包裹数量。在传统的离线计算中，我们可以直接用 SQL 通过 DISTINCT 函数，或者数据量继续增加时会用到类似 MapReduce 的思想。那么在实时计算中，去重计数是一个增量和长期的过程，并且不同的场景下因为效率和精度问题方案也需要变化。
针对上述问题，我们在这里列出几种常见的 Flink 中实时去重方案：
 基于状态后端 基于 HyperLogLog 基于布隆过滤器（BloomFilter） 基于 BitMap 基于外部数据库  下面我们依次讲解上述几种方案的适用场景和实现原理。
基于状态后端 我们在第 09 课时“Flink 状态与容错”中曾经讲过 Flink State 状态后端的概念和原理，其中状态后端的种类之一是 RocksDBStateBackend。它会将正在运行中的状态数据保存在 RocksDB 数据库中，该数据库默认将数据存储在 TaskManager 运行节点的数据目录下。
RocksDB 是一个 K-V 数据库，我们可以利用 MapState 进行去重。这里我们模拟一个场景，计算每个商品 SKU 的访问量。
整体的实现代码如下：
public class MapStateDistinctFunction extends KeyedProcessFunction&amp;lt;String,Tuple2&amp;lt;String,Integer&amp;gt;,Tuple2&amp;lt;String,Integer&amp;gt;&amp;gt; { private transient ValueState&amp;lt;Integer&amp;gt; counts; @Override public void open(Configuration parameters) throws Exception { //我们设置 ValueState 的 TTL 的生命周期为24小时，到期自动清除状态  StateTtlConfig ttlConfig = StateTtlConfig .</description>
    </item>
    
    <item>
      <title>第19讲：Flink 如何做维表关联</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-20/</link>
      <pubDate>Mon, 20 Jul 2020 10:31:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-20/</guid>
      <description>在实际生产中，我们经常会有这样的需求，需要以原始数据流作为基础，然后关联大量的外部表来补充一些属性。例如，我们在订单数据中，希望能得到订单收货人所在省的名称，一般来说订单中会记录一个省的 ID，那么需要根据 ID 去查询外部的维度表补充省名称属性。
在 Flink 流式计算中，我们的一些维度属性一般存储在 MySQL/HBase/Redis 中，这些维表数据存在定时更新，需要我们根据业务进行关联。根据我们业务对维表数据关联的时效性要求，有以下几种解决方案：
 实时查询维表 预加载全量数据 LRU 缓存 其他  上述几种关联外部维表的方式几乎涵盖了我们所有的业务场景，下面针对这几种关联维表的方式和特点一一讲解它们的实现方式和注意事项。
实时查询维表 实时查询维表是指用户在 Flink 算子中直接访问外部数据库，比如用 MySQL 来进行关联，这种方式是同步方式，数据保证是最新的。但是，当我们的流计算数据过大，会对外部系统带来巨大的访问压力，一旦出现比如连接失败、线程池满等情况，由于我们是同步调用，所以一般会导致线程阻塞、Task 等待数据返回，影响整体任务的吞吐量。而且这种方案对外部系统的 QPS 要求较高，在大数据实时计算场景下，QPS 远远高于普通的后台系统，峰值高达十万到几十万，整体作业瓶颈转移到外部系统。
这种方式的核心是，我们可以在 Flink 的 Map 算子中建立访问外部系统的连接。下面以订单数据为例，我们根据下单用户的城市 ID，去关联城市名称，核心代码实现如下：
public class Order { private Integer cityId; private String userName; private String items; public Integer getCityId() { return cityId; } public void setCityId(Integer cityId) { this.cityId = cityId; } public String getUserName() { return userName; } public void setUserName(String userName) { this.</description>
    </item>
    
    <item>
      <title>第18讲：如何进行生产环境作业监控</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-19/</link>
      <pubDate>Mon, 20 Jul 2020 10:30:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-19/</guid>
      <description>本课时主要讲解如何进行生产环境作业监控。
在第 15 课时“如何排查生产环境中的反压问题”中提到过我们应该如何发现任务是否出现反压，Flink 的后台页面是我们发现反压问题的第一选择，其后台页面可以直观、清晰地看到当前作业的运行状态。
在实际生产中，Flink 的后台页面可以方便我们对 Flink JobManager、TaskManager、执行计划、Slot 分配、是否反压等参数进行定位，对单个任务来讲可以方便地进行问题排查。
但是，对于很多大中型企业来讲，我们对进群的作业进行管理时，更多的是关心作业精细化实时运行状态。例如，实时吞吐量的同比环比、整个集群的任务运行概览、集群水位，或者监控利用 Flink 实现的 ETL 框架的运行情况等，这时候就需要设计专门的监控系统来监控集群的任务作业情况。
Flink Metrics 针对上面的情况，我们就用了 Flink 提供的另一个强大的功能：Flink Metrics。
Flink Metrics 是 Flink 实现的一套运行信息收集库，我们不但可以收集 Flink 本身提供的系统指标，比如 CPU、内存、线程使用情况、JVM 垃圾收集情况、网络和 IO 等，还可以通过继承和实现指定的类或者接口打点收集用户自定义的指标。
通过使用 Flink Metrics 我们可以轻松地做到：
 实时采集 Flink 中的 Metrics 信息或者自定义用户需要的指标信息并进行展示； 通过 Flink 提供的 Rest API 收集这些信息，并且接入第三方系统进行展示。  Flink Metrics 分类 Flink 提供了四种类型的监控指标，分别是：Counter、Gauge、Histogram、Meter。
Counter Counter 称为计数器，一般用来统计其中一个指标的总量，比如统计数据的输入、输出总量。
public class MyMapper extends RichMapFunction&amp;lt;String, String&amp;gt; { private transient Counter counter; @Override public void open(Configuration config) { this.</description>
    </item>
    
    <item>
      <title>第17讲：生产环境中的并行度和资源设置</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-18/</link>
      <pubDate>Mon, 20 Jul 2020 10:29:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-18/</guid>
      <description>在使用 Flink 处理生产实际问题时，并行度和资源的配置调优是我们经常要面对的工作之一，如何有效和正确地配置并行度是我们的任务能够高效执行的必要条件。这一课时就来看一下生产环境的并行度和资源配置问题。
Flink 中的计算资源 通常我们说的 Flink 中的计算资源是指具体任务的 Task。首先要理解 Flink 中的计算资源的一些核心概念，比如 Slot、Chain、Task 等，正确理解这些概念有助于开发者了解 Flink 中的计算资源是如何进行隔离和管理的，也有助于我们快速地定位生产中的问题。
Task Slot 我们在第 03 课时“Flink 的编程模型与其他框架比较” 中提到过，在实际生产中，Flink 都是以集群在运行，在运行的过程中包含了两类进程，其中之一就是：TaskManager。
在 Flink 集群中，一个 TaskManger 就是一个 JVM 进程，并且会用独立的线程来执行 task，为了控制一个 TaskManger 能接受多少个 task，Flink 提出了 Task Slot 的概念。
我们可以简单地把 Task Slot 理解为 TaskManager 的计算资源子集。假如一个 TaskManager 拥有 5 个 Slot，那么该 TaskManager 的计算资源会被平均分为 5 份，不同的 task 在不同的 Slot 中执行，避免资源竞争。但需要注意的是，Slot 仅仅用来做内存的隔离，对 CPU 不起作用。那么运行在同一个 JVM 的 task 可以共享 TCP 连接，以减少网络传输，在一定程度上提高了程序的运行效率，降低了资源消耗。
Slot 共享 默认情况下，Flink 还允许同一个 Job 的子任务共享 Slot。因为在一个 Flink 任务中，有很多的算子，这些算子的计算压力各不相同，比如简单的 map 和 filter 算子所需要的资源不多，但是有些算子比如 window、group by 则需要更多的计算资源才能满足计算所需。这时候那些资源需求大的算子就可以共用其他的 Slot，提高整个集群的资源利用率。</description>
    </item>
    
    <item>
      <title>第16讲：如何处理生产环境中的数据倾斜问题</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-17/</link>
      <pubDate>Mon, 20 Jul 2020 10:28:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-17/</guid>
      <description>这一课时我们主要讲解如何处理生产环境中的数据倾斜问题。
无论是对于 Flink、Spark 这样的实时计算框架还是 Hive 等离线计算框架，数据量从来都不是问题，真正引起问题导致严重后果的是数据倾斜。所谓数据倾斜，是指在大规模并行处理的数据中，其中某个运行节点处理的数据远远超过其他部分，这会导致该节点压力极大，最终出现运行失败从而导致整个任务的失败。
我们在这一课时中将分析出现数据倾斜的原因，Flink 任务中最容易出现数据倾斜的几个算子并且给出解决方案。
数据倾斜背景和危害 数据倾斜产生的原因和危害和解决方案有哪些呢？我们一一来看。
数据倾斜原理 目前我们所知道的大数据处理框架，比如 Flink、Spark、Hadoop 等之所以能处理高达千亿的数据，是因为这些框架都利用了分布式计算的思想，集群中多个计算节点并行，使得数据处理能力能得到线性扩展。
我们在第 03 课时“Flink 的编程模型与其他框架比较”中曾经讲过，在实际生产中 Flink 都是以集群的形式在运行，在运行的过程中包含了两类进程。其中 TaskManager 实际负责执行计算的 Worker，在其上执行 Flink Job 的一组 Task，Task 则是我们执行具体代码逻辑的容器。理论上只要我们的任务 Task 足够多就可以对足够大的数据量进行处理。
但是实际上大数据量经常出现，一个 Flink 作业包含 200 个 Task 节点，其中有 199 个节点可以在很短的时间内完成计算。但是有一个节点执行时间远超其他结果，并且随着数据量的持续增加，导致该计算节点挂掉，从而整个任务失败重启。我们可以在 Flink 的管理界面中看到任务的某一个 Task 数据量远超其他节点。
数据倾斜原因和解决方案 Flink 任务出现数据倾斜的直观表现是任务节点频繁出现反压，但是增加并行度后并不能解决问题；部分节点出现 OOM 异常，是因为大量的数据集中在某个节点上，导致该节点内存被爆，任务失败重启。
产生数据倾斜的原因主要有 2 个方面：
 业务上有严重的数据热点，比如滴滴打车的订单数据中北京、上海等几个城市的订单量远远超过其他地区； 技术上大量使用了 KeyBy、GroupBy 等操作，错误的使用了分组 Key，人为产生数据热点。  因此解决问题的思路也很清晰：
 业务上要尽量避免热点 key 的设计，例如我们可以把北京、上海等热点城市分成不同的区域，并进行单独处理； 技术上出现热点时，要调整方案打散原来的 key，避免直接聚合；此外 Flink 还提供了大量的功能可以避免数据倾斜。  那么我们就从典型的场景入手，看看在 Flink 任务中出现数据倾斜的主要场景和解决方案。</description>
    </item>
    
    <item>
      <title>第15讲：如何排查生产环境中的反压问题</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-16/</link>
      <pubDate>Mon, 20 Jul 2020 10:27:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-16/</guid>
      <description>这一课时我们主要讲解生产环境中 Flink 任务经常会遇到的一个问题，即如何处理好反压问题将直接关系到任务的资源使用和稳定运行。
反压问题是流式计算系统中经常碰到的一个问题，如果你的任务出现反压节点，那么就意味着任务数据的消费速度小于数据的生产速度，需要对生产数据的速度进行控制。通常情况下，反压经常出现在促销、热门活动等场景，它们有一个共同的特点：短时间内流量陡增造成数据的堆积或者消费速度变慢。
不同框架的反压对比 目前主流的大数据实时处理系统都对反压问题进行了专门的处理，希望框架自身能检测到被阻塞的算子，然后降低数据生产者的发送速率。我们所熟悉的 Storm、Spark Streaming、Flink 的实现稍微有所不同。
Storm Storm 从 1.0 版本以后引入了全新的反压机制，Storm 会主动监控工作节点。当工作节点接收数据超过一定的水位值时，那么反压信息会被发送到 ZooKeeper 上，然后 ZooKeeper 通知所有的工作节点进入反压状态，最后数据的生产源头会降低数据的发送速度。
Spark Streaming Spark Streaming 在原有的架构基础上专门设计了一个 RateController 组件，该组件利用经典的 PID 算法。向系统反馈当前系统处理数据的几个重要属性：消息数量、调度时间、处理时间、调度时间等，然后根据这些参数计算出一个速率，该速率则是当前系统处理数据的最大能力，Spark Streaming 会根据计算结果对生产者进行限速。
Flink Flink 的反压设计利用了网络传输和动态限流。在 Flink 的设计哲学中，纯流式计算给 Flink 进行反压设计提供了天然的优势。
我们在以前的课程中讲解过，Flink 任务的组成由基本的“流”和“算子”构成，那么“流”中的数据在“算子”间进行计算和转换时，会被放入分布式的阻塞队列中。当消费者的阻塞队列满时，则会降低生产者的数据生产速度。
如上图所示，我们看一下 Flink 进行逐级反压的过程。当 Task C 的数据处理速度发生异常时，Receive Buffer 会呈现出队列满的情况，Task B 的 Send Buffer 会感知到这一点，然后把数据发送速度降低。以此类推，整个反压会一直从下向上传递到 Source 端；反之，当下游的 Task 处理能力有提升后，会在此反馈到 Source Task，数据的发送和读取速率都会升高，提高了整个任务的处理能力。
反压的定位 当你的任务出现反压时，如果你的上游是类似 Kafka 的消息系统，很明显的表现就是消费速度变慢，Kafka 消息出现堆积。
如果你的业务对数据延迟要求并不高，那么反压其实并没有很大的影响。但是对于规模很大的集群中的大作业，反压会造成严重的“并发症”。首先任务状态会变得很大，因为数据大规模堆积在系统中，这些暂时不被处理的数据同样会被放到“状态”中。另外，Flink 会因为数据堆积和处理速度变慢导致 checkpoint 超时，而 checkpoint 是 Flink 保证数据一致性的关键所在，最终会导致数据的不一致发生。</description>
    </item>
    
    <item>
      <title>第14讲：Flink Exactly-once 实现原理解析</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-15/</link>
      <pubDate>Mon, 20 Jul 2020 10:26:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-15/</guid>
      <description>这一课时我们将讲解 Flink “精确一次”的语义实现原理，同时这也是面试的必考点。
Flink 的“精确一次”处理语义是，Flink 提供了一个强大的语义保证，也就是说在任何情况下都能保证数据对应用产生的效果只有一次，不会多也不会少。
那么 Flink 是如何实现“端到端的精确一次处理”语义的呢？
背景 通常情况下，流式计算系统都会为用户提供指定数据处理的可靠模式功能，用来表明在实际生产运行中会对数据处理做哪些保障。一般来说，流处理引擎通常为用户的应用程序提供三种数据处理语义：最多一次、至少一次和精确一次。
 最多一次（At-most-Once）：这种语义理解起来很简单，用户的数据只会被处理一次，不管成功还是失败，不会重试也不会重发。 至少一次（At-least-Once）：这种语义下，系统会保证数据或事件至少被处理一次。如果中间发生错误或者丢失，那么会从源头重新发送一条然后进入处理系统，所以同一个事件或者消息会被处理多次。 精确一次（Exactly-Once）：表示每一条数据只会被精确地处理一次，不多也不少。  Exactly-Once 是 Flink、Spark 等流处理系统的核心特性之一，这种语义会保证每一条消息只被流处理系统处理一次。“精确一次” 语义是 Flink 1.4.0 版本引入的一个重要特性，而且，Flink 号称支持“端到端的精确一次”语义。
在这里我们解释一下“端到端（End to End）的精确一次”，它指的是 Flink 应用从 Source 端开始到 Sink 端结束，数据必须经过的起始点和结束点。Flink 自身是无法保证外部系统“精确一次”语义的，所以 Flink 若要实现所谓“端到端（End to End）的精确一次”的要求，那么外部系统必须支持“精确一次”语义；然后借助 Flink 提供的分布式快照和两阶段提交才能实现。
分布式快照机制 我们在之前的课程中讲解过 Flink 的容错机制，Flink 提供了失败恢复的容错机制，而这个容错机制的核心就是持续创建分布式数据流的快照来实现。
同 Spark 相比，Spark 仅仅是针对 Driver 的故障恢复 Checkpoint。而 Flink 的快照可以到算子级别，并且对全局数据也可以做快照。Flink 的分布式快照受到 Chandy-Lamport 分布式快照算法启发，同时进行了量身定做，有兴趣的同学可以搜一下。
Barrier Flink 分布式快照的核心元素之一是 Barrier（数据栅栏），我们也可以把 Barrier 简单地理解成一个标记，该标记是严格有序的，并且随着数据流往下流动。每个 Barrier 都带有自己的 ID，Barrier 极其轻量，并不会干扰正常的数据处理。
如上图所示，假如我们有一个从左向右流动的数据流，Flink 会依次生成 snapshot 1、 snapshot 2、snapshot 3……Flink 中有一个专门的“协调者”负责收集每个 snapshot 的位置信息，这个“协调者”也是高可用的。</description>
    </item>
    
    <item>
      <title>第13讲：如何实现生产环境中的 Flink 高可用配置</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-14/</link>
      <pubDate>Mon, 20 Jul 2020 10:25:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-14/</guid>
      <description>我们在第 06 课时“Flink 集群安装部署和 HA 配置”中讲解了 Flink 的几种常见部署模式，并且简单地介绍了 HA 配置。
概述 事实上，集群的高可用（High Availablility，以下简称 HA）配置是大数据领域经典的一个问题。
 通常 HA 用来描述一个系统经过专门的设计，从而减少停工时间，而保持其服务的高度可用性。
 我们在第 03 课时“Flink 的编程模型与其他框架比较”中也提到过 Flink 集群中的角色，其中 JobManager 扮演的是集群管理者的角色，负责调度任务、协调 Checkpoints、协调故障恢复、收集 Job 的状态信息，并管理 Flink 集群中的从节点 TaskManager。
在默认的情况下，我们的每个集群都只有一个 JobManager 实例，假如这个 JobManager 崩溃了，那么将会导致我们的作业运行失败，并且无法提交新的任务。
因此，在生产环境中我们的集群应该如何配置以达到高可用的目的呢？针对不同模式进行部署的集群，我们需要不同的配置。
源码分析 Flink 中的 JobManager、WebServer 等组件都需要高可用保障，并且 Flink 还需要进行 Checkpoint 元数据的持久化操作。与 Flink HA 相关的类图如下图所示，我们跟随源码简单看一下 Flink HA 的实现。
HighAvailabilityMode 类中定义了三种高可用性模式枚举，如下图所示：
 NONE：非 HA 模式 ZOOKEEPER：基于 ZK 实现 HA FACTORY_CLASS：自定义 HA 工厂类，该类需要实现 HighAvailabilityServicesFactory 接口  具体的高可用实例对象创建则在 HighAvailabilityServicesUtils 类中有体现，如下图所示：</description>
    </item>
    
    <item>
      <title>第12讲：Flink 常用的 Source 和 Connector</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-13/</link>
      <pubDate>Mon, 20 Jul 2020 10:24:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-13/</guid>
      <description>本课时我们主要介绍 Flink 中支持的 Source 和常用的 Connector。
Flink 作为实时计算领域强大的计算能力，以及与其他系统进行对接的能力都非常强大。Flink 自身实现了多种 Source 和 Connector 方法，并且还提供了多种与第三方系统进行对接的 Connector。
我们可以把这些 Source、Connector 分成以下几个大类。
预定义和自定义 Source 在前面的第 04 课时“Flink 常用的 DataSet 和 DataStream API”中提到过几种 Flink 已经实现的新建 DataStream 方法。
基于文件 我们在本地环境进行测试时可以方便地从本地文件读取数据：
readTextFile(path) readFile(fileInputFormat, path) ... 可以直接在 ExecutionEnvironment 和 StreamExecutionEnvironment 类中找到 Flink 支持的读取本地文件的方法，如下图所示：
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // read text file from local files system DataSet&amp;lt;String&amp;gt; localLines = env.readTextFile(&amp;#34;file:///path/to/my/textfile&amp;#34;); // read text file from an HDFS running at nnHost:nnPort DataSet&amp;lt;String&amp;gt; hdfsLines = env.</description>
    </item>
    
    <item>
      <title>第11讲：Flink CEP 复杂事件处理</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-12/</link>
      <pubDate>Mon, 20 Jul 2020 10:23:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-12/</guid>
      <description>你好，欢迎来到第 11 课时，这一课时将介绍 Flink 中提供的一个很重要的功能：复杂事件处理 CEP。
背景 Complex Event Processing（CEP）是 Flink 提供的一个非常亮眼的功能，关于 CEP 的解释我们引用维基百科中的一段话：
 CEP, is event processing that combines data from multiple sources to infer events or patterns that suggest more complicated circumstances. The goal of complex event processing is to identify meaningful events (such as opportunities or threats) and respond to them as quickly as possible.
 在我们的实际生产中，随着数据的实时性要求越来越高，实时数据的量也在不断膨胀，在某些业务场景中需要根据连续的实时数据，发现其中有价值的那些事件。
说到底，Flink 的 CEP 到底解决了什么样的问题呢？
比如，我们需要在大量的订单交易中发现那些虚假交易，在网站的访问日志中寻找那些使用脚本或者工具“爆破”登录的用户，或者在快递运输中发现那些滞留很久没有签收的包裹等。
如果你对 CEP 的理论基础非常感兴趣，推荐一篇论文“Efﬁcient Pattern Matching over Event Streams”。</description>
    </item>
    
    <item>
      <title>第10讲：Flink Side OutPut 分流</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-11/</link>
      <pubDate>Mon, 20 Jul 2020 10:22:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-11/</guid>
      <description>这一课时将介绍 Flink 中提供的一个很重要的功能：旁路分流器。
分流场景 我们在生产实践中经常会遇到这样的场景，需把输入源按照需要进行拆分，比如我期望把订单流按照金额大小进行拆分，或者把用户访问日志按照访问者的地理位置进行拆分等。面对这样的需求该如何操作呢？
分流的方法 通常来说针对不同的场景，有以下三种办法进行流的拆分。
Filter 分流 Filter 方法我们在第 04 课时中（Flink 常用的 DataSet 和 DataStream API）讲过，这个算子用来根据用户输入的条件进行过滤，每个元素都会被 filter() 函数处理，如果 filter() 函数返回 true 则保留，否则丢弃。那么用在分流的场景，我们可以做多次 filter，把我们需要的不同数据生成不同的流。
来看下面的例子：
public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //获取数据源  List data = new ArrayList&amp;lt;Tuple3&amp;lt;Integer,Integer,Integer&amp;gt;&amp;gt;(); data.add(new Tuple3&amp;lt;&amp;gt;(0,1,0)); data.add(new Tuple3&amp;lt;&amp;gt;(0,1,1)); data.add(new Tuple3&amp;lt;&amp;gt;(0,2,2)); data.add(new Tuple3&amp;lt;&amp;gt;(0,1,3)); data.add(new Tuple3&amp;lt;&amp;gt;(1,2,5)); data.add(new Tuple3&amp;lt;&amp;gt;(1,2,9)); data.add(new Tuple3&amp;lt;&amp;gt;(1,2,11)); data.add(new Tuple3&amp;lt;&amp;gt;(1,2,13)); DataStreamSource&amp;lt;Tuple3&amp;lt;Integer,Integer,Integer&amp;gt;&amp;gt; items = env.fromCollection(data); SingleOutputStreamOperator&amp;lt;Tuple3&amp;lt;Integer, Integer, Integer&amp;gt;&amp;gt; zeroStream = items.</description>
    </item>
    
    <item>
      <title>第09讲：Flink 状态与容错</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-10/</link>
      <pubDate>Mon, 20 Jul 2020 10:21:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-10/</guid>
      <description>这一课时我们主要讲解 Flink 的状态和容错。
在 Flink 的框架中，进行有状态的计算是 Flink 最重要的特性之一。所谓的状态，其实指的是 Flink 程序的中间计算结果。Flink 支持了不同类型的状态，并且针对状态的持久化还提供了专门的机制和状态管理器。
状态 我们在 Flink 的官方博客中找到这样一段话，可以认为这是对状态的定义：
 When working with state, it might also be useful to read about Flink’s state backends. Flink provides different state backends that specify how and where state is stored. State can be located on Java’s heap or off-heap. Depending on your state backend, Flink can also manage the state for the application, meaning Flink deals with the memory management (possibly spilling to disk if necessary) to allow applications to hold very large state.</description>
    </item>
    
    <item>
      <title>第08讲：Flink 窗口、时间和水印</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-09/</link>
      <pubDate>Mon, 20 Jul 2020 10:17:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-09/</guid>
      <description>本课时主要介绍 Flink 中的时间和水印。
我们在之前的课时中反复提到过窗口和时间的概念，Flink 框架中支持事件时间、摄入时间和处理时间三种。而当我们在流式计算环境中数据从 Source 产生，再到转换和输出，这个过程由于网络和反压的原因会导致消息乱序。因此，需要有一个机制来解决这个问题，这个特别的机制就是“水印”。
Flink 的窗口和时间 我们在第 05 课时中讲解过 Flink 窗口的实现，根据窗口数据划分的不同，目前 Flink 支持如下 3 种：
 滚动窗口，窗口数据有固定的大小，窗口中的数据不会叠加； 滑动窗口，窗口数据有固定的大小，并且有生成间隔； 会话窗口，窗口数据没有固定的大小，根据用户传入的参数进行划分，窗口数据无叠加。  Flink 中的时间分为三种：
 事件时间（Event Time），即事件实际发生的时间； 摄入时间（Ingestion Time），事件进入流处理框架的时间； 处理时间（Processing Time），事件被处理的时间。  下面的图详细说明了这三种时间的区别和联系：
事件时间（Event Time） 事件时间（Event Time）指的是数据产生的时间，这个时间一般由数据生产方自身携带，比如 Kafka 消息，每个生成的消息中自带一个时间戳代表每条数据的产生时间。Event Time 从消息的产生就诞生了，不会改变，也是我们使用最频繁的时间。
利用 Event Time 需要指定如何生成事件时间的“水印”，并且一般和窗口配合使用，具体会在下面的“水印”内容中详细讲解。
我们可以在代码中指定 Flink 系统使用的时间类型为 EventTime：
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //设置时间属性为 EventTime env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStream&amp;lt;MyEvent&amp;gt; stream = env.addSource(new FlinkKafkaConsumer09&amp;lt;MyEvent&amp;gt;(topic, schema, props)); stream .keyBy( (event) -&amp;gt; event.getUser() ) .</description>
    </item>
    
    <item>
      <title>第07讲：Flink 常见核心概念分析</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-08/</link>
      <pubDate>Mon, 20 Jul 2020 10:16:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-08/</guid>
      <description>在 Flink 这个框架中，有很多独有的概念，比如分布式缓存、重启策略、并行度等，这些概念是我们在进行任务开发和调优时必须了解的，这一课时我将会从原理和应用场景分别介绍这些概念。
分布式缓存 熟悉 Hadoop 的你应该知道，分布式缓存最初的思想诞生于 Hadoop 框架，Hadoop 会将一些数据或者文件缓存在 HDFS 上，在分布式环境中让所有的计算节点调用同一个配置文件。在 Flink 中，Flink 框架开发者们同样将这个特性进行了实现。
Flink 提供的分布式缓存类型 Hadoop，目的是为了在分布式环境中让每一个 TaskManager 节点保存一份相同的数据或者文件，当前计算节点的 task 就像读取本地文件一样拉取这些配置。
分布式缓存在我们实际生产环境中最广泛的一个应用，就是在进行表与表 Join 操作时，如果一个表很大，另一个表很小，那么我们就可以把较小的表进行缓存，在每个 TaskManager 都保存一份，然后进行 Join 操作。
那么我们应该怎样使用 Flink 的分布式缓存呢？举例如下：
public static void main(String[] args) throws Exception { final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.registerCachedFile(&amp;#34;/Users/wangzhiwu/WorkSpace/quickstart/distributedcache.txt&amp;#34;, &amp;#34;distributedCache&amp;#34;); //1：注册一个文件,可以使用hdfs上的文件 也可以是本地文件进行测试  DataSource&amp;lt;String&amp;gt; data = env.fromElements(&amp;#34;Linea&amp;#34;, &amp;#34;Lineb&amp;#34;, &amp;#34;Linec&amp;#34;, &amp;#34;Lined&amp;#34;); DataSet&amp;lt;String&amp;gt; result = data.map(new RichMapFunction&amp;lt;String, String&amp;gt;() { private ArrayList&amp;lt;String&amp;gt; dataList = new ArrayList&amp;lt;String&amp;gt;(); @Override public void open(Configuration parameters) throws Exception { super.</description>
    </item>
    
    <item>
      <title>第06讲：Flink 集群安装部署和 HA 配置</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-07/</link>
      <pubDate>Mon, 20 Jul 2020 10:15:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-07/</guid>
      <description>我们在这一课时将讲解 Flink 常见的部署模式：本地模式、Standalone 模式和 Flink On Yarn 模式，然后分别讲解三种模式的使用场景和部署中常见的问题，最后将讲解在生产环境中 Flink 集群的高可用配置。
Flink 常见的部署模式 环境准备 在绝大多数情况下，我们的 Flink 都是运行在 Unix 环境中的，推荐在 Mac OS 或者 Linux 环境下运行 Flink。如果是集群模式，那么可以在自己电脑上安装虚拟机，保证有一个 master 节点和两个 slave 节点。
同时，要注意在所有的机器上都应该安装 JDK 和 SSH。JDK 是我们运行 JVM 语言程序必须的，而 SSH 是为了在服务器之间进行跳转和执行命令所必须的。关于服务器之间通过 SSH 配置公钥登录，你可以直接搜索安装和配置方法，我们不做过度展开。
Flink 的安装包可以在这里下载。需要注意的是，如果你要和 Hadoop 进行集成，那么我们需要使用到对应的 Hadoop 依赖，下面将会详细讲解。
Local 模式 Local 模式是 Flink 提供的最简单部署模式，一般用来本地测试和演示使用。
我们在这里下载 Apache Flink 1.10.0 for Scala 2.11 版本进行演示，该版本对应 Scala 2.11 版本。
将压缩包下载到本地，并且直接进行解压，使用 Flink 默认的端口配置，直接运行脚本启动：
➜ [SoftWare]# tar -zxvf flink-1.10.0-bin-scala_2.11.tgz 上图则为解压完成后的目录情况。</description>
    </item>
    
    <item>
      <title>第05讲：Flink SQL &amp; Table 编程和案例</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-06/</link>
      <pubDate>Mon, 20 Jul 2020 10:14:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-06/</guid>
      <description>我们在第 02 课时中使用 Flink Table &amp;amp; SQL 的 API 实现了最简单的 WordCount 程序。在这一课时中，将分别从 Flink Table &amp;amp; SQL 的背景和编程模型、常见的 API、算子和内置函数等对 Flink Table &amp;amp; SQL 做一个详细的讲解和概括，最后模拟了一个实际业务场景使用 Flink Table &amp;amp; SQL 开发。
Flink Table &amp;amp; SQL 概述 背景 我们在前面的课时中讲过 Flink 的分层模型，Flink 自身提供了不同级别的抽象来支持我们开发流式或者批量处理程序，下图描述了 Flink 支持的 4 种不同级别的抽象。
Table API 和 SQL 处于最顶端，是 Flink 提供的高级 API 操作。Flink SQL 是 Flink 实时计算为简化计算模型，降低用户使用实时计算门槛而设计的一套符合标准 SQL 语义的开发语言。
我们在第 04 课时中提到过，Flink 在编程模型上提供了 DataStream 和 DataSet 两套 API，并没有做到事实上的批流统一，因为用户和开发者还是开发了两套代码。正是因为 Flink Table &amp;amp; SQL 的加入，可以说 Flink 在某种程度上做到了事实上的批流一体。</description>
    </item>
    
    <item>
      <title>第04讲：Flink 常用的 DataSet 和 DataStream API</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-05/</link>
      <pubDate>Mon, 20 Jul 2020 10:13:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-05/</guid>
      <description>本课时我们主要介绍 Flink 的 DataSet 和 DataStream 的 API，并模拟了实时计算的场景，详细讲解了 DataStream 常用的 API 的使用。
说好的流批一体呢
现状 在前面的课程中，曾经提到过，Flink 很重要的一个特点是“流批一体”，然而事实上 Flink 并没有完全做到所谓的“流批一体”，即编写一套代码，可以同时支持流式计算场景和批量计算的场景。目前截止 1.10 版本依然采用了 DataSet 和 DataStream 两套 API 来适配不同的应用场景。
DateSet 和 DataStream 的区别和联系 在官网或者其他网站上，都可以找到目前 Flink 支持两套 API 和一些应用场景，但大都缺少了“为什么”这样的思考。
Apache Flink 在诞生之初的设计哲学是：用同一个引擎支持多种形式的计算，包括批处理、流处理和机器学习等。尤其是在流式计算方面，Flink 实现了计算引擎级别的流批一体。那么对于普通开发者而言，如果使用原生的 Flink ，直接的感受还是要编写两套代码。
整体架构如下图所示：
在 Flink 的源代码中，我们可以在 flink-java 这个模块中找到所有关于 DataSet 的核心类，DataStream 的核心实现类则在 flink-streaming-java 这个模块。
在上述两张图中，我们分别打开 DataSet 和 DataStream 这两个类，可以发现，二者支持的 API 都非常丰富且十分类似，比如常用的 map、filter、join 等常见的 transformation 函数。
我们在前面的课时中讲过 Flink 的编程模型，对于 DataSet 而言，Source 部分来源于文件、表或者 Java 集合；而 DataStream 的 Source 部分则一般是消息中间件比如 Kafka 等。</description>
    </item>
    
    <item>
      <title>第03讲：Flink 的编程模型与其他框架比较</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-04/</link>
      <pubDate>Mon, 20 Jul 2020 10:12:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-04/</guid>
      <description>本课时我们主要介绍 Flink 的编程模型与其他框架比较。
本课时的内容主要介绍基于 Flink 的编程模型，包括 Flink 程序的基础处理语义和基本构成模块，并且和 Spark、Storm 进行比较，Flink 作为最新的分布式大数据处理引擎具有哪些独特的优势呢？
Flink 的核心语义和架构模型 我们在讲解 Flink 程序的编程模型之前，先来了解一下 Flink 中的 Streams、State、Time 等核心概念和基础语义，以及 Flink 提供的不同层级的 API。
Flink 核心概念  Streams（流），流分为有界流和无界流。有界流指的是有固定大小，不随时间增加而增长的数据，比如我们保存在 Hive 中的一个表；而无界流指的是数据随着时间增加而增长，计算状态持续进行，比如我们消费 Kafka 中的消息，消息持续不断，那么计算也会持续进行不会结束。 State（状态），所谓的状态指的是在进行流式计算过程中的信息。一般用作容错恢复和持久化，流式计算在本质上是增量计算，也就是说需要不断地查询过去的状态。状态在 Flink 中有十分重要的作用，例如为了确保 Exactly-once 语义需要将数据写到状态中；此外，状态的持久化存储也是集群出现 Fail-over 的情况下自动重启的前提条件。 Time（时间），Flink 支持了 Event time、Ingestion time、Processing time 等多种时间语义，时间是我们在进行 Flink 程序开发时判断业务状态是否滞后和延迟的重要依据。 API：Flink 自身提供了不同级别的抽象来支持我们开发流式或者批量处理程序，由上而下可分为 SQL / Table API、DataStream API、ProcessFunction 三层，开发者可以根据需要选择不同层级的 API 进行开发。  Flink 编程模型和流式处理 我们在第 01 课中提到过，Flink 程序的基础构建模块是流（Streams）和转换（Transformations），每一个数据流起始于一个或多个 Source，并终止于一个或多个 Sink。数据流类似于有向无环图（DAG）。
在分布式运行环境中，Flink 提出了算子链的概念，Flink 将多个算子放在一个任务中，由同一个线程执行，减少线程之间的切换、消息的序列化/反序列化、数据在缓冲区的交换，减少延迟的同时提高整体的吞吐量。
官网中给出的例子如下，在并行环境下，Flink 将多个 operator 的子任务链接在一起形成了一个task，每个 task 都有一个独立的线程执行。</description>
    </item>
    
    <item>
      <title>第02讲：Flink 入门程序 WordCount 和 SQL 实现</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-03/</link>
      <pubDate>Mon, 20 Jul 2020 10:11:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-03/</guid>
      <description>本课时我们主要介绍 Flink 的入门程序以及 SQL 形式的实现。
上一课时已经讲解了 Flink 的常用应用场景和架构模型设计，这一课时我们将会从一个最简单的 WordCount 案例作为切入点，并且同时使用 SQL 方式进行实现，为后面的实战课程打好基础。
我们首先会从环境搭建入手，介绍如何搭建本地调试环境的脚手架；然后分别从DataSet（批处理）和 DataStream（流处理）两种方式如何进行单词计数开发；最后介绍 Flink Table 和 SQL 的使用。
Flink 开发环境 通常来讲，任何一门大数据框架在实际生产环境中都是以集群的形式运行，而我们调试代码大多数会在本地搭建一个模板工程，Flink 也不例外。
Flink 一个以 Java 及 Scala 作为开发语言的开源大数据项目，通常我们推荐使用 Java 来作为开发语言，Maven 作为编译和包管理工具进行项目构建和编译。对于大多数开发者而言，JDK、Maven 和 Git 这三个开发工具是必不可少的。
关于 JDK、Maven 和 Git 的安装建议如下表所示：
工程创建 一般来说，我们在通过 IDE 创建工程，可以自己新建工程，添加 Maven 依赖，或者直接用 mvn 命令创建应用：
mvn archetype:generate \ -DarchetypeGroupId=org.apache.flink \ -DarchetypeArtifactId=flink-quickstart-java \ -DarchetypeVersion=1.10.0 通过指定 Maven 工程的三要素，即 GroupId、ArtifactId、Version 来创建一个新的工程。同时 Flink 给我提供了更为方便的创建 Flink 工程的方法：
curl https://flink.apache.org/q/quickstart.sh | bash -s 1.</description>
    </item>
    
    <item>
      <title>开篇词：实时计算领域最锋利的武器 Flink</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-01/</link>
      <pubDate>Mon, 20 Jul 2020 10:10:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-01/</guid>
      <description>你好，欢迎来到 Flink 专栏，我是王知无，目前在某一线互联网公司从事数据平台架构和研发工作多年，算是整个大数据开发领域的老兵了。
我最早从 Release 版本开始关注 Flink，可以说是国内第一批钻研 Flink 的开发者，后来基于 Flink 开发过实时计算业务应用、实时数据仓库以及监控报警系统，在这个过程中积累了大量宝贵的生产实践经验。
面试是开发者永远绕不过去的坎 由于项目需要，我在工作中面试过很多 Flink 开发工程师，并且发现了一些普遍性问题，比如：
 对常用的 Flink 核心概念和原理掌握不牢，一旦参与到实战业务中必将寸步难行，一面直接被刷掉； 能够通过简历筛选的人基本都有实时流计算开发的经验，可以从容应对典型场景下的问题，但对于非典型但常见的业务场景问题就会支支吾吾，无从应答； 有些面试者自称参与过实时计算平台的架构设计、开发、发布和运维等全流程的工作，但稍微追问就会发现他在项目中的参与度其实很低，暴露出在上一家公司只是开发团队的一个“小透明”； 我们现在招聘其实是偏向招有相关经验并熟悉底层原理的人，曾经有面试者能熟练回答在项目中是如何应用 Flink 的，但是不知道底层源码级别的实现。  上面列举的这四个问题看似不同，但本质上都是在全方位考察你对技术原理的理解深度，以及在实际工作中解决问题的能力。
当然还有一类人，他们具备深厚的理论基础和丰富的实战经验，却往往因为缺乏面试经验，依然屡屡与大厂擦肩而过。很多开发者在学习完一个框架后，可以熟练地开发和排查问题，但是在面试的过程中却无法逻辑清晰地表述自己的观点。想象一下，当你在面试中被问到以下三个问题：
 Flink 如何实现 Exactly-once 语义？ Flink 时间类型的分类和各自的实现原理？ Flink 如何处理数据乱序和延迟？  你将如何作答？面试官满意的答案究竟长什么样？上述问题的答案，你都可以在这个专栏中找到。
想进大厂，必须掌握 Flink 技术 随着大数据时代的发展、海量数据的实时处理和多样业务的数据计算需求激增，传统的批处理方式和早期的流式处理框架也有自身的局限性，难以在延迟性、吞吐量、容错能力，以及使用便捷性等方面满足业务日益苛刻的要求。在这种形势下，Flink 以其独特的天然流式计算特性和更为先进的架构设计，极大地改善了以前的流式处理框架所存在的问题。
越来越多的国内公司开始用 Flink 来做实时数据处理，其中阿里巴巴率先将 Flink 技术在全集团推广使用，比如 Flink SQL 与 Hive 生态的集成、拥抱 AI 等；腾讯、百度、字节跳动、滴滴、华为等众多互联网公司也已经将 Flink 作为未来技术重要的发力点。在未来 3 ~ 5 年，Flink 必将发展成为企业内部主流的数据处理框架，成为开发者进入大厂的“敲门砖”。
反观国外，在 2019 年 Flink 已经成为 Apache 基金会和 GitHub 社区最为活跃的项目之一。在全球范围内，越来越多的企业都在迫切地进行技术迭代和更新，无论是更新传统的实时计算业务，还是实时数据仓库的搭建，Flink 都是最佳之选。</description>
    </item>
    
    <item>
      <title>第01讲：Flink 的应用场景和架构模型</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-02/</link>
      <pubDate>Mon, 20 Jul 2020 10:10:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-02/</guid>
      <description>你好，欢迎来到第 01 课时，本课时我们主要介绍 Flink 的应用场景和架构模型。
实时计算最好的时代 在过去的十年里，面向数据时代的实时计算技术接踵而至。从我们最初认识的 Storm，再到 Spark 的异军突起，迅速占领了整个实时计算领域。直到 2019 年 1 月底，阿里巴巴内部版本 Flink 正式开源！一石激起千层浪，Flink 开源的消息立刻刷爆朋友圈，整个大数据计算领域一直以来由 Spark 独领风骚，瞬间成为两强争霸的时代。
Apache Flink（以下简称 Flink）以其先进的设计理念、强大的计算能力备受关注，如何将 Flink 快速应用在生产环境中，更好的与现有的大数据生态技术完美结合，充分挖掘数据的潜力，成为了众多开发者面临的难题。
Flink 实际应用场景 Flink 自从 2019 年初开源以来，迅速成为大数据实时计算领域炙手可热的技术框架。作为 Flink 的主要贡献者阿里巴巴率先将其在全集团进行推广使用，另外由于 Flink 天然的流式特性，更为领先的架构设计，使得 Flink 一出现便在各大公司掀起了应用的热潮。
阿里巴巴、腾讯、百度、字节跳动、滴滴、华为等众多互联网公司已经将 Flink 作为未来技术重要的发力点，迫切地在各自公司内部进行技术升级和推广使用。同时，Flink 已经成为 Apache 基金会和 GitHub 社区最为活跃的项目之一。
我们来看看 Flink 支持的众多应用场景。
实时数据计算 如果你对大数据技术有所接触，那么下面的这些需求场景你应该并不陌生：
 阿里巴巴每年双十一都会直播，实时监控大屏是如何做到的？
  公司想看一下大促中销量最好的商品 TOP5？
  我是公司的运维，希望能实时接收到服务器的负载情况？
  &amp;hellip;&amp;hellip;
 我们可以看到，数据计算场景需要从原始数据中提取有价值的信息和指标，比如上面提到的实时销售额、销量的 TOP5，以及服务器的负载情况等。
传统的分析方式通常是利用批查询，或将事件（生产上一般是消息）记录下来并基于此形成有限数据集（表）构建应用来完成。为了得到最新数据的计算结果，必须先将它们写入表中并重新执行 SQL 查询，然后将结果写入存储系统比如 MySQL 中，再生成报告。
Apache Flink 同时支持流式及批量分析应用，这就是我们所说的批流一体。Flink 在上述的需求场景中承担了数据的实时采集、实时计算和下游发送。</description>
    </item>
    
  </channel>
</rss>