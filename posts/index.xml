<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 枕霞惜友</title>
    <link>https://xiaohao890809.github.io/posts/</link>
    <description>Recent content in Posts on 枕霞惜友</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Jul 2020 10:10:48 +0000</lastBuildDate>
    
	<atom:link href="https://xiaohao890809.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>开篇词：实时计算领域最锋利的武器 Flink</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-01/</link>
      <pubDate>Mon, 20 Jul 2020 10:10:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-01/</guid>
      <description>你好，欢迎来到 Flink 专栏，我是王知无，目前在某一线互联网公司从事数据平台架构和研发工作多年，算是整个大数据开发领域的老兵了。
我最早从 Release 版本开始关注 Flink，可以说是国内第一批钻研 Flink 的开发者，后来基于 Flink 开发过实时计算业务应用、实时数据仓库以及监控报警系统，在这个过程中积累了大量宝贵的生产实践经验。
面试是开发者永远绕不过去的坎 由于项目需要，我在工作中面试过很多 Flink 开发工程师，并且发现了一些普遍性问题，比如：
 对常用的 Flink 核心概念和原理掌握不牢，一旦参与到实战业务中必将寸步难行，一面直接被刷掉； 能够通过简历筛选的人基本都有实时流计算开发的经验，可以从容应对典型场景下的问题，但对于非典型但常见的业务场景问题就会支支吾吾，无从应答； 有些面试者自称参与过实时计算平台的架构设计、开发、发布和运维等全流程的工作，但稍微追问就会发现他在项目中的参与度其实很低，暴露出在上一家公司只是开发团队的一个“小透明”； 我们现在招聘其实是偏向招有相关经验并熟悉底层原理的人，曾经有面试者能熟练回答在项目中是如何应用 Flink 的，但是不知道底层源码级别的实现。  上面列举的这四个问题看似不同，但本质上都是在全方位考察你对技术原理的理解深度，以及在实际工作中解决问题的能力。
当然还有一类人，他们具备深厚的理论基础和丰富的实战经验，却往往因为缺乏面试经验，依然屡屡与大厂擦肩而过。很多开发者在学习完一个框架后，可以熟练地开发和排查问题，但是在面试的过程中却无法逻辑清晰地表述自己的观点。想象一下，当你在面试中被问到以下三个问题：
 Flink 如何实现 Exactly-once 语义？ Flink 时间类型的分类和各自的实现原理？ Flink 如何处理数据乱序和延迟？  你将如何作答？面试官满意的答案究竟长什么样？上述问题的答案，你都可以在这个专栏中找到。
想进大厂，必须掌握 Flink 技术 随着大数据时代的发展、海量数据的实时处理和多样业务的数据计算需求激增，传统的批处理方式和早期的流式处理框架也有自身的局限性，难以在延迟性、吞吐量、容错能力，以及使用便捷性等方面满足业务日益苛刻的要求。在这种形势下，Flink 以其独特的天然流式计算特性和更为先进的架构设计，极大地改善了以前的流式处理框架所存在的问题。
越来越多的国内公司开始用 Flink 来做实时数据处理，其中阿里巴巴率先将 Flink 技术在全集团推广使用，比如 Flink SQL 与 Hive 生态的集成、拥抱 AI 等；腾讯、百度、字节跳动、滴滴、华为等众多互联网公司也已经将 Flink 作为未来技术重要的发力点。在未来 3 ~ 5 年，Flink 必将发展成为企业内部主流的数据处理框架，成为开发者进入大厂的“敲门砖”。
反观国外，在 2019 年 Flink 已经成为 Apache 基金会和 GitHub 社区最为活跃的项目之一。在全球范围内，越来越多的企业都在迫切地进行技术迭代和更新，无论是更新传统的实时计算业务，还是实时数据仓库的搭建，Flink 都是最佳之选。</description>
    </item>
    
    <item>
      <title>15 | 用户增长：用户增长的本质是什么？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-16/</link>
      <pubDate>Thu, 16 Jul 2020 23:10:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-16/</guid>
      <description>本课时内容分为三部分：
 用户增长模型； 国内用户增长现状； 增长案例解析。  用户增长模型 用户增长的基本模型就是大家熟知的 AARRR，如下图所示。
实际上，这套模型看似很完美，然而很有问题。因为这套模型是从拉新的角度出发，对一款 APP 来说，拉新要花很多钱。如果从这个角度出发就是在不断烧钱，随着现在资本越来越理性，野蛮增长已经是过去式。
如果是从留存这个角度来出发，就会好很多，如下图所示。
先把产品打磨好，运营服务好，然后在留存的基础上进行变现，挣钱后再投入到渠道去拉新。这样会更加靠谱，毕竟人傻钱多的时代早已过去。如果一款产品在中期还要靠不断注水才能保持规模，那这样的产品肯定有极大问题，这样的团队也是非常不靠谱，只有早点转型做好留存才有希望。
先做留存，再做变现，然后做推荐、拉新、激活。现阶段大部分的产品都是这样一套玩法，而不是从拉新开始，因为拉新对渠道的要求太高。渠道思维和产品思维是两种思维，我个人觉得产品思维、用户思维会更加靠谱。
另外未来也有可能是这样，先变现，然后到推荐，再到拉新、激活、留存，如下图所示。
第一考虑因素是变现，你先说出你这产品能带来多少收入，净利润是多少。那么就先变现和推荐，然后再拿这些钱去拉新、激活、留存。因为随着资本越来越理性，普遍会从流量思维切换到 ROI 思维，毕竟活下去才是最重要的指标。
这三种不同模型的侧重点都不同，企业的打法也完全不一样。你可以想想，这套模型还可以怎么样？你可以在下方留言与我交流。
在我本人的工作中，接触到很多人，他们去做产品或者做数据分析时，都会看网上的模型 ，一直都在往某一套模型上去套，最后效果也不是很好。所以我建议你不要去纠结什么模型，也不要指望通过数据分析突然找到一个很厉害增长点，带来大量用户增长。如果有大腿可以抱，一定要坚决抱大腿。平时要学会研究自己的产品、用户，找到当前产品真正存在的问题，慢慢去解决它，建立自己的产品壁垒。你也可以去学习优秀产品的玩法，思考他们能成功的本质，找到真正的用户痛点，比如 QQ 浏览器和腾讯视频为何能后来居上？
其实分析师的任务就是做规模和带收入，一直都没变，所以一定要独立思考，不要被各种风带偏。
然后我们来看一个招聘解读，如下图所示。
职责描述中关键点就是指标体系、数据监控、数据分析和建模、A/B 测试。这些关键点与我前面讲的课时是一致的。
国内的用户增长现状 第二部分是国内用户增长的现状，先说一下关于用户增长的书和资料。
关于书，国内主要有这三本书：《增长黑客》《增长黑客实战》《引爆用户增长》。关于用户增长的大会有 Growing IO，每年一次，在会上可以推广自己的 Growing IO 大数据平台。现阶段也有些公司专门成立了用户增长小组。
对于以上信息，我都接触过。我个人觉得书确实很火，内容很有料，唯一瑕疵的就是可落地的干货不多。大会上的干货也不多，更多是以品牌宣传为主，千万不要指望参加一次大会就能学到很多黑科技。如果一个公司没有好好思考就单独成立了用户增长小组，这完全没必要。因为无论是职责还是 KPI 都会与其他组产生很大重合，所以成立用户增长小组要谨慎。
我相信你一定了解一些用户增长的方法，我这里要特意说下一些看似很管用，但落地很难导致效果不好的用户增长方法，如下所示。
 第一是魔法数字，假设我们发现一个用户阅读篇数超过 3 篇，留存将大大提升。基于这个数据，产品就会想让所有用户阅读篇数超过 3 篇，这个结论没有问题，但是在落地时却非常困难。因为这本身是用户的一种很主动的行为，单独让阅读篇数小于 3 篇的人多阅读，本身就非常难。如果做一些活动让他们多去做一些其他行为，到后来你会发现这些用户会流失。 第二是优化渠道结构来提升新增用户留存，数据分析师想要优化渠道结构，这也很难。因为用户量大、质量高的渠道总是有限，其实渠道人员在开始的时候就一直在想这件事。并且渠道链路非常长，很多因素控制不了，反馈周期也需要很久。因为渠道涉及与外部公司合作，所以很多抉择不是你直接能控制的，所以用优化渠道结构对方法，来提升新增用户留存也不靠谱。你如果真的想通过渠道提升新用户留存，最直接的方法就是把低质渠道给砍掉。 第三是流失用户召回，我经常看到很多人进行流失用户分析，然后通过一些手段召回。因为召回的手段很有限，除了 Push 也没有其他手段，Push 还经常被用户吐槽。与其把精力放在召回，还不如放在分析用户流失原因上。  其实有两个很好的增长思维，我也介绍一下。
 北极星指标：一定要找到最核心的指标。  你做产品时，北极星指标一定要找对，找到后需对北极星指标进行拆解，拆解后的指标需与每个团队的 KPI 挂钩。如果每个人都能够知道自己做的每件事是正向还是负向，那 KPI 完成程度就会很直观。
 A/B 测试：目的是公正性和快速反馈性。  A/B 测试有两个原则，第一要基于数据分析来做 A/B 测试，第二 A/B 测试不只是看结果数据，还要看过程数据，排坑是第一步。</description>
    </item>
    
    <item>
      <title>14 | 营销活动：日常运营活动的分析模板</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-15/</link>
      <pubDate>Thu, 16 Jul 2020 23:09:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-15/</guid>
      <description>本课时内容分为三部分：
 营销活动当前现状； 营销活动具体分析； 案例讲解——百度 APP。  营销活动当前现状 我之前在国企工作时，公司经常会做线上和线下活动，所以每天都会看到各种活动捷报。活动结束后，钱是花完了，真实用户数却没涨多少，大多数都被薅羊毛了。营销活动每年都会花很多钱，因此必须要找一个公正的第三方——数据分析师，来做这件事。而数据分析师既然要做，就一定要发挥出自己的专业性，大家都是罗列数字，为何你就是不一样，你的强大逻辑性在哪？
在这种背景下，我们看一下营销活动的运营人员现状。运营人员比较关注活动的三个维度：带来多少用户量的增长，拉来多少新增用户，外界传播量能覆盖多少人。而数据分析师只需在活动期间每天进行效果播报，活动后 1 ~ 2 周内产出活动报告即可。活动报告包括活动参与人数、拉新数、用户画像三部分内容。
数据分析师与营销活动运营人员相比，数据分析师的优势在于快和维度拆解性，劣势在于细节性。因为数据分析师在做分析时，只是开一个大树，在很多具体业务的细节上面，毕竟不是专门做营销活动出身，所以不是特别了解，也可能没去问，导致最后报告结论的解读可能多多少少有些问题。而对于营销活动人员来说，每过一段时间，就要搞各种活动，所以很清楚活动细节。
其实营销活动应该是一件长期的事件，不可能通过某一次活动就能够带来大量的用户增长，因此数据分析师在做这件事时，要保持以下特性。
 分析的连贯性：在活动前、活动中、活动后都要进行分析。 分析的对比性：不要单看活动本身，活动要与活动之间对比，这样才能更好分析什么样的活动更适合产品本身。 分析的公正性：该怎么样就怎么样，拉新、促活、品牌的评判都应该有一套商定好的标准。  营销活动分析无非就两件事：活动效果评估（本活动和活动对比）和活动优化建议。活动之后要对活动进行复盘，那些做得不好，之后可以避免。
营销活动具体分析 第二部分，我们来看营销活动具体应该怎么分析？我们先理一理，实际上在做任何活动之前，活动运营方都花了很多心思。活动之前必然会出文案，找开发，然后跟外面的合作方进行研讨，所有的这一切都会发生得很早。因此分析师要想做好活动分析，在这个时候就要与活动运营方多沟通，知道活动整体是怎么回事。
比如，第一要了解是谁来开发，靠不靠谱？第二要知道活动形式及测试体验，文案可能存在哪些问题。第三要想好活动大概有哪些指标。这些都要提前想一想。
活动前好好准备——前 1 ~ 2 周 在活动前，要好好准备这几件事。
 活动前和运营方商定本次活动的目标。一定要有目标，没有目标的运营不是一个好运营，没有目标的运营绝对不会使出 100% 的力气。这里能很好地培养你业务的敏感性。 活动前和研发沟通好埋点。不是每个研发都很靠谱，即使很靠谱也可能会犯错误。在埋点这件事上，分析师应该是主导地位，包括字段名、埋点位置、上报方式等。 提前搭建好指标体系和报表。一定要提前准备，活动前 1 天才发现问题，这样的情况太常见。 定好输出格式。要想好活动中、活动后每天输出哪些数据，什么形式展现，这些要与业务绑好。  正式活动前一定要好好地准备，对于一般中型的活动一般是提前 1～2 周。如果是大公司的活动，可能前一个月就要好好准备。像双 11 这种特大活动就不是前一个月才准备，可能在活动的前三个月，所有的数据分析师都在准备这件事了。
活动中好好观察——期间每一天，包括预热
其实正式活动，都有预热期，比如双 11 活动是 11 月 11 日，但 11 月 1 日起就已经很热闹了，甚至更早。所以在活动中，应注意以下事项。
 观察第 1 天的数据，这个非常关键。详细看指标体系的报表数据，查看是否有异常。因为前期修改成本非常小，对于负责人的研发也很乐意去解决这件事。 观察 1 ~ 3 天数据，预估活动目标的完成度，活动目标在前期一定是确定好的，这里要看是否要做适当调整。 定时输出活动战报，每天早上输出，让所有人都知道情况。实际上管理层都有一个比较好的心法，就是早上看数据，你不要以为他在群里面没回，实际上都会看。在工作中真实情况可能真的只有运营人员自己知道数据，有一些数据很可能还藏着掖着。这里的数据要注意真实性，该怎么样就怎么样，要敢于暴露问题，这里问题不会很大，因为所有人的目标就是希望把这个活动做好。 活动 1 周后数据复盘，1 周后进行一次详细复盘，并同步给管理层，让更高视野的人来给建议。  活动后好好复盘——公正性 到活动后期，需要好好复盘，其中最关键的是公正性，比如以下几点。</description>
    </item>
    
    <item>
      <title>13 | 竞品分析：教你如何做竞品分析</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-14/</link>
      <pubDate>Thu, 16 Jul 2020 23:08:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-14/</guid>
      <description>今天我主要讲解如何做竞品分析。
本课时内容主要分为三部分。
 为什么要做竞品分析； 竞品分析的步骤； 爱奇艺与优酷的竞品分析。  为什么要做竞品分析 在前面的课时，我介绍的分析方法都是针对自身 APP，像指标体系、流量分析、路径分析。
假设你当前所处的行业是老大，这个时候肯定要防止外来者，警惕行业老二和老三；假设你是当前行业老二，肯定要看老大最近在做什么，从而模仿超越；假设你当前是行业老三或老三以后，一方面肯定要紧跟老大老二，另一方面要放大招，弯道超车。所以在行业中无论处于什么位置都要去分析竞品。
在实际工作中，做竞品分析有以下几种情况：
 当你的产品准备进入某个行业时，需要先把该行业的竞品分析清楚，主要侧重行业规模和前景的分析； 当你的产品发展处于下降阶段，需要看竞争对手在做什么，主要侧重头部玩家的玩法分析； 当你的产品发展处于瓶颈阶段，需要看竞争对手的数据和功能迭代，持续监控对手数据，从中寻找突破； 当你的产品发展处于快速上升期，一般不会做竞品分析。  对于一款 APP ，分析师在初期就要监控好竞品的各项数据，只有这样才能保持对竞品数据的敏感性，同时跟自身 APP 数据结合起来思考优化迭代。
到底什么是竞品分析？
网上写的很多分析报告，把竞品的功能罗列太多，这其实是最初级的产品体验分析。竞品分析绝不是大而全地把竞品的功能罗列一遍，也不是日常的竞品数据监控，配置一张报表就完事。
竞品分析包含两个点：
 竞品的选择：哪些才是竞品？不要小看这件事，很多产品经理都没想清楚。并不是所有的头部行家都是你的竞品，而是要根据你做竞品分析的目的来选择。 分析什么点，这就需要你知道分析的背景是什么，从而有针对性切入。  其中分析什么点最关键，你需要知道你的 leader 想做什么，如果这件事他自己也说不太清楚，那最好先别投入大量时间去做，不是你做得对或不对，而是问题都没搞清楚，即使他是 leader。
竞品分析的步骤 竞品分析分为三步。
第一步：确定分析目的 所有数据分析的第一件事，就是要搞清楚分析的目的是什么？做分析时一定是要带着某种商业意图来做，不要忘记初心。分析目的分以下几种情况：
 当你尝试进入某个新的行业，需要评估可行性。比如唯品突然想做唯品金融，这个时候肯定就要评估可行性。这种竞品分析更加偏行业趋势、市场规模，财务收入，看大数不拘于小节。比如说看一下目前整个金融行业的情况，另外看一下电商类公司的发展，比如京东金融是怎么做的。 纯粹看竞品的功能、玩法和数据，学习优点。人无我有，人有我优，主要以学习为主。这种分析目的比较常见，主要以功能体验、运营手法、具体数据为主，落地性非常强。 通过看竞品的不同版本迭代的功能、玩法和数据，揣摩竞品想干什么，目的以预防为主。看竞品的版本迭代，思考竞品最近的战略中心在哪，这种情况往往是为了满足管理层的需要。  第二步：挑选 1～2 家竞品，进行对比分析 我们知道分析目的之后，下一步需要拉竞品跟我们自身进行对比。这时候首先挑选核心功能一样的 1～2 两家竞品，其次是功能体验分析，再就是运营手法分析。分析竞品的功能是怎么运用的，侧重对比运营手法。最后看宏观和微观的数据分析，也就是数据源，竞品数据源很关键，比如基础数据、财务数据、市场数据。
整个的过程是一项由分析师牵头与产品、运营协助的团队任务，同时可能还需要财务、市场部的参与才能完成。
第三步：给初步分析结论 第三步就是在第二步的基础之上给出一些初步结论。
比如你尝试进入某个新的行业需要评估可行性，那你最终的结论就是回答这个问题。是否可以进入这个行业？如果可以进入，步骤是什么？
比如你纯粹看竞品的功能玩法和数据，学习优点。那你最终的结论要落到竞品的什么功能好？接下来产品和运营如何去做？这样做预计能带来多少收益？
比如你只是看竞品不同版本迭代的数据，揣摩竞品想干什么，那你最终的结论就是竞品的下一步战略是什么？我们要不要也做某种尝试？实际上这种非常难，需要管理人去拍板。
优酷爱奇艺会员案例分享 以上是一些方法论，现在我以优酷、爱奇艺为例，具体来说下竞品分析（可能双方 APP 版本有更新，例子请以我截图为准）。
案例背景：
当下小 A 是负责优酷 APP 会员模块的数据分析师，Q3 季度优酷会员增长乏力，而爱奇艺会员仍处于高速增长阶段。管理层希望对爱奇艺会员进行一次分析，学习爱奇艺会员的优点，从而提升优酷会员数。
从案例背景可以看出分析目的是提升优酷会员数，分析对象为爱奇艺。我们来看具体怎么做？
第一步先看基础数据，如果公司有内部数据，直接用即可。如果没有可以使用外部数据源，外部数据源基本上可以从 Google、百度、Questmoblie、百度指数获取，但数据源只是一个参考。对于爱奇艺，其实我们能够拿到很多数据，但不是每一个数据都需要看，关键是你要想清楚看过这些数据后，对后面分析有什么用。
既然是分析会员数，我们首先要知道爱奇艺会员数跟优酷会员数的差距，如图所示。</description>
    </item>
    
    <item>
      <title>12 | 路径分析：用户的使用路径网络分析</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-13/</link>
      <pubDate>Thu, 16 Jul 2020 23:07:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-13/</guid>
      <description>今天我主要讲解路径分析。
本课时内容分为三部分：
 路径分析定义； 路径分析案例——以美团 APP 为例； 路径分析思考。  路径分析定义 我们在讲前面的案例中多次提到了漏斗模型，漏斗模型是非常经典的一种分析方法，但所有的漏斗都是人为假设的，也就是事前假设一条关键路径，事后看关键路径的转化数据。
随着各类 APP 的功能模块、坑位越来越多，用户的行为越来越分散化，比如很多 APP 不止有一个核心功能，可能有若干个核心功能。这个时候就要在用户的所有操作行为中，发现一些产品设计之初可能不知道、但非常有意思的用户前后行为，这就是路径分析。也就是说，路径分析是基于数据本身发现的，产品可能不太清楚，但是符合用户习惯的路径。
 漏斗分析：人为设定一条或者若干条漏斗。先有假设再数据验证。 路径分析：基于用户的所有行为，去挖掘出若干条重要的用户路径，通过优化界面交互让产品用起来更加流畅和符合用户习惯，产生更多价值。先有数据再验证假设。跟漏斗分析刚好相反。  我们举个例子。
比如对于美团 APP 来说，我们发现它有很多功能，比如“搜索”“美食”“电影演出”“酒店住宿”“休闲娱乐”“外卖”这 5 个 tab，下面还有“家居”等。然后下面有 4 个小模块：“很优惠”“有格调”“秒杀”“周末去哪儿”，再往下翻是“猜你喜欢”，也就是个人推荐，而在底部 button 有“附近”“发现”“订单”“我的”，各个 button 里面又有很多子模块。
基本上目前市面上大多数 APP 都是这种多坑位，把能做的都做了。在这种情况下，漏斗分析确实完全满足不了日常分析需求，因为漏斗分析相对来说都是人为事先假定的，而且内容比较符合大众认知的习惯，这个时候就要路径分析派上用场了。
这就是路径分析的背景。
路径分析过程 那么我们对路径分析的过程进行一个详细的说明。大家在听第二小模块的时候，一定要把美团 APP 多体验几次，后面会涉及大量的界面交互和路径使用，所以各个模块都要认真看几遍。
日志介绍 我们先说一下日志，因为路径分析实际上都是基于底层日志来做，有些同学可能没有看过公司本身的日志，用户在端内（ APP 内），所有的行为都是以表或者文件存储的，其中记录了用户最详细的行为信息，这就是日志。比如，你打开 APP，实际上在日志里面是有一条记录的，一般都是一行，格式如下图所示：
首先，对于日志我们怎么看？我们看到中间行与行之间是分段的，这个分段就代表每一条记录。这个日志首先是 Key-Value 格式，就以第一条记录来说，imei 和 ip 中间是以逗号分隔，每一条记录与记录间是行的分割。然后有哪些字段呢？我们有用户的设备号 imei、IP、内存、分辨率、机型、系统，event 和 active 事件、版本、子版本、操作时间 Unix time。
所以对于分析师来说，要知道自己公司本身底层日志是以什么格式来存储的，不一定是 Key-Value 格式，这块操作更多偏 Linux 命令，基本上分析师都会一些基本的命令，比如查今天的日志大小或者日志的一些字段，那么在这块查一下就行了。如果一个分析师每天大部分时间跟这种底层日志打交道，那一定是有问题的，因为这一块相对比较独立，更多的时候是数据研发工程师或者后台研发本身就应该做的事情。
日志分析步骤 当我们熟悉了日志的字段以及格式之后，就可以进行路径分析了，因为路径分析本身就是啃日志。路径分析一共分为四个步骤：
 筛选。第一步是对所有功能用户的量级进行查看，筛选出重要功能，因为当前 APP 可能有 100 个子功能，那么到底要看哪些呢？这个时候就要用用户量级来评判了，首先选出这样的功能，找到切入点。 日志关联（抽样）。第二步，就是对筛选出的功能进行时间序列的排序，比如对于一个用户来说，一天可能有 10 个重要功能，那这 10 个重要功能的先后顺序是什么样子？你要先排序，既然是路径分析，肯定有先后，排完序之后就是日志与日志之间的功能数据的匹配，比如用了 A 功能之后有多少用户用了功能 B？这个就是同一份日志相互间匹配，但是一定是先排序好。然后就是关联，对于所有的路径分析，日志关联都是抽样，因为公司的日志可能非常大，如果要做这一步关联，资源是跟不上的，所以抽样就可以了，基本上抽 10 万、 20 万数据就可以。 标准化及画图。第三步就是数据的标准化以及路径画图，因为第二步是相对绝对的数据，比如使用功能 A 的用户是 100 万，然后这 100 万里面有 80 万用了功能 B，实际上还是这种绝对量级的数据，而第三步是让第二步更加可视化以及标准化。 启发。第四步就是在第三步的基础之上，看有没有比较有启发性的路径，大部分公司做路径分析都要从第一步开始，一步一步来做。但是有一些公司在技术层面一二三步已经帮你搞定了，就是底层研发把一二三都已经做了，然后分析师相对来说要轻松，看第四步就可以了，这就比较好。  整个过程不是特别难，但是非常考验耐心，大家可以这样理解，就是你沉浸在日志当中，然后去干各种各样的行为。我们举例子来说，这样比较生动。</description>
    </item>
    
    <item>
      <title>11 | 流量分析：如何分析数据的波动？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-12/</link>
      <pubDate>Thu, 16 Jul 2020 23:07:47 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-12/</guid>
      <description>今天我主要讲解流量分析。
本课时内容分为四部分：
 背景； 渠道分析； 转化与价值分析； 流量波动逻辑性分析。  背景 上课时我讲解了指标体系，建立产品指标体系和报表之后，分析师和业务方最重要的事情就是每天看各种数据，而这个看数据的过程就是流量分析。这里的流量指广义的流量，并不一定单指日活。它是指所有的流量，比如用户从哪儿来，经过什么过程，产生什么价值，如果流量波动了，为何波动。
从流量分析的定义来看，可以分为以下四部分：
 渠道分析——从哪来； 转化分析——经过什么过程； 价值分析——产生什么价值； 波动分析——日常的监控分析。  下面我逐一来讲。
渠道分析 我们先来看渠道分析，渠道分析包含三部分：
 常见渠道及渠道分类； 渠道推广的整个过程； 渠道的关键指标及分析方法。  常见渠道及渠道分类 我们常见的渠道，如下图所示。
渠道分为内部渠道和外部渠道，内部渠道包括内部的产品矩阵。比如，今日头条会给抖音带量，内部渠道往往都是免费使用。外部渠道往往需要付费，包括多个渠道。第一是搜索引擎，比如打开百度会看到有很多广告的推广；第二是 App 内的广告，比如今日头条里会经常看到京东或双 11 的链接；第三是社交媒体，比如微信朋友圈的广告；第四是软件市场，比如应用宝、华为手机市场、豌豆荚等。
内部渠道和外部渠道都是为了拉新、拉增而用。对于一款健康的 App，前期靠渠道（特别是外部渠道）的品牌带量，后期靠自传播或者免费推广。大多公司都会单独设有渠道运营经理岗位，分析师在这部分的价值体现不会很大。
我们再看下渠道的分类，如下图所示。
横坐标是渠道给我们产品带来的量级，从左到右是越来越高。纵坐标是渠道本身的质量，一般我们可以用留存来看，有些同学可能会用收入衡量渠道质量，其实本质一样，因为留存跟收入本身就是高度相关性的指标。假设我们就是用留存来衡量，那么按照量级和质量画这样一个四象限，可以把渠道分为 4 类。
 量级少，但是质量比较高。对于这种渠道，需要扩量，需在扩量的基础上仔细观察数据。 量级多，同时质量也高。代表渠道非常好，这部分渠道需加快变现能力。 量级多，质量不太好。说明用户不匹配，交互有问题，这部分渠道需要拆解，精细化运营。 量级少，质量也差，这部分渠道直接放弃即可。  其实所有的分析都是先分析一级渠道，然后在此基础上进行拆解。比如，一级渠道的 A 渠道留存很差，我们要进一步对 A 渠道进行二级渠道拆解，看是所有二级渠道差还是部分二级渠道差，这里所有质量的好与差都是相对大盘来说。
渠道推广的整个过程 有些同学对渠道的整个推广过程理解不深刻，我举个例子来说明渠道推广的整个流程，如下图所示。
第一步是外部渠道，然后在外部渠道里，放了一个文案展示，点击文案展示之后会有一个落地页，落地页里面会提示用户进行下载，用户下载之后是打开 App ，打开 App 浏览一遍之后就是注册 App，然后直到最后一步退出。从外部渠道到注册的整个过程实际上是一个漏斗，在这里面分析师可以提供一些优化建议。
以百度搜索举例，如下图所示。
假设我搜索“外卖”两个字，出现的第一个链接是“外卖送药 京东到家”，这链接下方有个广告的标签，这是“京东到家”在百度搜索投放的外部渠道。当我点击进去之后，文案展示如上图所示，有“15元优惠券”的文案提示，下方有个横框，你有没有觉得这儿有一点不合理，因为没有提示输入什么。有些用户第一次打开会犹豫这里是不是输入手机号码，当我把手机号码输入进去之后，点击“立即领取”，就会出现一个下载页面。在下载的过程中又有很多弹窗，比如说外部风险提示等。
当 App 下载完成后，用户打开后在里面浏览或者注册，所有的这些步骤就是渠道推广的整个过程。虽然看似很简单的例子，但在页面的交互还是设计又或是产品的文案方面，都有很多优化点。
渠道的关键指标及分析方法 第三是渠道的关键指标以及分析方法。
 关键指标指前期看有效用户数和次留，中期看次日、7 日、30 日留存，后期看 ROI。  其中要注意，由于渠道都是收费的，所以有效用户数会有刷量的嫌疑，所以除了看直接量级，还要看有主动行为的用户数，比如上节课里面的停留大于 3 秒的用户数。所有渠道最终的目的还是商业变现，所以一定要计算每个渠道的 ROI，及时把 ROI 小于 1 的渠道砍掉。</description>
    </item>
    
    <item>
      <title>10 | 指标体系搭建：指标体系的经典四步</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-11/</link>
      <pubDate>Thu, 16 Jul 2020 23:06:47 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-11/</guid>
      <description>从本课时开始，我们正式进入微观模块的学习，其中包含指标体系、流量分析、路径分析、竞品分析、营销活动分析、用户增长分析。这些分析在工作过程中都非常实用，比如营销活动分析，我相信每个公司都会做活动，但具体如何评价活动，很多人都非常困惑。而在所有模块当中，最基本的是指标体系，所以我先讲解指标体系。
本课时内容分为三部分：
 指标体系的定义及选取原则； 建立指标体系的四个步骤； 知乎 App 指标体系实操。  指标体系的定义及选取原则 实际工作过程会出现令人不悦的两种情况。第一种是对于某核心数据，如日活，只知道数据在变化，但是不知道为何变化，特别是处于一个较大跌幅时，产品为了解释这种现象，就会向数据分析师要各种维度的数据。比如，年底汇报时，产品跟数据要各种各样的数据，要了之后发现跟现有数据对不上，数据内部要花费大量时间对各种各样的口径。第二种情况是每隔一段时间，产品都会拉上数据、研发一起对埋点，总是觉得当前的字段不够用，底层日志越来越大，数仓修改的也越来越多，取数越来越慢，错误越来越多。
这两种情况的根本原因都在于缺少指标体系的建设、宣贯（宣传）以及实施。其中宣贯和实施更为关键，因为有一些公司有指标体系的建设，但宣贯不到位，所以实施的时候就更不到位。比如，开会时决定要做哪些指标体系，大家都拍手叫好，但在落地时，很多动作没有做到位，这些都屡见不鲜。
业务方不重视指标体系是感觉指标体系是基建活，离完成 KPI 太远，只有出问题时才会临时重视。数据方不重视指标体系是因为这是一个吃力不讨好的工作，做好了可以，做不好就背锅。甚至有些分析师认为，指标体系只是一个思维导图而已。
要想把指标体系真正说明白并不容易，但如果你都说不明白，你怎么判断你自己真的很懂呢？作为埋点、取数、分析的一切前提，指标体系如果做不好，始终会很乱。
指标体系到底是什么？指标体系是在业务的不同阶段，分析师牵头与业务方协助，制定的一套能从各维度去反映业务状况的待实施框架。
这里面有几个关键点：
 在业务的前期、中期、后期，指标体系不一样； 一定是由分析师牵头与业务方协助，而不是闭门造车； 从各维度去反映业务的核心状况，指标有很多维度； 最后就是一个大实施框架，一定要实施，否则就是浪费大家时间。  而在指标选取时要注重几个原则：根本性、可理解性、结构性。
 根本性：对于核心数据一定要理解到位和准确，如果这里错了，后面基本不用看。 可理解性：所有指标要配上业务解释性，如日活的定义是什么，是产品的打开还是内容的点击还是后台进程在就行。 结构性：能够充分对业务进行解读，如新增用户只是一个大数，我们还需要知道每个渠道的新增用户、每个渠道的新增转化率、每个渠道的新增用户价值等。  建立指标体系的四个步骤 第二个模块是指标体系建立，知道了指标选取原则后，具体如何建立指标体系？可以分为四个步骤。在讲解指标体系建立的过程之前，我们先看一下所有指标的构成，如下图所示。
我们工作过程中遇到的指标都是派生性指标，派生性指标等于原子性指标加修饰词加时间段。修饰词本身是可选项，而原子性指标和时间段是必选项。原子性指标是最基础的，不可拆分的指标，比如交易额、支付金额，下单数。而修饰词往往是基于某种场景，注意它是一个可选的指标，比如是通过搜索带来的交易。时间段是一个必选的指标，比如，时间周期。我们选的是双 11 这一天，通过 1 加 2 加 3 就衍生出一个派生性指标——双 11 这一天通过搜索带来的交易额。如果不需要修饰词，那就是双 11 这一天的交易额。同样像次日留存、日活、月活、日转化率都是派生指标，这就是所有指标的构成。实际上它是由原子性指标加修饰词加时间段组成，这些知识在大家做数据仓库时，非常有用。
具体的指标体系建立分为四个步骤。
第一步：厘清业务阶段和方向 你要知道当前业务处于什么阶段，具体的业务方向是什么。对于一家公司往往会有三个阶段。
 第一阶段：业务前期（创业期），在业务前期更多是想快速抢占市场份额，看公司盘子大小。所以在业务前期最关注用户量，此时的指标体系应该紧密围绕用户量提升做各种维度的拆解，比如说渠道。 第二阶段：业务中期（快速发展期），在业务中期，除了关注盘子的大小，还要看产品的健康度，除了关注前面的用户量走势，更重要的是优化当前的用户量结构。如果留存偏低，必然跟产品模块有关系，是不是某功能流量承接效果太差。 第三阶段：业务后期（成熟发展期），在成熟期看变现能力以及市场份额，整个行业市场格局已定，一定要看收入指标，各种商业化模式的收入，同时做好市场份额和竞品监控，防止后来者居上。  第二步：确定核心指标 第二步最重要的是找到正确的核心指标，相信我，这可不是一件容易的事，不是因为这件事很难，而是所有人重新接受一些客观事实很难。
举个例子，某款产品的日活口径是打开 App，通过不断买量、外部刷量，日活也一直在上升。业务方觉得挺好，但分析师发现，打开 App 的用户中，3 秒跳出率达 30%，这非常不健康。这说明当前的核心指标（日活）有问题，更好的核心指标是停留时长大于 3 秒的用户数。
每个 App 的核心指标都不太一样，所以一定要多花时间去考虑这件事，这个非常重要，不只是看日活和留存。核心指标确定好之后，更重要的是对核心指标进行维度拆解。
第三步：指标核心维度拆解 核心指标的波动必然是由某种维度的波动引起，所以要监控核心指标，本质上还是要监控维度核心指标。通用的拆解方法是先对核心指标进行公式计算，再按照业务路径或者业务模块进行拆解。
比如，当前的核心指标是停留时长大于 3 秒的用户数。那么停留时长大于 3 秒的用户数等于打开进入 App 的用户数乘以停留时长大于 3 秒的占比。</description>
    </item>
    
    <item>
      <title>09 | 销售：传统行业如何做好交易额提升？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-10/</link>
      <pubDate>Thu, 16 Jul 2020 23:05:47 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-10/</guid>
      <description>今天我主要讲解传统销售行业的数据分析案例。
可能很多同学没有接触过销售行业，那如果接到一个陌生行业的数据分析需求，该如何去入手呢？我举个实际案例。
需求分析 这是我之前接到一个 case 的原始需求：
对方还提供了对应的四张表（门店信息表、产品信息表、销售经理表、销售数据表），如下图所示：
前面三张表可以理解为维度表，最后一张表为具体订单表。
 门店信息表：省区、城市、大区、门店编号、门店销售目标。 产品信息表：产品编码、产品名称、产品单价、产品经理、产品销售目标。 销售经理表：销售经理名称、大区，以及销售目标。 销售数据表：以上 3 个表的综合数据，具体的销售数据。  这四个表的需求是什么呢？
 产出 2016 年全国销售状况报告（维度多，包括时间、地域、产品、人等）。 产出 2016 年全国销售状况框架（结构化表现，X-mind 形式）。  针对这样的原始需求，该如何做呢？
实际工作过程中，原始需求往往比较模糊，数据分析师要跟业务方良好地沟通，因为有些业务方表达能力可能真的不太好。
核心指标分析 销售额完成率 实际上销售行业的核心指标是销售额完成率，所以我们按照正常业务理解进行维度拆解即可。
首先以 Boss 看报告的角度，去找到一条可以把所有的数据联系起来的清晰路径，一层一层剖析分解即可。分析路径如下图所示：
首先是总体的销售额完成率，假设上年末定的 7 月份预期完成目标是 50% 。我们看一下具体的数据，截止 7 月份目标完成情况，如下图所示：
上图左侧显示总目标为 60.5 亿，目前已完成 32.1 亿，完成率是 53%，预期目标是 50%，实际上已经完成了目标。因为这仅仅体现的是一个大数字，所以要分析总结因为哪些点做得好，哪些点做得不好，从而超额完成目标，这也是数据分析的价值。
我们前面说了总体完成率之后，再看了区域完成率，如下图所示：
从图中我们发现排名前三是华中、西南、东北，相对华南、华东、华北就差一点。
这里面西南地区虽然经济收入相对来说不高，但业绩却排在第 2 。而华北地域经济收入较高，但完成率不到 50%。实际上到了区域完成率还是比较抽象，在抽象化的基础之上，我们要想获得一些有价值的数据，必须要进行一个具体案例的分析。因此可以针对这两个地区，挑选门店单独进行分析。
我们看下门店完成率的排序，如下图所示：
Top 10 完成率最好的是门店 58，已经完成了 90%，而对于完成率最差的十家门店，最后一名是门店 3， 只完成了 37%。Top 10 的门店必然是有一些做得好的点，所以要进一步挖掘，比如以门店 58 为例，它哪里做得好，它肯定有一些可以借鉴的地方。而对于最差的十家门店，我们也要分析差在哪里，提升空间大不大。如果提升空间不大，从减少支出的角度来算，是不是可以建议直接关闭门店。接下来我会以这两个门店为例，进行详细地分析。
先看门店 58 ，我们现在手里有产品、订单、时间段的数据。因为统计的时间是 1 到 7 月份，所以会有一个持续的数据，经过这种数据处理之后，我拉了一张图，如下图所示：</description>
    </item>
    
    <item>
      <title>08 | 游戏：游戏行业的 ROI 和付费率是怎么算的？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-09/</link>
      <pubDate>Thu, 16 Jul 2020 23:04:47 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-09/</guid>
      <description>今天我以欢乐斗地主游戏为例介绍一下游戏行业的数据分析。
本节课内容分为四部分：
 背景； 指标口径； 用户流失分析； 用户付费分析。  背景 在前面两课时，我介绍了互联网和金融的数据思维，其实游戏行业兼具互联网与金融数据思维。我之前体验了狼人杀和欢乐斗地主两款 App，因为个人抵抗力比较差，每天都玩到很晚，对我的工作和生活都有一定影响，最后决定把它们删了，不玩了。所以游戏行业用户两极分化比较严重：要么快速流失，要么玩的时间就很长。
因此本节课重点围绕两个目标：
 尽量让用户晚点流失——流失分析 让花时间的用户多变现——商业分析  我们先来看看欢乐斗地主 App 的界面，如下图所示。
在上图界面的右侧，有经典、排位、残局、比赛的坑位，点进去之后是对应玩法。最右下角的有一个“商城” button （小人推着小推车），点击进去之后如下图所示。
这是欢乐斗地主 App 最重要的界面，后面的讲解我会围绕这些图片来展开。
指标口径 第二个模块是指标口径，指标口径包括常规指标和商业化指标。
常规指标 常规指标包含四类：
常规指标包含四类：
1、 DAU、WAU、MAU
DAU、WAU、MAU 分别指产品的日活、周活以及月活。对于任何产品，首先要看用户规模，用户规模是一个亿还是五千万，要有具体的数值。
2、 留存率
留存率一般是看次留、7 留、30 留存率。留存率是一个比例，反映产品的联系程度。以次留率为例，次留率等于第一天打开欢乐斗地主并且第二天也打开欢乐斗地主的人数除以第一天打开欢乐斗地主的人数。
比如，第一天打开欢乐斗地主 App 的有 1000 人，第二天这 1000 人里面又有 800人打开了App，那么它的次留率为：800/1000 * 100=80%。
3、 渗透率
每个产品都有很多子功能，每个子功能的渗透率等于该模块的使用人数除以该产品的日活。
比如，欢乐斗地主 App 的昨日 DAU 为 1000 人，昨日有 900 人点击“商城”。
那么“商城”渗透率为： 900 / 1000 * 100=90%。
4、 转化率
转化率针对某个连贯路径。它等于使用下一个节点的用户数除以使用上一个节点的用户数。</description>
    </item>
    
    <item>
      <title>07 | 互联网金融：芝麻信用分的建模过程是怎样的？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-08/</link>
      <pubDate>Thu, 16 Jul 2020 23:04:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-08/</guid>
      <description>今天我以芝麻信用为例介绍一下互联网金融行业的数据模型。
本节课内容分为三部分：
 背景； 授信模型； 模型落地。  背景 互联网金融的本质是风控，数据分析师在金融行业基本上有两种角色：
 数据建模师，要求对算法的理解较深，相对来说对行业经验要求不是很高； 风控分析师，除了一定的模型理解能力，还需要大量的行业和法律法规经验。  互联网金融与其他行业不太一样，互联网金融在产品对象上分为 to B 和 to C。
 to B：对企业整体的信用进行评估做整体授信。 to C：对个人的个人信用分。  而无论是 to B 还是 to C，在决策上都是依赖央行征信报告（数据最全）。
数据建模师工作内容 数据建模师平时工作的主要内容是什么呢？我找了一家国内大型的互联网金融公司的职位描述，如下图所示。
关键词为数据源、信用评分卡模型、模型上线监控维护、其他数据挖掘。与其他行业数据分析师差异比较大的是数据源，因为互联网金融行业很多时候要规避风险，怎么去规避风险呢？基于大数据，所以数据源越多越好，因此数据建模师平时要与其他公司进行数据合作或数据采购。
总的来说，数据建模师偏算法，但要很懂业务，不是纯算法分析师。
授信模型 接下来我重点说一下授信模型。模型具体是什么呢？以芝麻信用分来为例，如下图所示。
芝麻信用分是由五大维度构成：
 身份特质：你的学历（高中毕业还是博士毕业），表示人本身的稳定性，长时间改变不了的特质。 履约能力：看你消费后按时还款的能力（是否有房有车），表示人消费的兜底性。 信用历史：看你历史的信用（信用卡有无逾期），表示人本身的诚信。 人脉关系：看你支付宝好友的信用分是不是都很高，表示个人身份的稳定性及弱价值性。 行为偏好：看你是喜欢买价格高的还是低的，这部分数据最重要，表示人本身的当前信息,对产品后续决策有非常大的价值。  芝麻信用能够很好地判断一个人的信用到底好不好，另外一个潜在的价值是可以结合人的行为偏好来做更精准的推荐。
数据源 数据建模授信模型的第一步是数据源，同样以芝麻信用为例（如下图所示）。
从图中可以看出这里有一级分类：身份特质、行为偏好、履约能力、信用历史、人脉关系。这五大维度实际上有很多字段的中文名，每一个维度大概用了哪些字段，这些就是数据源。
这里真实的数据变量有上千个，为什么会有这么多变量呢？实际上数据变量分为原始变量和衍生变量。
 原始变量：是直接存储在数据库里的最基础变量，如每天的交易额（你今天花了多少钱）。 衍生变量：衍生变量是在基础变量的基础之上进行的，因为金融的本质是风险，所以都要对原始变量进行加工转化，一般是三种。  时间维度衍生，最近 1 个月交易额、最近 3 个月交易额。 函数衍生，最大交易额、最小交易额、交易额方差。 比率衍生，最近 1 个月交易额/最近 3 个月交易额。    基于这三种变换就可以对原始变量进行扩充，所以最终的数据模型里的数据变量非常多。在选择变量的时候，基于 RFM 原则，即最近、频次、钱。所有跟这三个属性相关的变量都要先保留，因为金融行业本身就是在和钱打交道。</description>
    </item>
    
    <item>
      <title>06 | 电商数据分析：京东 App 的详细产品分析</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-07/</link>
      <pubDate>Thu, 16 Jul 2020 23:03:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-07/</guid>
      <description>今天我讲一下京东 App 的详细产品分析。
目前电商数据分析比较成熟，整个分析脉络非常庞大，如果我单纯地讲一些分析方法可能会大同小异，所以本节课以零售的北极星指标（关键性指标）交易额为切入点，针对交易额的核心转化率做一些分析，并且对新用户的获取进行一些场景分析。
本课时分为四部分：
 如何看待京东 App； 整体数据的分发效率； 漏斗分析； 新用户分析。  如何看待京东 App 当你拿到一个 App 时，首先是从用户视角去体验，以京东 App 用户视角为例（如下图所示）。
App 的主界面主要分为以下几个模块。
 搜索：流量最大的入口； 广告 Banner：用于各种活动宣传； 导航：十宫格（超市、数码、美妆等十大类主要产品），受众覆盖广，分类相对比较稳定； Feeds 流：电商+内容； 个性化推荐：实现千人千面； 底部 Button：五大主模块（我的、购物车、发现、分类、首页）方便快速查看。  首先作为分析师，视角要高于普通用户，除了知道有哪些功能模块以及所在位置以外，还要更深入且有层次性地去看 App 。比如，当前产品有什么痛点，怎么样去优化？这里有三个问题需要你思考。
 引流（场）：首页作为最大的带量位，分发效率怎么评估。 漏斗（货）：北极星指标交易额只是一个数字，更加重要的是理解这个数字转化的过程。 用户（人）：作为一款非常成熟的 App，老用户相对比较稳定，但新用户获取应该怎么优化。  其中引流是对整个 App 的整体分析，漏斗是对核心路径的分析，用户是对产品当前痛点进行分析。
整体数据分发效率 对于分发效率的评估除了要关注日活、留存、渗透率等常规指标外，还要找到能反映产品问题的指标。比如 CTR 和人均访问页面数，这两个指标就能很好反映产品问题。
 CTR：CTR = 点击 UV / 曝光 UV，反映用户点击欲望的指标。  点击 UV：每天有多少用户点击进入到页面。
曝光 UV：每天有多少用户看到了页面。
这个非常重要，因为只有点击才能产生交易，如果较小，首页问题较大。
 人均访问页面数：总访问页面数（PV）/ 总访问 UV。  总访问页面数 PV：点击所有页面的次数总和是多少。
总访问 UV：点击所有页面的人数总和是多少。</description>
    </item>
    
    <item>
      <title>05 | 多元思维模型：数据分析需要具备的四大能力？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-06/</link>
      <pubDate>Thu, 16 Jul 2020 23:02:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-06/</guid>
      <description>今天我讲一下多元思维模型。
本节课内容一共分为四部分：
 背景； 中观能力； 微观能力； 宏观能力。  背景 目前在整个数据分析行业中，大部分同学偏数据库和机器学习，又或是学计算机专业出身，所以会造成一种行业错觉——会点技术、会点 PPT 就可以做数据分析，感觉门槛比较低；有时候觉得自己做出的分析报告，别人能很快的发现问题，自己也认可，但就是不知道如何避免；有时候针对某个问题，有些人总是能有很多想法，但自己却不知道怎么理解；有些人职业发展的很顺利，而自己却始终有瓶颈。
其实这些困惑都非常正常，所以这一节课的目的就是告诉你，到底掌握好哪些能力才能够成为一名优秀的数据分析师，这也是后面所有课程的一个基础。这一节课会有一点点抽象，但是你要好好学。
先给出数据分析的多元思维模型，就是从中观、微观、宏观三个角度去出发。
 中观能力：中观能力是真正的专业度，看你是否能够发现其他数据分析师在分析中的问题。这个专业度不单指你的技术，而是需要你长期总结和思考。 微观能力：微观能力指有效沟通力+快速发散收敛力，看你是否能够从业务的交流中发现问题，找到方向，很多同学都还没意识到这一点。 宏观能力：宏观能力是洞见性的全局观，能够从社会事件和整个行业发展中找到业务的决策方向，这是极难的能力，同时平台和天赋缺一不可。 怎么获得这些能力呢？我逐一来讲。  中观能力 中观能力指专业度，包括技术理解、逻辑性、价值点三个点。中观能力是反映分析师基本功怎么样、套路熟不熟练、思考到不到位的一种标准。
 技术理解：指对分析时用到的技术理解是否到位，是停留在理论阶段还是在实践阶段。很多同学看了很多数据分析的书籍，理论说起来无所不知，但在实践过程中还是遇到很多坑； 逻辑性：指对整体思考的逻辑性是否欠缺； 价值点：强调价值，你做出来的分析价值在哪。如果现在你是决策者，你敢不敢立马规划落地。  中观能力的提升相对比较容易，基本上就是从他人那里获得有效反馈，所以你做出的分析一定要获得高手的反馈，让他指出一些不足或建议，然后多实践。
（1）技术理解 在数据处理中经常用到数据标准化方法，比如常见的 MAX-MIN （最大最小值）方法、Z-score （z 分数）方法、指数对数法。但这只是理论上的方法，你需要理解到数据标准化的本质目的是去除量纲、量级的差异性，才能在业务中有效地利用。
举例：对于 to B 的金融公司来说，除了头部的大客户（前期资源），剩下的都是中小客户（后期拓展）。大客户和小客户需按照每天的交易额来区分，所以需要我们对交易额及用户进行建模。这时就要用到数据标准化，以 MAX-MIN 方法为例，如果直接用这种方法，会造成除了头部几个数据有数值外，其他基本都是 0。到与业务方沟通时，业务方 leader 会觉得你这个方法很有问题。数据非常稀疏，无论是可用性还是理解性都很困难。
所以，有两个解法：
第一对客户进行先分群，再用 MAX-MIN 进行标准化。
第二以 90% 中位数替代 MAX，消除头部影响，让数据变得不那么稀疏。
实际上所有的技术都是为了让业务更加方便，更加高效，而不是让人很困惑，这就是技术理解。
（2）逻辑性 关于逻辑性，我举一个资讯类 App 的真实案例：数据分析师在研究最近一个月的数据，发现所有与留存相关的因素中，留存和自媒体文章下发的占比存在高度相关性。于是就建议业务方多下发自媒体文章，业务方觉得这个点很好，还真的做了。结果是刚开始几天留存是微涨，后续却大跌。
你觉得这个案例的问题出在哪 ？
其实相关性是一种基于向量的伴随关系，不代表直接的因果关系（但确实是因果关系的一种可能性），也就是说留存和自媒体文章下发的占比是一种伴随关系，而不是因果关系。留存的影响因素非常多，不仅仅是因为某一两个指标就能很好地提升留存。
上述例子后来复盘发现，最近刚好是因为有一些重大热点导致留存提升，而这些重大热点文章是以自媒体文章为主，最终造成了自媒体文章下发占比能提升留存的假象。
所以分析师的逻辑性是非常重要的，每一环节的推导必须要讲究严谨性，不能有侥幸心理。
（3）价值点 第三价值点，强调你做的所有分析一定要有价值点。
我曾经见到一个同学，竟然在业务分析报告里面用到了大量的复杂公式，最后还画了一张非常复杂的技术图，在会议上大家都不好意思反驳他。后来问了业务方，得到的反馈是，整个分析报告用了两个月时间，看似解决了很多技术痛点，但是对业务提升没意义，因为听完不知道怎么去落地。
在数据分析过程中，有些是避免不了的描述性统计，你要快速解决，切记不要耽误时间。而对于指导性、预测性的分析，最花时间也是价值最大。你一定要利用好有效时间找到价值点，即使这个价值点只有 1 个。请注意：有没有价值不是分析师说了算，而是业务方说了算，有些点很好但暂时无法落地，就先不要管它。
中观能力能体现分析师的专业度，基本上就是多沉淀、多思考、多反馈、多总结。
微观能力 第二个是微观能力，我们先看下微观能力的背景。</description>
    </item>
    
    <item>
      <title>04 | 如何挑选适合项目场景的数据分析工具？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-05/</link>
      <pubDate>Thu, 16 Jul 2020 23:01:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-05/</guid>
      <description>今天我讲一下各种数据分析工具的使用场景对比。
本节课内容分为四大部分：
 数据分析整体流程； Excel 常用操作； SQL 常见问题； R 语言以及 Python 脚本案例。  数据分析整体流程 数据分析有一套标准化流程，很多人在做数据分析的时候，不知道怎么去开展或者怀疑自己做出的分析报告是否合理、是否全面，这是因为你对这一套标准化流程没有真正的理解。
来看一下标准化流程的九个步骤：
 明确问题，先把问题定义清楚，因为很多人还没理清问题就直接去看数据了； 搭建框架，定义问题之后再把问题考虑全面、找到一条分析主线； 数据提取，用 MySQL、Hive 等工具提取相关数据； 数据处理，用 Excel、R、Python 处理数据； 数据分析，以数据分析方法论为主来分析数据； 数据展现，用 Tableau、Excel、R、Python 工具把你的数据展现出来； 撰写报告，考验你的文笔功底以及整体逻辑性； 报告演讲，考验你沟通能力，表达能力，被提问能力。所有的报告撰写完成之后不要直接去讲，还是要和业务方进行大量的沟通，如果不提前做好沟通，你在会议或公众场合上讲时很容易被别人挑战； 报告闭环，这是最难也是最大价值的地方。  这里面的数据提取、数据处理、数据展现是数据分析师前期的基本功，以工具为主，都是比较容易学到的，也比较容易完成。而明确问题、搭建框架、撰写报告、报告演讲、报告闭环更多是考验分析师的综合能力以及智商、情商，所以这块往往需要很多时间去沉淀。基于数据分析这一套标准化流程，其中涉及一些你必须要学的工具，下面我简单讲一下这些工具。
 MySQL、Hive：基本上所有的数据获取方式都是通过 MySQL、Hive 这两种语言来实现，同时你要学习一些 Linux 命令，因为在排查数据异常时会用到。你需要对这两门工具超级熟练，因为数据提取环节是不能出错的，这一步有问题，后面就都有问题。 Excel：Excel 是最高频的数据处理工具。工作中你经常遇到的一种情况，你的 leader 直接让你现场画个图，这时你最有可能用 Excel 而不是 R、Python。 R：R 是一门统计型语言，专门为数据分析而生，简单易学，但缺点是计算能力确实比较差，你导入两个 GB 数据就有可能导致死机。 Python：Python 是一门真正的脚本语言，可扩展性极强，算法研发同学必备。而数据分析以 Pands 包为主，其他常用包含爬虫、文本挖掘。  Excel 常用操作 先看 Excel 常用操作，一般通过 SQL 在数据库中提取数据，保存到本地 Excel，所以 Excel 是最基础也是最重要的一个数据分析工具，能用 Excel 坚决不用其他工具。
Excel 对比分析（筛选和色阶功能） 对比分析是数据分析中常见的一种分析方法。所有的数据只有对比才有意义，比如：每年的双 11 都会与之前的双 11 进行消费额对比。在工作中最常见的对比对象就是大盘，比如：新上线一个功能，怎么样评估这个功能效果，除了看功能使用人数，更要做这个功能和大盘的留存对比，如果高于大盘留存，代表这个功能有非常好的正向效应。</description>
    </item>
    
    <item>
      <title>03 | 怎样才更好地转型或成功跳槽？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-04/</link>
      <pubDate>Thu, 16 Jul 2020 22:59:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-04/</guid>
      <description>今天我讲一下中小企业的数据分析工作。
本节课内容一共分为三部分：
 对上节课三个问题进行解答； 日常工作分析； 转型四步法。  三个问题的解答 Q1. 流量波动，数据突然涨了怎么分析——考察分析师的经验怎么样 这个问题非常经典，因为它能够很好地考察分析师的经验，虽然很多分析师工作了很长时间，但依然解答不好这个问题。
举个例子：美团外卖近期的订单量突然下降 5%，需要分析师给出解释并提供下一步建议。
一些经验不足的分析师遇到这个问题时可能会盲目地检查原因，比如是否由口径问题、数据存储问题、产品变化问题等原因导致。但这样的回答都是单点分析，缺少全面性，我们看一下参考答案。
先对命题进行解析，订单量下降 5% 属于什么水位，影响范围有多大。如果发现对收入有重大影响，这个时候 CEO 都可能会关注这件事情，所以要更全面地分析原因。
具体的分析模块包括以下几点（如图所示）。
 常识判断：最近是否有比较重大的节日，用户外出度假旅游导致订单量有所下降。 竞品数据：竞品最近的数据有没有大涨，最近有没有做一些营销活动导致我们的订单量下降。 外部事件：社会上有没有针对外卖的一些负面事件，是否对品牌本身造成影响。 产品变化：用户订单下降的产品本身有没有发布最新的版本，部分功能有缺陷导致用户无法下单。 用户行为：整体的订单量下降是因为全国的订单量均普降，还是因为部分区域的订单量下降。 数据问题：是否是因为采集数据的统计口径发生变化。  当你从这六大模块去分析这件事，面试官会觉得你具有条理性，有架构。最主要是能很快定位到问题，这个非常关键。所以一定要有组织有架构地回答问题，而不是单点进行分析。
Q2. 常用三个 App-考察分析师的思考深度怎么样 这个问题我建议你回答跟应聘岗位相关的 App，比如：你应聘公司的产品是 QQ 音乐。
这个时候你可以说常用的三款 App 是网易云音乐、微信读书、知乎。为什么这里会说微信读书和知乎，是为了给面试官衬托你的亮点——网易云音乐。
面试官进一步问：“就以网易云来说，能不能说下你对这个产品最喜欢的点，以及最想吐槽的点。” 请注意这个问题考察你是不是高于普通用户。
你可以这样回答：“最喜欢网易云的每日推荐，最想吐槽的是很多时候通过搜索来选择听某歌，但是搜索栏里面没有语音输入，同时下面的热门搜索跟我的画像非常不准，都不是我喜欢的，那些歌、明星可能我都没听过”。这个回答代表你已经有了自己的见解，不只是简单使用产品而已。
面试官再问：“好的，那么如果你是产品经理，你会怎么样解决这个问题？”
你要回答：&amp;ldquo;先看热门搜索的点击率多少，如果较低说明确实有问题 ，可以把热门搜索功能与用户的画像匹配，实现千人千面，同时在搜索栏增加语音输入功能。&amp;rdquo;
面试官再问：“怎么样评估这样做就能够带来指标的提升？ ”
你可以回答：“这种先开始小流量 A/B 测试，然后再慢慢放量，如果效果还不错，就全量 。”
面试官问：“能不能说下你对 A/B 测试的理解，如正交性，A/B 测试的设计……”
问到这儿表示面试官对你已经非常感兴趣啦。
这就是面试官问你常用三个 App 问题，如果你回答微信、淘宝、B 站之类，你就不能衬托你的亮点，让面试官接着往下问。
Q3:商业化变现—考察对商业的最终目的是否敏感 实际上分析师日常所有的 PV、UV、MAU、DAU 等都是围绕最终的本质目的——商业变现。
举例：面试官让你举例一款工具类产品，说说如何商业化。
你可以这样回答：“以墨迹天气 App 为例，首页&amp;quot;天气&amp;rdquo; Button 是主流量入口，进行底部下拉时会出现资讯，而在资讯里面有较多广告 App 下载链接， 所以这是一个 App 带量商业化时景 Button，内部有较多旅游景点、住宿类 App下载推荐，所以这里是一个自身 App 高相关商业化推荐 。&amp;ldquo;Me&amp;rdquo; 这个 Button，分生活、娱乐、休闲、游戏四大板块，每个模块都有自己的商业化坑位 。”</description>
    </item>
    
    <item>
      <title>02 | 如何搞定 BAT 大厂的数据分析项目？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-03/</link>
      <pubDate>Thu, 16 Jul 2020 22:58:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-03/</guid>
      <description>今天我讲一下 BAT 的数据分析工作。
BAT 招聘解析 通过招聘解析的讲解，一是希望你能了解互联网大厂数据分析师的日常工作内容是什么，揭开大厂招聘的神秘面纱；二是通过他们的岗位要求，了解到自身的差距，并对不足有针对性的面试准备，或提升自己在行业内的技术能力。
阿里 先看下阿里的岗位职责描述，关键词有“代理”“方向”“痛点”“转化”“风险”“创新”“落地”“合作”，岗位要求的关键词有“敏感度”“方法论”“管理整合”。由此可以看出，阿里的分析师岗位对技术要求不是很高，但对人的综合能力要求非常高，需要具备一定的数据敏感度和方法论。之所以没有特别要求技术能力是因为市场上工作三五年的分析师基本上都具备了一定的技术能力，但仍有很多人缺少宏观、中观与微观意识，这也是大部分人的职业瓶颈。
腾讯 再来看下腾讯的岗位职责描述，关键词有“埋点”“异常检测”“决策”“A/B 测试”等，岗位要求包括“海量数据处理”“用户增长”等，可以看出腾讯的岗位职责中规中矩，基本涵盖了 80% 的互联网公司数据分析师的日常工作内容。值得关注的是腾讯特别提到了增长黑客的经验，目前独角兽企业都会要求有这样的经验。
百度 再来看下百度的岗位职责描述，关键词有“分析体系”“专项策略”“用户增长”“逻辑思维”“敏感度”等，可以看出百度对个人的“发散思维”和“策略研究”要求非常高，同时在用户运营领域也要有自己的体系。
我们可以看出对于 BAT，他们会特别关注用户增长，毕竟独角兽企业只有持续增长才能获得发展，而在实际工作中，你也会感同身受，所有项目都是为了用户增长而构建。
日常工作 第二部分就是日常工作，日常工作主要包括数据异常的排查和融入专项，做专项分析并负责 KPI、埋点，指标体系等。
数据异常排查 我们先看下数据异常排查的背景，一般情况下，BAT 的数据产品 DAU 都比较大，动辄几百万上千万，甚至过亿，因此业务方和管理层每天都会盯着核心数据，而在这些核心数据中肯定会有一些数据是波动比较大的。这时，分析师需要对这些波动进行排查并解释原因，如果没有一套方法论面对问题就会很头痛，你可以回想下自己是否面对过这种情况，每天早上面对波动数据无从下手，找不到原因，解决不了，进而浪费很多时间做了很多无用功，感觉永无出头之日。
实际上，数据有较大波动，无非就两个原因：一是目前数据本身有问题；二是业务本身有问题。
如果能够透过问题看本质，你就可以在数据波动方面成为专家。当然数据异常排查是需要一些前期准备的：
 业务理解； 指标口径； 当前数据产出过程。  第一个业务理解，比如某个 App 的 DAU 低于1000w，那么请问这个 DAU 代表的是什么行动的DAU，是在进程中还是需要打开 App，还是必须有主动行为，这理解起来是不一样的。第二个指标口径，同样是 DAU 一千万，是 Android 还是所有系统。第三个产出过程，对于 DAU 一千万目前是由哪份日志做了哪些数据清洗计算出来的，只有了解清楚这些才能够开始异常排查。
举个例子，市场部领导看了某一张日活数据和你提供的数据相差较大，就来询问是怎么回事。实际上这时你首先需要弄清楚他看到的数据表是怎样产出的，然后指标口径是什么，指标的业务含义是什么，只有熟悉这些情况后才能分析出产生差异的原因。
实际工作中，有些分析师在进入一家公司时产品已经比较成熟，但指标口径没有文档化，所以可能对业务理解不深，这个时候面对领导的提问就会手足无措，一旦不能解决问题就会失去信任，所以前期准备工作一定要做好。
有了前期准备工作，接下来就是异常排查步骤了，异常排查主要分三步：
 判断是否异常； 最大概率法则归类； 闭环。  第一步判断是否异常，有四个关键点：
 亲自去看数据准确性，不要人云亦云，比如业务方说 DAU 下降了就立马去调查，这是不对的，而是应该亲自查看数据是否真实，有时候业务方不一定多专业，也会出现错误。 时间轴拉长，看是近期异常（3 个月）还是历史异常，一般分析师看数据时习惯看近一两周或一个月的数据，然后突然出现波峰或波谷就认为数据异常了，但实际上往往不是。我们一定要拉长时间轴，如果仍出现波峰或波谷可能就真的出现异常了。 看和该指标关联的其他指标或其他核心指标是否也异常，比如 DAU 异常时，需要查看自流、渗透率是否异常，如果也异常就需要一起解决，而不是按下葫芦浮起瓢，反复做无用功。 找到一个关键人物（产品/数据），提前沟通，也就是当我们确认是数据异常后，找经验丰富的人提前沟通，看他们对此是否有什么见解，往往经验能够快速的定位问题。  第二步就是最大概率法则原因归类，很多分析师遇到异常时无从下手，抓不到问题主线，无法对问题进行有效分类，而我把异常问题分为了六大类，基本上所有的异常问题都归属于这六大类。
 假期效应：开学季、暑假、四大节、当地节日； 热点事件：常规热点（世界杯）、突发热点（爆款 IP）； 活动影响：双 11、618，公司层面活动； 政策影响：互联网金融监管，快递实名； 底层系统故障：数据传输、存储、清洗有无问题； 统计口径：业务逻辑更改、指标计算方式更改。  所以当我们遇到问题时，就可以按照降序在这六大类中逐一排查找到问题原因。</description>
    </item>
    
    <item>
      <title>01 | 如何解决临时提数需求？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-02/</link>
      <pubDate>Thu, 16 Jul 2020 22:58:42 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-02/</guid>
      <description>今天主要讲解国企的数据分析工作是什么样的。
国企招聘解析 我们先看第一部分招聘解析。学习招聘解析，我们的主要目的有两个：
 通过对比分析工行、招行、广发、电信四家公司的数据分析岗位，找出不同类型企业对应聘者的具体要求，或者说共性及差异点，从而能够有所准备的去面试。 对岗位职责及相关要求进行提炼和总结，从而让你能够匹配出适合自己的企业。 刚参加工作的人往往比较看重薪酬，但随着工作年限的增长，你会发现自身能力是否可以在工作中得到提升至关重要。  刚参加工作的人往往比较看重薪酬，但随着工作年限的增长，你会发现自身能力是否可以在工作中得到提升至关重要。
而想要找到适合的工作，同时自身能力又能够得到提升，首先需要明确数据分析师发展的三个方向。
 业务型：业务型对分析师的业务理解能力要求非常高。 数据研发型：这个发展方向要求你对技术非常熟练，能够通过技术对数据进行分析。 算法型：算法型要求你数学功底要好，而且课题的研究能力非常强。  了解发展方向后，数据分析师一定要结合自身情况在工作三年后找到自己的定位。
当然，如果在投递简历前就能够从 JD（岗位职责）上看出是否适合自己，从而决定是否投递简历，这对于最终是否能成功进入心仪公司来说事半功倍。
工行 我们先来看工行的数据分析师岗位职责：
你说工行好不好，肯定非常好，但是它对数据挖掘岗位的职责是非常模糊的，而且要求一定的技术基础。如果你不是一个对技术感兴趣的人，而又在一个错误的时间进去，那么你可能会很快跳槽。
招行 下面我们来看下招行的数据分析岗位职责：
包括分析大零售客户群、标签、客户细分及客户画像等，要求非常具体。在专业上也提出了具体要求：金融、统计、经济、管理和计算机专业。如果你符合以上专业，那么就有很大的机会进入招行。而招行作为后起之秀，往往追求一定的创新，所以如果你思维发达，喜欢挑战，招行还是比较适合的。
广发 下面我们再来看下广发银行的数据分析岗位职责：
从它的岗位职责描述来看，偏向业务型，事情繁杂，这时你就需要看是否适合自身的发展了。但它的应聘要求并不十分明确，优秀的数据分析能力，对数据敏感，有一定的市场敏感度，这些都是无法准确衡量的，这时候面试时的临场表现就至关重要了。
电信 最后看下电信的数据分析岗位职责：
相比于银行来说，电信的岗位职责中规中矩，日常工作主要是统计报表、专题分析，要求你具备一定的独立思考能力。
以上是大部分国有企业对数据分析师的岗位要求，你在看招聘信息时，一定要从岗位职责及能力要求进行分析，综合考虑企业目前所处状态及日常工作内容，这对于此岗位是否适合你起关键作用，同样在面试中也非常重要。
国企常规工作 接下来，看下国企数据分析日常工作都做些什么，主要包括三个方面。
 日/周/月报； 临时数据； 常规工作的优化。  日/周/月报 作为一名分析师，日报是每天都需要关注的，但是在日常工作中往往得不到足够的重视。而分析日报主要有三个目的：
 了解业务现状； 培养数据敏感性； 提供业务发展建议。  先说了解业务现状，我问过很多同学：“你为什么不愿意看日报？”很多人会说：“每天数据就那样，没什么好看的。”实际上，这个理解是非常浅层次的，如果数据一直就那样，那就说明公司业务出问题了，而这时如果你能够指出问题，一定能够得到展现自己的机会。
然后是培养数据的敏感性，很多公司在招聘时都要求具有良好的敏感性，但实际中敏感性都是慢慢培养出来的，没有人天生就对具体业务敏感。如果你可以每天关注数据的波动，潜移默化的便会培养一定的敏感性，比如之前每日收入在 500 ~ 600 万之间，突然变成了 650 万，这个时候你能够发现便会并去寻找原因，这便是数据敏感性。
最后就是为业务提供发展建议，因为数据波动时肯定需要寻找波动原因，为何涨跌，久而久之你就会发现产品或运营在做出何种调整后数据会出现涨跌，进而能够为业务发展提供更合理的发展建议。
在日报的基础上，周报就可以看作是一个短期趋势了，因为很多公司的发版周期往往是一周。新版本的效果能够直接体现在周报中，同时一周的数据会更加稳定、更具说服力。
而月报的周期就比较长了，基本上所有公司每个月都会进行一次例会，而在月例会中会对业务数据进行分析，也更能够为接下来的业务提供更合理的发展建议。
数据敏感性和业务的发展建议都是从日报/周报/月报的分析总结中不断积累的，而对于大部分公司来说日/周/月报只是常规性的经营分析，罗列数字，很多时候失去了数据分析的本质意义。
作为一名优秀的数据分析师一定要经常看三种报，培养自身的敏感性，从而找到获取业务增长的发力点。
临时数据 第二块就是临时数据，这是大部分数据分析师职业生涯的第一个痛点。
我面试过很多工作时间较长的同学“你工作中最主要的事情是什么？”，很多人的回答都是在不断地满足临时提数需求，如果你也是这样就非常危险了，一定要想办法进行优化。
目前我将临时提数需求分为两种，一种是管理层的需求，另一种是业务执行人员需求。
 对于高管层的临时提数需求，优先级肯定是最高。但此时一定不能立马去做，而是需要思考为何需要这个数据，通过这个数据能进行什么决策。举个例子，CEO 现在正在与外部公司商谈合作事宜，需要了解用户人均时长，如果你不知道这个背景前提，给出的数据往往有误，这时需要及时的与领导进行沟通。 而业务线的临时提数需求是非常繁多的，比如数据指标口径、数据增长计算，等等，这时就需要你根据自身的实际工作情况合理安排，学会合理地拒绝一些业务线可以自己解决的事情，不断提升自己的工作价值，而现实情况往往是大部分数据分析师都陷在临时数据需求的泥潭。  以“掌上生活” App 最近上线的一个线下餐饮优惠券功能为例，产品经理需要快速得知优惠券的使用人数，如果你拿到这个需求后，立马分析使用人数字段口径是哪个日志，然后写 SQL 把结果反馈给产品经理，那么他可能会发现使用人数不够广，接下来便会要求你提供优惠券下发人数，再从头做一遍，周而复始，你会发现后面还会有使用频次、消费金额，等等需求在等着你，面对这样的窘况如何解决？</description>
    </item>
    
    <item>
      <title>开篇词：数据分析能力，是每个职场人必备的核心竞争力</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-01/</link>
      <pubDate>Thu, 16 Jul 2020 22:58:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-01/</guid>
      <description>你好，我是花木，你也可以叫我 Spring。
我本人是学计算机的，毕业后因为机缘巧合入了数据分析这行，如今也有近 10 年时间了。这期间，我在各种类型的公司都待过，一路从数据分析小白到数据分析工程师、高级数据分析工程师，最后到行业数据分析专家。因为非专业出身，我遇到了大多数人都会遇到的问题，踩了大多数人都会踩的坑，走了不少弯路。但也正因为不懂，对于各种业务问题，我都会从不同层次和视角出发去思考和验证，因而对各种层面的问题和解决方案也都有更切实的体会和深入思考。
我平时喜欢和同行业的人沟通交流，而越来越发现，很多新人甚至从业者都对数据分析岗位或者自身职业发展存在困惑，我总结了以下几点：
 做不好：学了很多工具，看了很多学习资料，却还是做不好数据分析？ 没经验：明明精通各类算法，但到了分析数据的时候，却还是败给了懂业务的公司同事！ 没想法：看到的数据只是数据，而领导看到的是机遇和方案！ 低薪水：前同事每两年换次工作，已经薪水翻倍，而自己面试时却节节失利？薪水原地踏步。  我经历过自己摸索的迷茫期，也在项目中体会过灵光乍现的喜悦，你经历的问题，我都经历过。
工作中，我前前后后负责过很多项目，一方面在职场上为公司做了贡献，一方面也成就了自己。
我打造过一款从 0 到 1 的信用分产品，目前仍在金融领域被千万级用户的公司持续使用，而我也是依靠该项目拿到了大厂的 Offer；我也从 0 到 1 摸索过电商领域的亿级用户精细化运营，并且沉淀了一套从渠道到流失用户运营的方法论，目前仍然在多家公司被持续使用；后来，我还负责过工具产品的规模增长和变现，带来规模的 2 倍和收入的 3 倍增长，在业内增长大会上有过多次分享。
工作之余，我经常思考如何能够在“数据为王”的时代，帮助更多的人了解数据分析工作和提升数据分析能力，思索再三，我决定通过专栏的形式，体系化地输出自己的方法和经验。同时，我也希望这个课程可以帮助你以终为始，更好地规划自己的成长路径。
数据分析 = 分析工具 + 分析思维 工作以后我们发现，数据分析行业不乏因为好奇和薪酬而毕业入行的从业者。
然而进入职场后，却发现现实与理想相差甚远，原本非常“有意思”的工作现在变得&amp;quot;让人恼火&amp;rdquo;：学了各种工具，写了很多代码，工作中最常用的却还是按照条件导出数据，自嘲像个&amp;quot;提数机器&amp;rdquo;，没有成就感&amp;hellip;&amp;hellip; 工作热情也一天天消失。
我认为，上面的问题可以归为以下三类。
问题1：缺乏对业务的理解，更多的是被动做事。
很多分析师在做事的时候，充当老好人，别人提什么都做，上班非常忙，成为别人口中的好好先生。自己也想自我提升，但没时间，因为你的时间掌握在了别人手中。
 简单测试一下你：今年产品团队的目标具体是多少，有哪些战略打法，你看过这份报告吗？
 问题2：对数据分析的理解片面化，更多的还是停留在工具层面。
现在很多高校都在开数据分析这个专业，我看过学生们学的课程，像 Python、Tableau、Java 这些工具都有，实际上是有问题的。大数据是很好，但如果不解决业务问题，你玩得再花，都是空架子。现在很多公司面试的时候都写了一大堆代码要求，这本身就是外行人。
问题3：缺乏引路人，想提升但找不到导师
很多悟性不错的同学，在思考自身能力提升的同时，会去和职场上的老同事或者领导沟通，希望得到一些指导。但沟通后发现他们更多还是一些工具层面的指导，对思维的解惑不多，或者也是在重复类似的工作场景，没有更深入地思考问题。以专题报告为例，你发给对方希望他们给些意见，得到的却更多是报告格式、文字描述、图形可视化上的一些意见，对分析套路的建议却非常少。
我想说的是，数据分析不是简单的工具使用和重复的数据处理，数据分析的本质是：从大量事物中发现关键信息，用于直接决策，而不是辅助。
但是，市面上的数据分析资料也多以各类技术工具讲解为主，部分人对数据分析的认知还停留在 SQL 和其他工具操作阶段；很多中小企业的数据分析从业者，又限于自身业务场景问题，无法在本职工作中得到锻炼和成长；想要提升能力、求职体验新鲜的工作内容，却又不了解心仪岗位的要求，不知从哪下手，面试求职又屡屡败北。
怎么更好地解决这些问题呢？
课程设计 在这个课程中，我会依循大多数人学习新技能的方法路径，通过“找定位、扩思维、精方法、知流程、找不足、寻突破”这样一个流程，来带你全方位掌握数据分析。
具体来说，课程分为 5 个模块，23 篇文章：
 模块一，数据分析的行业需求与要求。我会从不同企业的业务类型着手分析，带你掌握不同企业要求的数据分析基本技能。学完这个部分，不论你在从事什么类型的业务，都能找到属于自己的数据化思维与方法。 模块二，拓展你的宏观视野：通过 4 大行业（电商、互联网金融、游戏、传统行业）的知名案例，讲述数据分析思维模型。该模块最大的亮点就是案例实战，比如电商是怎么做数据分析的，游戏又是怎么做的，很多案例你都可以直接去套。同时，我还会给出优秀数据分析人员的能力模型和 4 个评价指标。对应能力模型，你很容易知道一个人处于什么段位。 模块三，聚焦微观方法论：聚焦不同业务分析的分析框架，讲解关键阶段动作，比如流量分析、路径分析、竞品分析、活动分析、用户增长分析等核心操作，带你掌握数据分析的微观方法论。 模块四，知流程，找不足。因为专题报告就是分析师对外推广自己的产品，而一份专题报告实际上是有一套标准化流程的，像问题的定义与拆解、数据的获取与拆解、专题报告的撰写与落地，以及AB测试等。学完后，你就知道怎么写完美的专题报告了。 模块五，人人都是数据分析师：除了以上数据分析的思维与方法，你还需要提升比如行业分析、数据仓库研究、用户研究、时间管理等专业素养，这些可以解决你对于数据分析至关重要的一些大问题，让你具备一个优秀数据分析者的专业素养。  可以看到，这门课基本上没有工具的讲解，都是针对一些具体产品，结合案例来说明数据分析是怎么帮助产品进行优化的。只有这样，每个数据分析从业者才能掌握这个职业的精髓。</description>
    </item>
    
    <item>
      <title>数据仓库之范式理论</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-13-the-concept-of-fanshi/</link>
      <pubDate>Mon, 13 Jul 2020 17:02:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-13-the-concept-of-fanshi/</guid>
      <description>范式概念 关系型数据库设计时，遵照一定的规范要求，目的在于降低数据的冗余性，目前业界范式有：第一范式(1NF)、 第二范式(2NF)、 第三范式(3NF)等。
使用范式的根本目的是：
 减少数据冗余，尽量让每个数据只出现一次 保证数据一致性  缺点是获取数据时，需要通过 join 拼接出最后的数据。
函数依赖    学号 姓名 系名 系主任 课程 分数     1 小明 经济系 王强 高等数据 95   1 小明 经济系 王强 大学英语 87   1 小明 经济系 王强 普通化学 76   2 小莉 经济系 王强 高等数据 72   2 小莉 经济系 王强 大学英语 98   2 小莉 经济系 王强 计算机 88   3 小芳 法律系 刘玲 高等数学 82   3 小芳 法律系 刘玲 法学基础 82    完全函数依赖 (学号，课程)推出分数，但是单独用学号推断不出来分数，那么就可以说：分数完全依赖于(学号，课程)。</description>
    </item>
    
    <item>
      <title>B站优秀Up主记录</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-10-the-up-of-bilibili/</link>
      <pubDate>Fri, 10 Jul 2020 23:11:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-10-the-up-of-bilibili/</guid>
      <description>高校类 华中科技大学  首页 2018-12-17 致敬所有在考研路上的逐梦人 2018-06-15 2018华中科技大学原创毕业主题曲MV —— 年少路远，远方再见 2018-06-10 这一次，笑着离开【2018华中科技大学毕业季大电影】《同学，帮帮忙》首映预告出炉！ 2018-03-07 2018女生节 | 看完这个采访的女生都哭了 2018-03-02 【IN HUST】——情深不自知 2018-02-28 【华中科技大学2018毕业电影】花絮|二轮试镜 2018-02-28 【2018华中科技大学毕业电影】花絮|演员大骗局 2017-11-11 一首《华科男，别哭》送给大家 | 双十一献礼 2017-09-15 「授权填词翻唱」愿得一人心HUST版 毕业献礼，祝愿你半生归来还是那个人 2017-09-15 江城几处清佳，喻山共赏「玉兰辞」 2017-09-15 妈妈，我想对你说…… 2017-09-15 「小科街采」震惊！HUSTer了解华师居然是因为…… 2017-09-15 高考加油，我们在HUST等你  </description>
    </item>
    
    <item>
      <title>中华经典唐诗之王维</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-11-the-poem-of-wangwei/</link>
      <pubDate>Fri, 10 Jul 2020 23:11:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-11-the-poem-of-wangwei/</guid>
      <description>王维（701－761年，一说699－761年），字摩诘，号摩诘居士。河东蒲州（今山西运城）人，祖籍山西祁县。唐朝诗人、画家。
送 别  下马饮君酒，问君何所之？
君言不得意，归卧南山陲。
但去莫复问，白云无尽时。
 山居秋暝  空山新雨后，天气晚来秋。
明月松间照，清泉石上流。
竹喧归浣女，莲动下渔舟。
随意春芳歇，王孙自可留。
 参考文献  诗词名句网——王维  </description>
    </item>
    
    <item>
      <title>英语单词记忆</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-09-the-english-new-words/</link>
      <pubDate>Thu, 09 Jul 2020 15:20:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-09-the-english-new-words/</guid>
      <description>A开头的 abode n. 房屋，家，住所 
搭 right of abode 居住权 
eg. I do not dare to linger in that gloom-hidden abode.
我不敢在那个隐没于黑暗中的房屋里逗留。 
E开头的 exotic n. 外来的，奇异的，醒目的，吸引人的 
搭 exotic experience 异国体验 
同 foreign(a. 外国的，外来的)
反 native(a. 本国的，本地的); indigenous(a. 本土的)
 evaporate v. (使)蒸发，消失，不复存在 
P开头的 privilege n. 特权，优惠 vt. 给予特权，特别优待 
eg. Education is not a privilege of some people, but a right of all citizens.
受教育不是某些人的特权，而是所有公民的权利。</description>
    </item>
    
    <item>
      <title>数据开发岗位面试准备</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-04-the-jd-of-data-analysis/</link>
      <pubDate>Sat, 04 Jul 2020 21:01:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-04-the-jd-of-data-analysis/</guid>
      <description>存储过程 什么是存储过程？  存储过程是一个预编译的 SQL 语句 优点是允许模块化的设计，就是说只需创建一次，以后在该程序中就可以调用多次 如果某次操作需要执行多次 SQL，使用存储过程比单纯 SQL 语句执行要快  用什么来调用？  可以用一个命令对象来调用存储过程 可以供外部程序调用，比如：java 程序  存储过程的优缺点？ 优点：
 存储过程是预编译过的，执行效率高 存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯 安全性高，执行存储过程需要有一定权限的用户 存储过程可以重复使用，可减少数据库开发人员的工作量  缺点：
 移植性差 过程化编程，复杂业务处理的维护成本高 调试不便  hive 分区和分桶的区别 分区 partition
 划分数据集，通过分区减少每次扫描的总数据量 分区使用 hdfs 的子目录功能实现，每个子目录都包含了分区对应的列名和每一列的值，但是 hdfs 并不支持大量的子目录，所以分区的数量是有限制的，要先对表中分区数量进行预估，从而避免分区数量过大带来的问题 分区的划分是非随机的  分桶 bucket
 在分区数量过于庞大以至于可能导致文件系统崩溃时使用分桶来解决问题 分桶是通过对指定列进行哈希计算来实现，使用列的哈希值对数据打散，然后分发到不同的桶中从而完成数据的分桶 在数据量够大的情况下，分桶比分区更有查询效率 分桶的划分是随机的  </description>
    </item>
    
    <item>
      <title>牛客网错题集</title>
      <link>https://xiaohao890809.github.io/2020/2020-06-30-the-wrong-problems-of-nowcoder/</link>
      <pubDate>Tue, 30 Jun 2020 21:43:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-06-30-the-wrong-problems-of-nowcoder/</guid>
      <description>知识类 夏天，打开冰箱冷冻室的门，常常看到冷冻室中冒出一股白雾，这是
A. 冰箱里原有水蒸气凝结成小水滴
B. 冰箱里的冰升华后凝结成小水滴
C. 冰箱里的水变成水蒸气
D. 冰箱外部空气中的水变成小水滴
正确答案：D 
打开冰箱冷冻室的门时，冰箱内的低温气体飘散到冰箱外。使周围空气中平时看不见的水蒸气迅速冷却液化，成为很多微小水珠形成了&amp;quot;白雾&amp;rdquo;。
 关于宇航员在太空中的生活，下列说法中不正确的是：
A. 宇航员可以使用特定的加热器对食品加热
B. 宇航员从太空返回地面之后，由于失重，质量会有所增加
C. 宇航员应该睡在固定的睡袋中，以免被气流推动误碰仪器设备开关
D. 在同一航空器中的宇航员可以直接交谈，无需借助无线电通信设备
正确答案：B 
质量是物体的本质属性，在哪都不会改变 
技术类 请问下面哪个赋值语句不是合法的()
A. float a = 3.0
B. int c = 3
C. long d = 3
D. double b = 3.0
正确答案：A  `float a = 3.0f` 
 请阅读下面代码
public class HelloWorld { public static void main(String[] args) { Integer f1 = 100, f2 = 100, f3 = 150, f4 = 150; } } 请问以下哪些判断会返回false()</description>
    </item>
    
    <item>
      <title>居士自习室作业第五周</title>
      <link>https://xiaohao890809.github.io/2019/2019-08-18-the-data-science-learning_5/</link>
      <pubDate>Sun, 18 Aug 2019 12:51:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2019/2019-08-18-the-data-science-learning_5/</guid>
      <description>最常用的统计量与抽样分布
统计量
  样本均值：样本均值(sample mean)又叫样本均数，即为样本的均值。均值是表示一组数据集中趋势的量数，是指在一组数据中所有数据之和再除以这组数据的个数。它是反映数据集中趋势的一项指标。
  样本方差：先求出总体各单位变量值与其算术平均数的离差的平方，然后再对此变量取平均数，就叫做样本方差。样本方差用来表示一列数的变异程度。
  变异系数：在概率论和统计学中，变异系数，又称“离散系数”(coefficient of variation)，是概率分布离散程度的一个归一化量度，其定义为标准差与平均值之比。变异系数也被称为标准离差率或单位风险。
  样本矩：有一类常用的统计量是样本的数字特征，他们是模拟总体数字特征构造的，称为样本矩，看看以下两种：
 样本 $k$ 阶原点矩：$\bar{\alpha}_{k}=\frac{1}{n}\sum_{i=1}^{n}x_{i}^{k}$ 样本 $k$ 阶中心矩：$\bar{\beta}_{k}=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^{k}$    样本偏度：样本偏度(sample skewness)一种基本统计量，是样本三阶中心矩除以样本二阶中心矩的3/2次幂的商。
  样本峰度：样本峰度(sample kurtosis)一种基本统计量，样本的峰度和偏度都是作为检验总体分布正态性的统计量。是样本四阶中心矩除以样本二阶中心矩平方的商再减去3。
  抽样分布
  卡方分布：若 $n$ 个相互独立的随机变量 $\xi_{1},\xi_{2},&amp;hellip;,\xi_{n}$ 均服从标准正态分布(也称独立同分布于标准正态分布)，则这 $n$ 个服从标准正态分布的随机变量的平方和构成一新的随机变量，其分布规律称为卡方($\chi^{2}$)分布(chi-square distribution)。
  T分布：假设 $X$ 服从标准正态分布$N(0,1)$，Y服从 $\chi^{2}(n)$ 分布，那么 $Z=\frac{X}{\sqrt{Y/n}}$ 的分布称为自由度为 $n$ 的 T分布,记为$Z\sim t(n)$。
  F分布：若总体$X\sim N(0,1)$，$(X_{1},X_{2},&amp;hellip;,X_{n_{1}})$与$(Y_{1},Y_{2},&amp;hellip;,Y_{n_{2}})$来自 $X$ 的两个独立样本，设统计量$F=\cfrac{\sum_{i=1}^{n_{1}}X_{i}^{2}}{N_{1}}/\cfrac{\sum_{i=1}^{n_{2}}Y_{i}^{2}}{N_{2}}$，则称统计量 $F$ 服从自由度 $n_{1}$ 和 $n_{2}$ 的 F分布，记为$F\sim F(n_{1},n_{2})$。</description>
    </item>
    
    <item>
      <title>居士自习室作业第二周</title>
      <link>https://xiaohao890809.github.io/2019/2019-07-28-the-data-science-learning_2/</link>
      <pubDate>Sun, 28 Jul 2019 17:42:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2019/2019-07-28-the-data-science-learning_2/</guid>
      <description>在用 pandas 处理报表邮件的时候，有时会遇到处理二级表头，并进行合并的例子。例如：
研究了一下 pandas 的多级表头功能，发现它有一个 columns 的设置参数，可以新增两列参数，作为二级表头。
data = [[&amp;#39;2016-07-23&amp;#39;,&amp;#39;整体&amp;#39;,4540,557.34,1235,469,69.23,334], [&amp;#39;2016-07-22&amp;#39;,&amp;#39;整体&amp;#39;,4410,567.34,1135,459,68.23,324], [&amp;#39;2016-07-21&amp;#39;,&amp;#39;整体&amp;#39;,4380,564.34,1115,439,67.23,314]] data_pd = pd.DataFrame(data) data_pd.columns = [[&amp;#39;日期&amp;#39;, &amp;#39;品类&amp;#39;, &amp;#39;下单&amp;#39;, &amp;#39;下单&amp;#39;, &amp;#39;下单&amp;#39;, &amp;#39;支付&amp;#39;, &amp;#39;支付&amp;#39;, &amp;#39;支付&amp;#39;], [&amp;#39;日期&amp;#39;, &amp;#39;品类&amp;#39;, &amp;#39;下单笔数&amp;#39;, &amp;#39;下单金额&amp;#39;, &amp;#39;下单人数&amp;#39;, &amp;#39;成功笔数&amp;#39;, &amp;#39;成功金额&amp;#39;, &amp;#39;成功人数&amp;#39;]] 二级表头处理
发现第一列和第二列并没有合并，下单和支付的大类也没有居中显示。那么怎么样才能实现这样的功能呢？其实 html 里的表格有一个 colspan 参数，我们可以对这个参数进行修改即可，比如，我们可以手动给第二行表头的日期和品类改为删除标识，然后对其进行删除，最后把第一行表头的 colspan 扩展为2，就可以进行合并了，也不影响整体表格的功能，至于居中样式可以设置表格的 style 样式。
# 重新自定义html的格式 def get_type_html(df_html): html = str(df_html).replace(&amp;#39;&amp;lt;table border=&amp;#34;1&amp;#34; class=&amp;#34;dataframe&amp;#34;&amp;gt;&amp;#39;, &amp;#39;&amp;lt;table border=&amp;#34;1&amp;#34; style=&amp;#34;font-family: verdana,arial,sans-serif;font-size:11px;\ color:#333333;border-width: 1px;border-color: #666666;border-collapse: collapse;&amp;#34;&amp;gt;&amp;#39;) html = html.replace(&amp;#39;&amp;lt;td&amp;gt;&amp;#39;, &amp;#39;&amp;lt;td colspan=&amp;#34;1&amp;#34; rowspan=&amp;#34;1&amp;#34; style=&amp;#34;background-color:#FFFFFF;color:#000000;font-weight:normal;\ padding:10px;text-align:center;white-space:pre&amp;#34;&amp;gt;&amp;#39;) &amp;#34;&amp;#34;&amp;#34; html = html.</description>
    </item>
    
    <item>
      <title>居士自习室作业第一周</title>
      <link>https://xiaohao890809.github.io/2019/2019-07-20-the-data-science-learning_1/</link>
      <pubDate>Sat, 20 Jul 2019 22:42:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2019/2019-07-20-the-data-science-learning_1/</guid>
      <description>数据的集中趋势 众数 一组数据中出现次数最多的数值，叫众数。
 注意：有时众数在一组数中有1个或多个，也可能不存在。
 中位数 中位数，又称中点数，中值。
中位数是按顺序排列的一组数据中居于中间位置的数，即在这组数据中，有一半的数据比他大，有一半的数据比他小。
平均数   算数平均数
 算术平均数是指在一组数据中所有数据之和再除以数据的个数。 它是反映数据集中趋势的一项指标。    加权平均数
 加权平均数即将各数值乘以相应的权数，然后加总求和得到总体值，再除以总的单位数。 加权平均数也称加权平均值。    几何平均数
 几何平均数是n个变量值连乘积的n次方根。    分位数 分位数(Quantile)，亦称分位点，是指将一个随机变量的概率分布范围分为几个等份的数值点。
常用的有中位数(即二分位数)、四分位数、百分位数等。
数据的离中趋势 数值型数据   方差
 统计中的方差(样本方差)是每个样本值与全体样本值的平均数之差的平方值的平均数。    标准差
 标准差是方差的算术平方根。    极差
 极差又称范围误差或全距(Range)，以R表示，是用来表示统计资料中的变异量数(measures of variation)。 其最大值与最小值之间的差距，即最大值减最小值后所得之数据。    平均差
 平均差(Mean Deviation)是表示各个变量值之间差异程度的数值之一。 指各个变量值同平均数的离差绝对值的算术平均数。    顺序数据  四分位差  四分位差(quartile deviation)，它是上四分位数(Q3，即位于75%)与下四分位数(Q1，即位于25%)的差。    分类数据  异众比率  异众比率(variation ratio)是统计学名词，是统计学当中研究现象离中趋势的指标之一。 异众比率指的是总体中非众数次数与总体全部次数之比。 换句话说，异众比率指非众数组的频数占总频数的比例。    相对离散程度 离散系数 在概率论和统计学中，离散系数(coefficient of variation)，是概率分布离散程度的一个归一化量度，其定义为标准差 $\sigma$ 与平均值 $\mu$ 之比。</description>
    </item>
    
    <item>
      <title>机器学习基础之概率论</title>
      <link>https://xiaohao890809.github.io/2018/2018-03-07-the-note-of-probability-theory/</link>
      <pubDate>Wed, 07 Mar 2018 23:54:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2018/2018-03-07-the-note-of-probability-theory/</guid>
      <description>机器学习算法中有很多用到概率论知识的地方，比如贝叶斯定理，频繁挖掘，条件概率等，掌握好概率论的基础知识有利于更好的理解机器学习中的相关算法。
三箱零件，其中第一箱 10 个零件，第二箱 20 个零件，第三箱 15 个。检验结果表明第一箱有 1 个不合格，第二箱有 3 个不合格，第三箱有 2 个不合格，从中抽取一个零件，合格的概率有多少？
 记 $A_i$ 为从第 $i$ 个箱子拿的零件，拿到正品的事件记为B。
 $$ \begin{align} P(B)&amp;amp;=P(A_1)\cdot P(B|A_1)+P(A_2)\cdot P(B|A_2)+P(A_3)\cdot P(B|A_3)\\\
&amp;amp;=P(A_1)+P(A_2)\cdot P(B|A_2)+P(A_3)\cdot P(B|A_3)\\\
&amp;amp;=\frac{1}{3}\cdot \frac{9}{10}+\frac{1}{3}\cdot \frac{17}{20}+\frac{1}{3}\cdot \frac{13}{15}\\\
&amp;amp;=0.872 \end{align} $$
问题：求逆向概率$P(A_{1}|B)$，抽到的这个合格品来自箱子 $A_{1}$ 的概率。
$$ \begin{align} P(A_{1}|B)&amp;amp;=\frac{P(A_1\cdot B)}{P(B)}\\\
&amp;amp;=\frac{P(A_1)\cdot P(B|A_1)}{P(B)}\\\
&amp;amp;=\frac{\frac{1}{3}\cdot \frac{9}{10}}{P(B)}\\\
&amp;amp;=\frac{0.3}{0.872}=0.344 \end{align} $$
注意条件概率 $P(B|A_1)$ 和联合概率 $P(A_1\cdot B)$ 的使用。
分类问题概述： 通过对已知类别信息的数据进行学习后获得分类模型(classifier)，利用分类模型对未知类别信息的数据进行分类(classification)。
朴素贝叶斯 分类算法举例——朴素贝叶斯(Naive Bayesian Model, NBM)
$$ \begin{cases} P(C_i|X)=\frac{P(X|C_i)\cdot P(C_i)}{P(X)}\\
P(X|C_i)=\prod_{K=1}^{n}P(X_k|C_i)\\
P(AB)=P(A)\cdot P(B)\\</description>
    </item>
    
    <item>
      <title>VBA知识点总结</title>
      <link>https://xiaohao890809.github.io/2018/2018-03-01-the-note-of-vba/</link>
      <pubDate>Thu, 01 Mar 2018 22:53:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2018/2018-03-01-the-note-of-vba/</guid>
      <description>VBA是一门比较早的语言了，用于处理 Office 办公软件的数据，不过最近听说以后会被 Python 代替（笑脸），现在总结一些使用 VBA 的过程遇到的一些问题以及解决办法。
常见错误 问题1  此文件正由应用程序或另一用户使用。
 解决方案：打开后记得做退出关闭操作。
Set wdApp = GetObject(,&amp;#34;word.application&amp;#34;) If wdApp is Nothing Then Sef wdApp = CreatObject(&amp;#34;word.application&amp;#34;) wdApp.Visible = True End If wdApp.NormalTemplate.Saved = True wdApp.Quit Set wdApp = Nothing 常用函数 获取最大有效行数 Public Function GetLastRow(theSheet As Worksheet, ByVal col As Integer) As Integer Dim findreg As Range, ret As Range Set findreg = theSheet.Columns(col) Set ret = findreg.Find(what:=&amp;#34;*&amp;#34;, searchDirection:=xlPrevious) If Not ret Is Nothing Then GetLastRow = ret.</description>
    </item>
    
    <item>
      <title>正则表达式知识积累</title>
      <link>https://xiaohao890809.github.io/2018/2018-02-27-the-study-of-reg-expression/</link>
      <pubDate>Tue, 27 Feb 2018 23:07:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2018/2018-02-27-the-study-of-reg-expression/</guid>
      <description>众所周知，正则表达式是非常重要的一个模块，在爬虫中用的好的话可以事半功倍，从复杂文本中根据规则去筛选数据等，掌握常用的一些正则通配符，从此刻开始。
正则表达式概念  使用单个字符串来描述匹配一系列符合某个句法规则的字符串 是对字符串操作的一种逻辑公式 应用场景：处理文本和数据  举例分析 大括号 匹配中括号中有任何一个字符，匹配里面的或者情况
ma = re.match(r&amp;#39;\[[\w]\]&amp;#39;,&amp;#39;[a]&amp;#39;) ret = re.findall(r&amp;#39;[abc]f&amp;#39;,&amp;#39;afufobfidlfodcfr&amp;#39;) # [&amp;#39;af&amp;#39;, &amp;#39;bf&amp;#39;, &amp;#39;cf&amp;#39;] 含有换行 正则修饰符re.S可以匹配包括换行在内的所有字符
import re content = &amp;#39;&amp;#39;&amp;#39;Hello 1234567Word-This is a Regex Demo&amp;#39;&amp;#39;&amp;#39; # 非贪婪匹配 result = re.match(&amp;#39;^He.*?(\d+).*?Demo$&amp;#39;, content, re.S) print(result.group(1)) # 1234567 贪婪模式和非贪婪模式 import re # 贪婪模式，最大范围的匹配标准 ret = re.findall(r&amp;#39;&amp;lt;div&amp;gt;(.*)&amp;lt;/div&amp;gt;&amp;#39;,&amp;#39;&amp;lt;div&amp;gt;hello&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;hello&amp;lt;/div&amp;gt;&amp;#39;) print(ret) # [&amp;#39;hello&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;hello&amp;#39;] # 非贪婪模式 ret = re.findall(r&amp;#39;&amp;lt;div&amp;gt;(.*?)&amp;lt;/div&amp;gt;&amp;#39;,&amp;#39;&amp;lt;div&amp;gt;hello&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;hello&amp;lt;/div&amp;gt;&amp;#39;) print(ret) # [&amp;#39;hello&amp;#39;, &amp;#39;hello&amp;#39;] ma = re.match(r&amp;#39;[0-9][a-z]*&amp;#39;,&amp;#39;1bc&amp;#39;) #全部匹配 print(ma.group()) #1bc ma = re.</description>
    </item>
    
    <item>
      <title>VBA实现Excel的笛卡尔积</title>
      <link>https://xiaohao890809.github.io/2018/2018-01-12-the-decare-by-excel/</link>
      <pubDate>Fri, 12 Jan 2018 23:40:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2018/2018-01-12-the-decare-by-excel/</guid>
      <description>最近有一个任务需求，就是把 Excel 里的两列元素，里面的元素有多个信息，将其分别拆分，然后取笛卡尔积，写到一个新的工作簿中。刚开始准备用 Python 实现的，后来想了下，VBA作为 Office 工具的原生态语言，何不用 VBA 实现呢，于是整理了下便写出来了，下面介绍下过程，为方便起见，弄了一些简易的数据，针对不同的数据需要做一些相应的调整。
Excel的原始数据如下：
整个 VBA 的代码框架如下：
类模块 原始数据对应的首行信息，每一列对应一个元素，将其列出，作为一个新的数组。
Public a As String Public b As String Public c As String Public d As String 常量 Public Const maxNum = 100000 Public MyArr(maxNum) As New MyAttr Public MyNum As Integer 主函数 思路是分别拆分每一行的那两列元素，然后将得到的元素做两个循环，遍历写入新的数组中，然后将新的数组传到工作簿中。
获取最大行数 Public Function GetLastRow(theSheet As Worksheet, ByVal col As Integer) As Integer Dim findreg As Range, ret As Range Set findreg = theSheet.</description>
    </item>
    
    <item>
      <title>经典排序法之Python版</title>
      <link>https://xiaohao890809.github.io/2018/2018-01-11-the-classic-sorts/</link>
      <pubDate>Thu, 11 Jan 2018 23:11:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2018/2018-01-11-the-classic-sorts/</guid>
      <description>大学的时候学习的经典算法忘记差不多了，现在赶紧拾起来，基本算法往往是复杂算法的基础，牢记掌握才是王道。
冒泡排序 def BubbleSorted(nums): length = len(nums) for i in range(length-1): for j in range(length-i-1): if nums[j]&amp;gt;nums[j+1]: nums[j],nums[j+1] = nums[j+1],nums[j] return nums 思考
如果原来的列表是有序列表呢，能否优化一下呢？
def bubble_sort(input_list): &amp;#34;&amp;#34;&amp;#34;冒泡排序&amp;#34;&amp;#34;&amp;#34; length = len(input_list) for j in range(length - 1): # 当列表已经是有序列表的，节省空间 count = 0 for i in range(length - j - 1): if input_list[i] &amp;gt; input_list[i+1]: input_list[i], input_list[i+1] = input_list[i+1], input_list[i] count += 1 if count == 0: return 属性：
 最优时间复杂度：$O(n)$ (表示遍历一次没有发现任何可以交换的元素，排序结束) 最坏时间复杂度：$O(n^2)$ 稳定性：稳定  插入排序 def InsertSorted(nums): # 从第二元素开始直到最后一个元素 for i in range(1,len(nums)): tmp = nums[i] print(nums) j = i-1 while j &amp;gt;= 0 and nums[j] &amp;gt; tmp: nums[j+1] = nums[j] j = j - 1 nums[j+1] = tmp return nums 选择排序 def SelectSorted(nums): for i in range(len(nums)-1): minIndex = i for j in range(i+1,len(nums)): if nums[j] &amp;lt; nums[minIndex]: minIndex = j nums[i],nums[minIndex] = nums[minIndex],nums[i] return nums 图片来源：常用算法js版</description>
    </item>
    
    <item>
      <title>保持某些好的习惯</title>
      <link>https://xiaohao890809.github.io/2017/2017-09-27-some-good-habits-need-to-insist/</link>
      <pubDate>Tue, 26 Sep 2017 10:45:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2017/2017-09-27-some-good-habits-need-to-insist/</guid>
      <description>优秀是一种习惯，保持一个良好的习惯往往能引导一种健康的生活姿态，现在记录一些自己认为是比较好的一些习惯，自己经常去看看，用于监督和督促自己。
生活  一周至少运动两次（跑步，爬山，打球等） 晚上11点半准备看书，然后睡觉 一天至少三大杯水 平时多吃点水果 一个月清理一次房间 晚上睡前刷牙  技术  代码写完后多检查下注释有没有写全，没写的补上，以免以后都不知道自己当初写的啥 LeetCode过三遍 js和go语言了解下 正则表达式多熟悉  反思  每天问一遍自己，想进BAT吗，以你现在的能力能进BAT吗，不能的话哪些地方需要加强呢？  工作  平时多看下面试相关的信息，刷刷题 把跟自己业务相关的数据库表多检查下  英语  熟读新概念英语第三册前30篇  </description>
    </item>
    
    <item>
      <title>记录博客的第一天</title>
      <link>https://xiaohao890809.github.io/2015/2015-08-14-first-day/</link>
      <pubDate>Fri, 14 Aug 2015 22:47:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2015/2015-08-14-first-day/</guid>
      <description>好久不见，大家好！很高兴，今天终于开通了这个博客，这期间也得到了不少网友的帮助。之前在网上找了很多模板，但是选来选去，最终还是定了这个模板，因为这个模板比较简洁，而且也符合我预期的效果。
首先要感谢 enml，我是引用了他的模板。
先把他的模板下载到本地，然后传到自己建立的仓库下就可以进行修改模板了。将一些基本的信息，比如名字，多说的 id 以及一些图标都加以修改即可。刚开始的模板的导航栏上没有留言板和关于我这一栏，我后来自己补上的。之前模板的一个出口的 page 被我删掉了，因为感觉那个功能不太实用。还有原来的评论系统貌似用的是国外的一个系统，我将其改为了多说的评论系统，非常好用！
其次是感谢 liberize 的帮助
在他的帮助下，我在评论框的头像加上了炫酷的旋转效果，看起来非常漂亮。只是原作者是圆形的图标，而我这个背景如果用了圆形的头像，会有一个阴影在下面，看起来非常不舒服，于是我改为了方形的效果，默认下的头像旋转是没有阴影的，这一点我至今还是很疑惑。
再者感谢 tk 域名
博客搭建好了，但是域名太长，不太方便随时输入。所以也百度了很多资料，看到网上都是说加一个 CNAME 文件，然后把域名加上去，在去域名管理页面加一个 A 地址。折腾了半天也没有成功。最后发现前提应该是自己得有一个 .com 或者 .me 域名，但是我木有。于是乎这条路便走不通了。后来无意间看到 tk 域名的网址，听说进去可以免费注册域名，于是抱着试一试的态度进去了，后来发现其实根本不用那么麻烦，只有把自己的博客地址指向到你要注册的 .tk 域名就 ok 了。省去了很多步骤。而且最后的网页比之前的网页更加完整了，之前没显示出来额头像图标和 github 图标都显示出来了，太惊喜了。只是某些字体不知道怎么回事，还是显示不完美。不过这样已经很满足了。
最后感谢 liberize 和百度文库的帮助
模板本来已经弄得差不多了，后来想来想去看了别人的很多模板，都在首页有分页的功能（PS：分类和标签是没有这个功能的），于是自己也想加上，所以结合了好几个人的模板，最后把每一个标签换了个颜色，显得不是那么单调。最后文章也要截取部分显示在首页中，这部分调式了好久，最后用了下面这个代码搞定了。
post.content | truncate:300 其实就是把中间一部分舍掉了，因为加上中间那一部分的话，是按照文本进行分割的，而我想保留自己原来的格式，于是只取后面一部分就搞定了。当理想的界面出现在 html 上的时候，那个感觉非常好。也许博客到现在格式方面已经差不多了。不过到后面估计还得修改一些小细节，不断地进步才有动力前进！
总结
github 真是个不错的平台，幸好天朝没有进行封杀，里面的资源和牛人特别多。他们秉着开源的精神，无私地分享了自己得许多经验和代码。所以，学无止境，多学习，多总结肯定是没有错的。
但无论如何，万事开头难，希望自己能够坚持下去，记录生活点滴，同时也写一些技术类的博客，虽然比较菜，但是进步空间比较大，加油，耗子！
另外本博客已经使用 hexo 主题，之前的 jekyll 已经不用了。因为这个框架的主题配置起来比较容易，不用修改很多地方。</description>
    </item>
    
  </channel>
</rss>