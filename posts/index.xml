<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 枕霞惜友</title>
    <link>https://xiaohao890809.github.io/posts/</link>
    <description>Recent content in Posts on 枕霞惜友</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Jul 2020 10:41:48 +0000</lastBuildDate>
    
	<atom:link href="https://xiaohao890809.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>第29讲：项目背景和实时处理系统架构设计</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-30/</link>
      <pubDate>Mon, 20 Jul 2020 10:41:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-30/</guid>
      <description>本从这一课时开始我们进入“Flink 实时统计 PV、UV”项目的学习。本课时先介绍实时统计项目的背景、架构设计和技术选型。
背景 PV（Page View，网站的浏览量）即页面的浏览次数，一般用来衡量网站用户访问的网页数量。我们可以简单地认为，一个用户每次打开一个页面便会记录一次 PV，也就是说，PV 是指页面刷新的次数，刷新一次页面，记录一次 PV。
在互联网项目中，PV 的度量方法是指发起一次请求（Request）从浏览器到网络服务器，网络服务器在接收到请求后会将对应的网页返回给访问者，这个过程就是一次 PV。
UV（Unique Visitor，独立访客次数）是一天内访问某个站点的人数。一天内同一个用户不管有多少次访问网站，那么只记录一次 UV。一般来讲，会通过 IP 或者 Cookie 来进行判断。
如果网站的一个页面有 100 次访问，其中有一些访问者多次访问和点击，那么页面的 UV 一定是小于 100 的。
PV 和 UV 是我们业务中十分常见的场景，对于这种需求一般是通过什么样的技术架构实现呢？
技术选型和整体架构 在互联网常见的后端应用中，技术选型可能多种多样，比如基于 Spring、Dubbo、Spring Cloud 等，但是基本的处理框架大同小异。最常见的数据就是日志数据，如果是 Web 应用，那么我们的日志数据就是用户的访问日志或者用户的点击日志等。
从上图中可以看到有几个关键的处理步骤：
 日志数据如何上报 日志数据的清洗 实时计算程序 前端展示  所以基于以上的业务处理流程，我们常用的实时处理技术选型和架构如下图所示：
在上图的架构图中，涉及几个关键的技术选型和处理步骤，我们下面一一进行讲解。
日志数据埋点 “埋点”是数据采集领域中的一个术语，特别是采集用户行为数据，针对用户的某些事件和行为进行捕捉、处理并发送的过程。
埋点数据是十分重要的数据，也可用来进行：
 流量监控、页面访问的热点分析 构建用户的行为数据，获取用户整个访问的路径和习惯 可以根据用户的访问数据进行偏好分析、个性化推荐 AB 测试 ……  数据埋点的方式多种多样，可以在页面中使用 JavaScript 埋点开发，也可以简单地通过 Nginx 代理服务器进行日志的规范化。
当前大型网站的架构一般都是通过 Nginx 等服务器做网管和代理，Nginx 服务器本身支持日志格式的定制化。
服务端也可以先定义好全局的规范化日志，然后再进行采集，可以更加精细化的定制，满足精细化分析的需求。
我们在本案例中的场景是计算 PV 和 UV 数据，所以数据来源最有可能的是 Nginx 定制化的日志格式，例如：</description>
    </item>
    
    <item>
      <title>第28讲：TopN 热门商品功能实现</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-29/</link>
      <pubDate>Mon, 20 Jul 2020 10:40:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-29/</guid>
      <description>本课时主要讲解 Flink 中的 TopN 功能的设计和实现。
TopN 在我们的业务场景中是十分常见的需求，比如电商场景中求热门商品的销售额、微博每天的热门话题 TopN、贴吧中每天发帖最多的贴吧排名等。TopN 可以进行分组排序，也可以按照需要全局排序，比如若要计算用户下单总金额的 Top 10 时，就需要进行全局排序，然而当我们计算每个城市的 Top10 时就需要将订单按照城市进行分组然后再进行计算。
下面我们就详细讲解 TopN 的设计和实现。
整体设计 我们下面使用订单数据进行讲解，整体的数据流向如下图所示：
订单数据由业务系统产生并发送到 Kafka 中，我们的 Flink 代码会消费 Kafka 中的数据，并进行计算后写入 Redis，然后前端就可以通过读取 Redis 中的数据进行展示了。
订单设计 简化后的订单数据如下，主要包含：下单用户 ID、商品 ID、用户所在城市名称、订单金额和下单时间。
public class OrderDetail { private Long userId; //下单用户id  private Long itemId; //商品id  private String citeName;//用户所在城市  private Double price;//订单金额  private Long timeStamp;//下单时间 } 我们采用 Event-Time 来作为 Flink 程序的时间特征，并且设置 Checkpoint 时间周期为 60 秒。
StreamExecutionEnvironment env = StreamExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>第27讲：Flink Redis Sink 实现</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-28/</link>
      <pubDate>Mon, 20 Jul 2020 10:39:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-28/</guid>
      <description>我们在第 12 课时“Flink 常用的 Source 和 Connector”中提过 Flink 提供了比较丰富的用来连接第三方的连接器，可以在官网中找到 Flink 支持的各种各样的连接器。
此外，Flink 还会基于 Apache Bahir 发布一些 Connector，其中就有我们非常熟悉的 Redis。很多人在 Flink 项目中访问 Redis 的方法都是自己进行实现的，我们也可以使用 Bahir 实现的 Redis 连接器。
事实上，使用 Redis Sink 常用的方法有很多，比如自定义 Sink 函数、依赖开源的 Redis Sink 实现等。
下面我们就分别介绍常用的 Redis Sink 实现。
自定义 Redis Sink  REmote DIctionary Server（Redis）是一个由 Salvatore Sanfilippo 写的 key-value 存储系统。Redis 是一个开源的使用 ANSI C 语言编写、遵守 BSD 协议、支持网络、可基于内存亦可持久化的日志型、Key-Value 数据库，并提供多种语言的 API。
  它通常被称为数据结构服务器，因为值（value）可以是字符串（String）、哈希（Hash）、列表（List）、集合（Sets）和有序集合（Sorted Sets）等类型。
 如果你对 Redis 不熟悉，可以参考官网上的说明。 点击这里下载一个稳定版本的 Redis，我在本地安装的是 2.8.5 版本。使用下面命令进行安装：</description>
    </item>
    
    <item>
      <title>第26讲：Flink 中的聚合函数和累加器的设计和使用</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-27/</link>
      <pubDate>Mon, 20 Jul 2020 10:38:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-27/</guid>
      <description>我们在第 08 课时中提到了 Flink 所支持的窗口和时间类型，并且在第 25 课时中详细讲解了如何设置时间戳提取器和水印发射器。
实际的业务中，我们在使用窗口的过程中一定是基于窗口进行的聚合计算。例如，计算窗口内的 UV、PV 等，那么 Flink 支持哪些基于窗口的聚合函数？累加器又该如何实现呢？
Flink 支持的窗口函数 我们在定义完窗口以后，需要指定窗口上进行的计算。目前 Flink 支持的窗口函数包含 3 种：
 ReduceFunction 增量聚合 AggregateFunction 增量聚合 ProcessWindowFunction 全量聚合  最后还有一种 FlodFunction，但是在 Flink 1.9 版本后已经废弃，推荐使用 AggregateFunction 代替。
下面我们详细讲解以上 3 种窗口聚合函数的定义和使用。
ReduceFunction ReduceFunction 基于两个类型一致的输入进行增量聚合，我们可以自定义 ReduceFunction 来增量聚合窗口内的数据。
可以这样定义自己的 ReduceFunction，覆写 reduce 方法：
DataStream&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt; input = ...; input .keyBy(&amp;lt;key selector&amp;gt;) .window(&amp;lt;window assigner&amp;gt;) .reduce(new ReduceFunction&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt; { public Tuple2&amp;lt;String, Long&amp;gt; reduce(Tuple2&amp;lt;String, Long&amp;gt; v1, Tuple2&amp;lt;String, Long&amp;gt; v2) { return new Tuple2&amp;lt;&amp;gt;(v1.</description>
    </item>
    
    <item>
      <title>第25讲：Flink 中 watermark 的定义和使用</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-26/</link>
      <pubDate>Mon, 20 Jul 2020 10:37:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-26/</guid>
      <description>第 08 课时我们提过窗口和时间的概念，Flink 框架支持事件时间、摄入时间和处理时间三种。Watermark（水印）的出现是用于处理数据从 Source 产生，再到转换和输出，在这个过程中由于网络和反压的原因导致了消息乱序问题。
那么在实际的开发过程中，如何正确地使用 Watermark 呢？
使用 Watermark 必知必会 Watermark 和事件时间 事件时间（Event Time）是数据产生的时间，这个时间一般在数据中自带，由消息的生产者生成。例如，我们的上游是 Kafka 消息，那么每个生成的消息中自带一个时间戳代表该条数据的产生时间，这个时间是固定的，从数据的诞生开始就一直携带。所以，我们在处理消息乱序的情况时，会用 EventTime 和 Watermark 进行配合使用。
我们只需要一行代码，就可以在代码中指定 Flink 系统使用的时间类型为 EventTime：
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); 那么为什么不用处理时间（Processing Time）和摄入时间（Ingestion Time）呢？
处理时间（Processing Time）指的是数据被 Flink 框架处理时机器的系统时间，这个时间本身存在不确定性，比如因为网络延迟等原因。
摄入时间（Ingestion Time）理论上处于事件时间（Event Time）和处理时间（Processing Time）之间，可以用来防止 Flink 内部处理数据发生乱序的情况，但是无法解决数据进入 Flink 之前的乱序行为。
所以我们一般都会用 EventTime、WaterMark 和窗口配合使用来解决消息的乱序和延迟问题。
水印的本质是时间戳 水印的本质是一个一个的时间戳，这个时间戳存在 DataStream 的数据流中，Watermark 的生成方式有两种：
 AssignerWithPeriodicWatermarks 生成周期水印，周期默认的时间是 200ms； AssignerWithPunctuatedWatermarks 按需生成水印。  当 Flink 系统中出现了一个 Watermark T，那么就意味着 EventTime &amp;lt;= T 的数据都已经到达。当 Wartermark T 通过窗口后，后续到来的迟到数据就会被丢弃。
窗口触发和乱序时间 Flink 在用时间 + 窗口 + 水印来解决实际生产中的数据乱序问题，有如下的触发条件：</description>
    </item>
    
    <item>
      <title>第24讲：Flink 消费 Kafka 数据业务开发</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-25/</link>
      <pubDate>Mon, 20 Jul 2020 10:36:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-25/</guid>
      <description>在上一课时中我们提过在实时计算的场景下，绝大多数的数据源都是消息系统，而 Kafka 从众多的消息中间件中脱颖而出，主要是因为高吞吐、低延迟的特点；同时也讲了 Flink 作为生产者像 Kafka 写入数据的方式和代码实现。这一课时我们将从以下几个方面介绍 Flink 消费 Kafka 中的数据方式和源码实现。
Flink 如何消费 Kafka Flink 在和 Kafka 对接的过程中，跟 Kafka 的版本是强相关的。上一课时也提到了，我们在使用 Kafka 连接器时需要引用相对应的 Jar 包依赖，对于某些连接器比如 Kafka 是有版本要求的，一定要去官方网站找到对应的依赖版本。
我们本地的 Kafka 版本是 2.1.0，所以需要对应的类是 FlinkKafkaConsumer。首先需要在 pom.xml 中引入 jar 包依赖：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-connector-kafka_2.11&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.10.0&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 下面将对 Flink 消费 Kafka 数据的方式进行分类讲解。
消费单个 Topic 上一课时我们在本地搭建了 Kafka 环境，并且手动创建了名为 test 的 Topic，然后向名为 test 的 Topic 中写入了数据。
那么现在我们要消费这个 Topic 中的数据，该怎么做呢？
public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.</description>
    </item>
    
    <item>
      <title>第23讲：Mock Kafka 消息并发送</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-24/</link>
      <pubDate>Mon, 20 Jul 2020 10:35:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-24/</guid>
      <description>本课时主要讲解 Kafka 的一些核心概念，以及模拟消息并发送。
大数据消息中间件的王者——Kafka 在上一课时中提过在实时计算的场景下，我们绝大多数的数据源都是消息系统。所以，一个强大的消息中间件来支撑高达几十万的 QPS，以及海量数据存储就显得极其重要。
Kafka 从众多的消息中间件中脱颖而出，主要是因为高吞吐、低延迟的特点；另外基于 Kafka 的生态越来越完善，各个实时处理框架包括 Flink 在消息处理上都会优先进行支持。在第 14 课时“Flink Exactly-once 实现原理解析”中提到 Flink 和 Kafka 结合实现端到端精确一次语义的原理。
Kafka 从众多的消息中间件中脱颖而出，已经成为大数据生态系统中必不可少的一员，主要的特性包括：
 高吞吐 低延迟 高容错 可靠性 生态丰富  为了接下来更好地理解和使用 Kafka，我们首先来看一下 Kafka 中的核心概念和基本入门。
Kafka 核心概念 Kafka 是一个消息队列，生产者向消息队列中写入数据，消费者从队列中获取数据并进行消费。作为一个企业级的消息中间件，Kafka 会支持庞大的业务，不同的业务会有多个队列，我们用 Topic 来给队列命名，在使用 Kafka 时必须指定 Topic。
我们可以认为一个 Topic 就是一个队列，每个 Topic 又会被分成多个 Partition，这样做是为了横向扩展，提高吞吐量。
Kafka 中每个 Partition 都对应一个 Broker，一个 Broker 可以管理多个 Partition。举个例子，假如 Kafka 的某个 Topic 有 10 个 Partition、2 个 Broker，那么每个 Broker 就会管理 5 个 Partition。我们可以把 Partition 简单理解为一个文件，在接收生产者的数据时，需要将数据动态追加到 Partition 上。</description>
    </item>
    
    <item>
      <title>第22讲：项目背景和整体架构设计</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-23/</link>
      <pubDate>Mon, 20 Jul 2020 10:34:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-23/</guid>
      <description>从这一课时开始我们进入实战课程的学习。本项目是一个模拟实时电商数据大屏，本课时先介绍该项目的背景、架构设计和技术选型。
背景 我们在第 01 课时“Flink 的应用场景和架构模型”中提到过，Flink 应用最广的一个场景便是实时计算大屏。每年的双十一、618 电商大促等，各大公司的实时数据战报和数据大屏是一道亮丽的风景线。
实时大屏对数据有非常高的稳定性和精确性要求，特别是面向公众第三方的数据大屏，同时要求高吞吐、低延迟、极高的稳定性和绝对零误差。随时电商大促的成交记录一次次被刷新，背后是下单、支付、发货高达几万甚至十几万的峰值 QPS。
在面向实际运营的数据大屏中，需要提供高达几十种维度的数据，每秒的数据量高达千万甚至亿级别，这对于我们的实时计算架构提出了相当高的要求。那么我们的大屏背后的实时处理在这种数据量规模如何才能达到高吞吐、低延迟、极高的稳定性和绝对零误差的呢？
技术选型和整体架构 在上图的架构图中，涉及几个关键的技术选型，我们下面一一进行讲解。
业务库 Binlog 同步利器——Canal 我们的实时计算架构一般是基于业务数据进行的，但无论是实时计算大屏还是常规的数据分析报表，都不能影响业务的正常进行，所以这里需要引入消息中间件或增量同步框架 Canal。
我们生产环境中的业务数据绝大多数都是基于 MySQL 的，所以需要一个能够实时监控 MySQL 业务数据变化的工具。Canal 是阿里巴巴开源的数据库 Binlog 日志解析框架，主要用途是基于 MySQL 数据库增量日志解析，提供增量数据订阅和消费。
Canal 的原理也非常简单，它会伪装成一个数据库的从库，来读取 Binlog 并进行解析。关于 Canal 的更多资料，你可以参考这里。
解耦和海量数据支持——Kafka 在实时大屏的技术架构下，我们的数据源绝大多数情况下都是消息。我们需要一个强大的消息中间件来支撑高达几十万 QPS，同时支持海量数据存储。
首先，我们为什么需要引入消息中间件？主要是下面三个目的：
 同步变异步 应用解耦 流量削峰  在我们的架构中，为了和业务数据互相隔离，需要使用消息中间件进行解耦从而互不影响。另外在双十一等大促场景下，交易峰值通常出现在某一个时间段，这个时间段系统压力陡增，数据量暴涨，消息中间件还起到了削峰的作用。
为什么选择 Kafka？
Kafka 是最初由 Linkedin 公司开发，是一个分布式、高吞吐、多分区的消息中间件。Kafka 经过长时间的迭代和实践检验，因为其独特的优点已经成为目前主流的分布式消息引擎，经常被用作企业的消息总线、实时数据存储等。
Kafka 从众多的消息中间件中脱颖而出，主要是因为高吞吐、低延迟的特点；另外基于 Kafka 的生态越来越完善，各个实时处理框架包括 Flink 在消息处理上都会优先进行支持。在第 14 课时“Flink Exactly-once 实现原理解析”中提到了 Flink 和 Kafka 结合实现端到端精确一次语义的原理。
Kafka 作为大数据生态系统中已经必不可少的一员，主要的特性如下所示。
 高吞吐：可以满足每秒百万级别消息的生产和消费，并且可以通过横向扩展，保证数据处理能力可以得到线性扩展。 低延迟：以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间复杂度的访问性能。 高容错：Kafka 允许集群的节点出现失败。 可靠性：消息可以根据策略进行磁盘的持久化，并且读写效率都很高。 生态丰富：Kafka 周边生态极其丰富，与各个实时处理框架结合紧密。  实时计算服务——Flink Flink 在当前的架构中主要承担了消息消费、维表关联、消息发送等，我们在之前的课程中多次提到过 Flink 的优势，主要包括：</description>
    </item>
    
    <item>
      <title>第21讲：Flink 在实时计算平台和实时数据仓库中的作用</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-22/</link>
      <pubDate>Mon, 20 Jul 2020 10:33:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-22/</guid>
      <description>基于 Flink 的实时计算平台 大部分公司随着业务场景的不断丰富，同时在业界经过多年的实践检验，基于 Hadoop 的离线存储体系已经足够成熟。但是离线计算天然时效性不强，一般都是隔天级别的滞后，业务数据随着实践的推移，本身的价值就会逐渐减少。越来越多的场景需要使用实时计算，在这种背景下实时计算平台的需求应运而生。
架构选型 我们在第 03 课时“Flink 的编程模型与其他框架比较”中，提到过 Flink 自身独有的优势。
首先在架构上，Flink 采用了经典的主从模式，DataFlow Graph 与 Storm 形成的拓扑 Topology 结构类似，Flink 程序启动后，会根据用户的代码处理成 Stream Graph，然后优化成为 JobGraph，JobManager 会根据 JobGraph 生成 ExecutionGraph。ExecutionGraph 才是 Flink 真正能执行的数据结构，当很多个 ExecutionGraph 分布在集群中，就会形成一张网状的拓扑结构。
其次在容错方面，针对以前的 Spark Streaming 任务，我们可以配置对应的 checkpoint，也就是保存点（检查点）。当任务出现 failover 的时候，会从 checkpoint 重新加载，使得数据不丢失。但是这个过程会导致原来的数据重复处理，不能做到“只处理一次”的语义。Flink 基于两阶段提交实现了端到端的一次处理语义。
在任务的反压上，Flink 没有使用任何复杂的机制来解决反压问题，Flink 在数据传输过程中使用了分布式阻塞队列。我们知道在一个阻塞队列中，当队列满了以后发送者会被天然阻塞住，这种阻塞功能相当于给这个阻塞队列提供了反压的能力。
这些优势和特性，使得 Flink 在实时计算平台的搭建上占有一席之地。
实时计算平台整体架构 一般的实时计算平台的构成大都是以下几部分构成。
 实时数据收集层  在实际业务中，大量的实时计算都是基于消息系统进行的数据收集和投递，这都离不开强大的消息中间件。目前业界使用最广的是 Kafka，另外一些重要的业务数据还会用到其他消息系统比如 RocketMQ 等。Kafka 因为高吞吐、低延迟的特性，特别适合大数量量、高 QPS 下的业务场景，而 RocketMQ 则在事务消息、一致性上有独特的优势。
 实时计算层  Flink 在计算层同时支持流式及批量分析应用，这就是我们所说的批流一体。Flink 承担了数据的实时采集、实时计算和下游发送的角色。随着 Blink 的开源和一些其他实时产品的开源，支持可视化、SQL 化的开发模式已经越来越普及。</description>
    </item>
    
    <item>
      <title>第20讲：Flink 高级应用之海量数据高效去重</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-21/</link>
      <pubDate>Mon, 20 Jul 2020 10:32:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-21/</guid>
      <description>本课时我们主要讲解 Flink 中的海量数据高效去重。
消除重复数据是我们在实际业务中经常遇到的一类问题。在大数据领域，重复数据的删除有助于减少存储所需要的存储容量。而且在一些特定的业务场景中，重复数据是不可接受的，例如，精确统计网站一天的用户数量、在事实表中统计每天发出的快递包裹数量。在传统的离线计算中，我们可以直接用 SQL 通过 DISTINCT 函数，或者数据量继续增加时会用到类似 MapReduce 的思想。那么在实时计算中，去重计数是一个增量和长期的过程，并且不同的场景下因为效率和精度问题方案也需要变化。
针对上述问题，我们在这里列出几种常见的 Flink 中实时去重方案：
 基于状态后端 基于 HyperLogLog 基于布隆过滤器（BloomFilter） 基于 BitMap 基于外部数据库  下面我们依次讲解上述几种方案的适用场景和实现原理。
基于状态后端 我们在第 09 课时“Flink 状态与容错”中曾经讲过 Flink State 状态后端的概念和原理，其中状态后端的种类之一是 RocksDBStateBackend。它会将正在运行中的状态数据保存在 RocksDB 数据库中，该数据库默认将数据存储在 TaskManager 运行节点的数据目录下。
RocksDB 是一个 K-V 数据库，我们可以利用 MapState 进行去重。这里我们模拟一个场景，计算每个商品 SKU 的访问量。
整体的实现代码如下：
public class MapStateDistinctFunction extends KeyedProcessFunction&amp;lt;String,Tuple2&amp;lt;String,Integer&amp;gt;,Tuple2&amp;lt;String,Integer&amp;gt;&amp;gt; { private transient ValueState&amp;lt;Integer&amp;gt; counts; @Override public void open(Configuration parameters) throws Exception { //我们设置 ValueState 的 TTL 的生命周期为24小时，到期自动清除状态  StateTtlConfig ttlConfig = StateTtlConfig .</description>
    </item>
    
    <item>
      <title>第19讲：Flink 如何做维表关联</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-20/</link>
      <pubDate>Mon, 20 Jul 2020 10:31:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-20/</guid>
      <description>在实际生产中，我们经常会有这样的需求，需要以原始数据流作为基础，然后关联大量的外部表来补充一些属性。例如，我们在订单数据中，希望能得到订单收货人所在省的名称，一般来说订单中会记录一个省的 ID，那么需要根据 ID 去查询外部的维度表补充省名称属性。
在 Flink 流式计算中，我们的一些维度属性一般存储在 MySQL/HBase/Redis 中，这些维表数据存在定时更新，需要我们根据业务进行关联。根据我们业务对维表数据关联的时效性要求，有以下几种解决方案：
 实时查询维表 预加载全量数据 LRU 缓存 其他  上述几种关联外部维表的方式几乎涵盖了我们所有的业务场景，下面针对这几种关联维表的方式和特点一一讲解它们的实现方式和注意事项。
实时查询维表 实时查询维表是指用户在 Flink 算子中直接访问外部数据库，比如用 MySQL 来进行关联，这种方式是同步方式，数据保证是最新的。但是，当我们的流计算数据过大，会对外部系统带来巨大的访问压力，一旦出现比如连接失败、线程池满等情况，由于我们是同步调用，所以一般会导致线程阻塞、Task 等待数据返回，影响整体任务的吞吐量。而且这种方案对外部系统的 QPS 要求较高，在大数据实时计算场景下，QPS 远远高于普通的后台系统，峰值高达十万到几十万，整体作业瓶颈转移到外部系统。
这种方式的核心是，我们可以在 Flink 的 Map 算子中建立访问外部系统的连接。下面以订单数据为例，我们根据下单用户的城市 ID，去关联城市名称，核心代码实现如下：
public class Order { private Integer cityId; private String userName; private String items; public Integer getCityId() { return cityId; } public void setCityId(Integer cityId) { this.cityId = cityId; } public String getUserName() { return userName; } public void setUserName(String userName) { this.</description>
    </item>
    
    <item>
      <title>第18讲：如何进行生产环境作业监控</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-19/</link>
      <pubDate>Mon, 20 Jul 2020 10:30:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-19/</guid>
      <description>本课时主要讲解如何进行生产环境作业监控。
在第 15 课时“如何排查生产环境中的反压问题”中提到过我们应该如何发现任务是否出现反压，Flink 的后台页面是我们发现反压问题的第一选择，其后台页面可以直观、清晰地看到当前作业的运行状态。
在实际生产中，Flink 的后台页面可以方便我们对 Flink JobManager、TaskManager、执行计划、Slot 分配、是否反压等参数进行定位，对单个任务来讲可以方便地进行问题排查。
但是，对于很多大中型企业来讲，我们对进群的作业进行管理时，更多的是关心作业精细化实时运行状态。例如，实时吞吐量的同比环比、整个集群的任务运行概览、集群水位，或者监控利用 Flink 实现的 ETL 框架的运行情况等，这时候就需要设计专门的监控系统来监控集群的任务作业情况。
Flink Metrics 针对上面的情况，我们就用了 Flink 提供的另一个强大的功能：Flink Metrics。
Flink Metrics 是 Flink 实现的一套运行信息收集库，我们不但可以收集 Flink 本身提供的系统指标，比如 CPU、内存、线程使用情况、JVM 垃圾收集情况、网络和 IO 等，还可以通过继承和实现指定的类或者接口打点收集用户自定义的指标。
通过使用 Flink Metrics 我们可以轻松地做到：
 实时采集 Flink 中的 Metrics 信息或者自定义用户需要的指标信息并进行展示； 通过 Flink 提供的 Rest API 收集这些信息，并且接入第三方系统进行展示。  Flink Metrics 分类 Flink 提供了四种类型的监控指标，分别是：Counter、Gauge、Histogram、Meter。
Counter Counter 称为计数器，一般用来统计其中一个指标的总量，比如统计数据的输入、输出总量。
public class MyMapper extends RichMapFunction&amp;lt;String, String&amp;gt; { private transient Counter counter; @Override public void open(Configuration config) { this.</description>
    </item>
    
    <item>
      <title>第17讲：生产环境中的并行度和资源设置</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-18/</link>
      <pubDate>Mon, 20 Jul 2020 10:29:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-18/</guid>
      <description>在使用 Flink 处理生产实际问题时，并行度和资源的配置调优是我们经常要面对的工作之一，如何有效和正确地配置并行度是我们的任务能够高效执行的必要条件。这一课时就来看一下生产环境的并行度和资源配置问题。
Flink 中的计算资源 通常我们说的 Flink 中的计算资源是指具体任务的 Task。首先要理解 Flink 中的计算资源的一些核心概念，比如 Slot、Chain、Task 等，正确理解这些概念有助于开发者了解 Flink 中的计算资源是如何进行隔离和管理的，也有助于我们快速地定位生产中的问题。
Task Slot 我们在第 03 课时“Flink 的编程模型与其他框架比较” 中提到过，在实际生产中，Flink 都是以集群在运行，在运行的过程中包含了两类进程，其中之一就是：TaskManager。
在 Flink 集群中，一个 TaskManger 就是一个 JVM 进程，并且会用独立的线程来执行 task，为了控制一个 TaskManger 能接受多少个 task，Flink 提出了 Task Slot 的概念。
我们可以简单地把 Task Slot 理解为 TaskManager 的计算资源子集。假如一个 TaskManager 拥有 5 个 Slot，那么该 TaskManager 的计算资源会被平均分为 5 份，不同的 task 在不同的 Slot 中执行，避免资源竞争。但需要注意的是，Slot 仅仅用来做内存的隔离，对 CPU 不起作用。那么运行在同一个 JVM 的 task 可以共享 TCP 连接，以减少网络传输，在一定程度上提高了程序的运行效率，降低了资源消耗。
Slot 共享 默认情况下，Flink 还允许同一个 Job 的子任务共享 Slot。因为在一个 Flink 任务中，有很多的算子，这些算子的计算压力各不相同，比如简单的 map 和 filter 算子所需要的资源不多，但是有些算子比如 window、group by 则需要更多的计算资源才能满足计算所需。这时候那些资源需求大的算子就可以共用其他的 Slot，提高整个集群的资源利用率。</description>
    </item>
    
    <item>
      <title>第16讲：如何处理生产环境中的数据倾斜问题</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-17/</link>
      <pubDate>Mon, 20 Jul 2020 10:28:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-17/</guid>
      <description>这一课时我们主要讲解如何处理生产环境中的数据倾斜问题。
无论是对于 Flink、Spark 这样的实时计算框架还是 Hive 等离线计算框架，数据量从来都不是问题，真正引起问题导致严重后果的是数据倾斜。所谓数据倾斜，是指在大规模并行处理的数据中，其中某个运行节点处理的数据远远超过其他部分，这会导致该节点压力极大，最终出现运行失败从而导致整个任务的失败。
我们在这一课时中将分析出现数据倾斜的原因，Flink 任务中最容易出现数据倾斜的几个算子并且给出解决方案。
数据倾斜背景和危害 数据倾斜产生的原因和危害和解决方案有哪些呢？我们一一来看。
数据倾斜原理 目前我们所知道的大数据处理框架，比如 Flink、Spark、Hadoop 等之所以能处理高达千亿的数据，是因为这些框架都利用了分布式计算的思想，集群中多个计算节点并行，使得数据处理能力能得到线性扩展。
我们在第 03 课时“Flink 的编程模型与其他框架比较”中曾经讲过，在实际生产中 Flink 都是以集群的形式在运行，在运行的过程中包含了两类进程。其中 TaskManager 实际负责执行计算的 Worker，在其上执行 Flink Job 的一组 Task，Task 则是我们执行具体代码逻辑的容器。理论上只要我们的任务 Task 足够多就可以对足够大的数据量进行处理。
但是实际上大数据量经常出现，一个 Flink 作业包含 200 个 Task 节点，其中有 199 个节点可以在很短的时间内完成计算。但是有一个节点执行时间远超其他结果，并且随着数据量的持续增加，导致该计算节点挂掉，从而整个任务失败重启。我们可以在 Flink 的管理界面中看到任务的某一个 Task 数据量远超其他节点。
数据倾斜原因和解决方案 Flink 任务出现数据倾斜的直观表现是任务节点频繁出现反压，但是增加并行度后并不能解决问题；部分节点出现 OOM 异常，是因为大量的数据集中在某个节点上，导致该节点内存被爆，任务失败重启。
产生数据倾斜的原因主要有 2 个方面：
 业务上有严重的数据热点，比如滴滴打车的订单数据中北京、上海等几个城市的订单量远远超过其他地区； 技术上大量使用了 KeyBy、GroupBy 等操作，错误的使用了分组 Key，人为产生数据热点。  因此解决问题的思路也很清晰：
 业务上要尽量避免热点 key 的设计，例如我们可以把北京、上海等热点城市分成不同的区域，并进行单独处理； 技术上出现热点时，要调整方案打散原来的 key，避免直接聚合；此外 Flink 还提供了大量的功能可以避免数据倾斜。  那么我们就从典型的场景入手，看看在 Flink 任务中出现数据倾斜的主要场景和解决方案。</description>
    </item>
    
    <item>
      <title>第15讲：如何排查生产环境中的反压问题</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-16/</link>
      <pubDate>Mon, 20 Jul 2020 10:27:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-16/</guid>
      <description>这一课时我们主要讲解生产环境中 Flink 任务经常会遇到的一个问题，即如何处理好反压问题将直接关系到任务的资源使用和稳定运行。
反压问题是流式计算系统中经常碰到的一个问题，如果你的任务出现反压节点，那么就意味着任务数据的消费速度小于数据的生产速度，需要对生产数据的速度进行控制。通常情况下，反压经常出现在促销、热门活动等场景，它们有一个共同的特点：短时间内流量陡增造成数据的堆积或者消费速度变慢。
不同框架的反压对比 目前主流的大数据实时处理系统都对反压问题进行了专门的处理，希望框架自身能检测到被阻塞的算子，然后降低数据生产者的发送速率。我们所熟悉的 Storm、Spark Streaming、Flink 的实现稍微有所不同。
Storm Storm 从 1.0 版本以后引入了全新的反压机制，Storm 会主动监控工作节点。当工作节点接收数据超过一定的水位值时，那么反压信息会被发送到 ZooKeeper 上，然后 ZooKeeper 通知所有的工作节点进入反压状态，最后数据的生产源头会降低数据的发送速度。
Spark Streaming Spark Streaming 在原有的架构基础上专门设计了一个 RateController 组件，该组件利用经典的 PID 算法。向系统反馈当前系统处理数据的几个重要属性：消息数量、调度时间、处理时间、调度时间等，然后根据这些参数计算出一个速率，该速率则是当前系统处理数据的最大能力，Spark Streaming 会根据计算结果对生产者进行限速。
Flink Flink 的反压设计利用了网络传输和动态限流。在 Flink 的设计哲学中，纯流式计算给 Flink 进行反压设计提供了天然的优势。
我们在以前的课程中讲解过，Flink 任务的组成由基本的“流”和“算子”构成，那么“流”中的数据在“算子”间进行计算和转换时，会被放入分布式的阻塞队列中。当消费者的阻塞队列满时，则会降低生产者的数据生产速度。
如上图所示，我们看一下 Flink 进行逐级反压的过程。当 Task C 的数据处理速度发生异常时，Receive Buffer 会呈现出队列满的情况，Task B 的 Send Buffer 会感知到这一点，然后把数据发送速度降低。以此类推，整个反压会一直从下向上传递到 Source 端；反之，当下游的 Task 处理能力有提升后，会在此反馈到 Source Task，数据的发送和读取速率都会升高，提高了整个任务的处理能力。
反压的定位 当你的任务出现反压时，如果你的上游是类似 Kafka 的消息系统，很明显的表现就是消费速度变慢，Kafka 消息出现堆积。
如果你的业务对数据延迟要求并不高，那么反压其实并没有很大的影响。但是对于规模很大的集群中的大作业，反压会造成严重的“并发症”。首先任务状态会变得很大，因为数据大规模堆积在系统中，这些暂时不被处理的数据同样会被放到“状态”中。另外，Flink 会因为数据堆积和处理速度变慢导致 checkpoint 超时，而 checkpoint 是 Flink 保证数据一致性的关键所在，最终会导致数据的不一致发生。</description>
    </item>
    
    <item>
      <title>第14讲：Flink Exactly-once 实现原理解析</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-15/</link>
      <pubDate>Mon, 20 Jul 2020 10:26:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-15/</guid>
      <description>这一课时我们将讲解 Flink “精确一次”的语义实现原理，同时这也是面试的必考点。
Flink 的“精确一次”处理语义是，Flink 提供了一个强大的语义保证，也就是说在任何情况下都能保证数据对应用产生的效果只有一次，不会多也不会少。
那么 Flink 是如何实现“端到端的精确一次处理”语义的呢？
背景 通常情况下，流式计算系统都会为用户提供指定数据处理的可靠模式功能，用来表明在实际生产运行中会对数据处理做哪些保障。一般来说，流处理引擎通常为用户的应用程序提供三种数据处理语义：最多一次、至少一次和精确一次。
 最多一次（At-most-Once）：这种语义理解起来很简单，用户的数据只会被处理一次，不管成功还是失败，不会重试也不会重发。 至少一次（At-least-Once）：这种语义下，系统会保证数据或事件至少被处理一次。如果中间发生错误或者丢失，那么会从源头重新发送一条然后进入处理系统，所以同一个事件或者消息会被处理多次。 精确一次（Exactly-Once）：表示每一条数据只会被精确地处理一次，不多也不少。  Exactly-Once 是 Flink、Spark 等流处理系统的核心特性之一，这种语义会保证每一条消息只被流处理系统处理一次。“精确一次” 语义是 Flink 1.4.0 版本引入的一个重要特性，而且，Flink 号称支持“端到端的精确一次”语义。
在这里我们解释一下“端到端（End to End）的精确一次”，它指的是 Flink 应用从 Source 端开始到 Sink 端结束，数据必须经过的起始点和结束点。Flink 自身是无法保证外部系统“精确一次”语义的，所以 Flink 若要实现所谓“端到端（End to End）的精确一次”的要求，那么外部系统必须支持“精确一次”语义；然后借助 Flink 提供的分布式快照和两阶段提交才能实现。
分布式快照机制 我们在之前的课程中讲解过 Flink 的容错机制，Flink 提供了失败恢复的容错机制，而这个容错机制的核心就是持续创建分布式数据流的快照来实现。
同 Spark 相比，Spark 仅仅是针对 Driver 的故障恢复 Checkpoint。而 Flink 的快照可以到算子级别，并且对全局数据也可以做快照。Flink 的分布式快照受到 Chandy-Lamport 分布式快照算法启发，同时进行了量身定做，有兴趣的同学可以搜一下。
Barrier Flink 分布式快照的核心元素之一是 Barrier（数据栅栏），我们也可以把 Barrier 简单地理解成一个标记，该标记是严格有序的，并且随着数据流往下流动。每个 Barrier 都带有自己的 ID，Barrier 极其轻量，并不会干扰正常的数据处理。
如上图所示，假如我们有一个从左向右流动的数据流，Flink 会依次生成 snapshot 1、 snapshot 2、snapshot 3……Flink 中有一个专门的“协调者”负责收集每个 snapshot 的位置信息，这个“协调者”也是高可用的。</description>
    </item>
    
    <item>
      <title>第13讲：如何实现生产环境中的 Flink 高可用配置</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-14/</link>
      <pubDate>Mon, 20 Jul 2020 10:25:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-14/</guid>
      <description>我们在第 06 课时“Flink 集群安装部署和 HA 配置”中讲解了 Flink 的几种常见部署模式，并且简单地介绍了 HA 配置。
概述 事实上，集群的高可用（High Availablility，以下简称 HA）配置是大数据领域经典的一个问题。
 通常 HA 用来描述一个系统经过专门的设计，从而减少停工时间，而保持其服务的高度可用性。
 我们在第 03 课时“Flink 的编程模型与其他框架比较”中也提到过 Flink 集群中的角色，其中 JobManager 扮演的是集群管理者的角色，负责调度任务、协调 Checkpoints、协调故障恢复、收集 Job 的状态信息，并管理 Flink 集群中的从节点 TaskManager。
在默认的情况下，我们的每个集群都只有一个 JobManager 实例，假如这个 JobManager 崩溃了，那么将会导致我们的作业运行失败，并且无法提交新的任务。
因此，在生产环境中我们的集群应该如何配置以达到高可用的目的呢？针对不同模式进行部署的集群，我们需要不同的配置。
源码分析 Flink 中的 JobManager、WebServer 等组件都需要高可用保障，并且 Flink 还需要进行 Checkpoint 元数据的持久化操作。与 Flink HA 相关的类图如下图所示，我们跟随源码简单看一下 Flink HA 的实现。
HighAvailabilityMode 类中定义了三种高可用性模式枚举，如下图所示：
 NONE：非 HA 模式 ZOOKEEPER：基于 ZK 实现 HA FACTORY_CLASS：自定义 HA 工厂类，该类需要实现 HighAvailabilityServicesFactory 接口  具体的高可用实例对象创建则在 HighAvailabilityServicesUtils 类中有体现，如下图所示：</description>
    </item>
    
    <item>
      <title>第12讲：Flink 常用的 Source 和 Connector</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-13/</link>
      <pubDate>Mon, 20 Jul 2020 10:24:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-13/</guid>
      <description>本课时我们主要介绍 Flink 中支持的 Source 和常用的 Connector。
Flink 作为实时计算领域强大的计算能力，以及与其他系统进行对接的能力都非常强大。Flink 自身实现了多种 Source 和 Connector 方法，并且还提供了多种与第三方系统进行对接的 Connector。
我们可以把这些 Source、Connector 分成以下几个大类。
预定义和自定义 Source 在前面的第 04 课时“Flink 常用的 DataSet 和 DataStream API”中提到过几种 Flink 已经实现的新建 DataStream 方法。
基于文件 我们在本地环境进行测试时可以方便地从本地文件读取数据：
readTextFile(path) readFile(fileInputFormat, path) ... 可以直接在 ExecutionEnvironment 和 StreamExecutionEnvironment 类中找到 Flink 支持的读取本地文件的方法，如下图所示：
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // read text file from local files system DataSet&amp;lt;String&amp;gt; localLines = env.readTextFile(&amp;#34;file:///path/to/my/textfile&amp;#34;); // read text file from an HDFS running at nnHost:nnPort DataSet&amp;lt;String&amp;gt; hdfsLines = env.</description>
    </item>
    
    <item>
      <title>第11讲：Flink CEP 复杂事件处理</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-12/</link>
      <pubDate>Mon, 20 Jul 2020 10:23:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-12/</guid>
      <description>你好，欢迎来到第 11 课时，这一课时将介绍 Flink 中提供的一个很重要的功能：复杂事件处理 CEP。
背景 Complex Event Processing（CEP）是 Flink 提供的一个非常亮眼的功能，关于 CEP 的解释我们引用维基百科中的一段话：
 CEP, is event processing that combines data from multiple sources to infer events or patterns that suggest more complicated circumstances. The goal of complex event processing is to identify meaningful events (such as opportunities or threats) and respond to them as quickly as possible.
 在我们的实际生产中，随着数据的实时性要求越来越高，实时数据的量也在不断膨胀，在某些业务场景中需要根据连续的实时数据，发现其中有价值的那些事件。
说到底，Flink 的 CEP 到底解决了什么样的问题呢？
比如，我们需要在大量的订单交易中发现那些虚假交易，在网站的访问日志中寻找那些使用脚本或者工具“爆破”登录的用户，或者在快递运输中发现那些滞留很久没有签收的包裹等。
如果你对 CEP 的理论基础非常感兴趣，推荐一篇论文“Efﬁcient Pattern Matching over Event Streams”。</description>
    </item>
    
    <item>
      <title>第10讲：Flink Side OutPut 分流</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-11/</link>
      <pubDate>Mon, 20 Jul 2020 10:22:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-11/</guid>
      <description>这一课时将介绍 Flink 中提供的一个很重要的功能：旁路分流器。
分流场景 我们在生产实践中经常会遇到这样的场景，需把输入源按照需要进行拆分，比如我期望把订单流按照金额大小进行拆分，或者把用户访问日志按照访问者的地理位置进行拆分等。面对这样的需求该如何操作呢？
分流的方法 通常来说针对不同的场景，有以下三种办法进行流的拆分。
Filter 分流 Filter 方法我们在第 04 课时中（Flink 常用的 DataSet 和 DataStream API）讲过，这个算子用来根据用户输入的条件进行过滤，每个元素都会被 filter() 函数处理，如果 filter() 函数返回 true 则保留，否则丢弃。那么用在分流的场景，我们可以做多次 filter，把我们需要的不同数据生成不同的流。
来看下面的例子：
public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //获取数据源  List data = new ArrayList&amp;lt;Tuple3&amp;lt;Integer,Integer,Integer&amp;gt;&amp;gt;(); data.add(new Tuple3&amp;lt;&amp;gt;(0,1,0)); data.add(new Tuple3&amp;lt;&amp;gt;(0,1,1)); data.add(new Tuple3&amp;lt;&amp;gt;(0,2,2)); data.add(new Tuple3&amp;lt;&amp;gt;(0,1,3)); data.add(new Tuple3&amp;lt;&amp;gt;(1,2,5)); data.add(new Tuple3&amp;lt;&amp;gt;(1,2,9)); data.add(new Tuple3&amp;lt;&amp;gt;(1,2,11)); data.add(new Tuple3&amp;lt;&amp;gt;(1,2,13)); DataStreamSource&amp;lt;Tuple3&amp;lt;Integer,Integer,Integer&amp;gt;&amp;gt; items = env.fromCollection(data); SingleOutputStreamOperator&amp;lt;Tuple3&amp;lt;Integer, Integer, Integer&amp;gt;&amp;gt; zeroStream = items.</description>
    </item>
    
    <item>
      <title>第09讲：Flink 状态与容错</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-10/</link>
      <pubDate>Mon, 20 Jul 2020 10:21:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-10/</guid>
      <description>这一课时我们主要讲解 Flink 的状态和容错。
在 Flink 的框架中，进行有状态的计算是 Flink 最重要的特性之一。所谓的状态，其实指的是 Flink 程序的中间计算结果。Flink 支持了不同类型的状态，并且针对状态的持久化还提供了专门的机制和状态管理器。
状态 我们在 Flink 的官方博客中找到这样一段话，可以认为这是对状态的定义：
 When working with state, it might also be useful to read about Flink’s state backends. Flink provides different state backends that specify how and where state is stored. State can be located on Java’s heap or off-heap. Depending on your state backend, Flink can also manage the state for the application, meaning Flink deals with the memory management (possibly spilling to disk if necessary) to allow applications to hold very large state.</description>
    </item>
    
    <item>
      <title>第08讲：Flink 窗口、时间和水印</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-09/</link>
      <pubDate>Mon, 20 Jul 2020 10:17:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-09/</guid>
      <description>本课时主要介绍 Flink 中的时间和水印。
我们在之前的课时中反复提到过窗口和时间的概念，Flink 框架中支持事件时间、摄入时间和处理时间三种。而当我们在流式计算环境中数据从 Source 产生，再到转换和输出，这个过程由于网络和反压的原因会导致消息乱序。因此，需要有一个机制来解决这个问题，这个特别的机制就是“水印”。
Flink 的窗口和时间 我们在第 05 课时中讲解过 Flink 窗口的实现，根据窗口数据划分的不同，目前 Flink 支持如下 3 种：
 滚动窗口，窗口数据有固定的大小，窗口中的数据不会叠加； 滑动窗口，窗口数据有固定的大小，并且有生成间隔； 会话窗口，窗口数据没有固定的大小，根据用户传入的参数进行划分，窗口数据无叠加。  Flink 中的时间分为三种：
 事件时间（Event Time），即事件实际发生的时间； 摄入时间（Ingestion Time），事件进入流处理框架的时间； 处理时间（Processing Time），事件被处理的时间。  下面的图详细说明了这三种时间的区别和联系：
事件时间（Event Time） 事件时间（Event Time）指的是数据产生的时间，这个时间一般由数据生产方自身携带，比如 Kafka 消息，每个生成的消息中自带一个时间戳代表每条数据的产生时间。Event Time 从消息的产生就诞生了，不会改变，也是我们使用最频繁的时间。
利用 Event Time 需要指定如何生成事件时间的“水印”，并且一般和窗口配合使用，具体会在下面的“水印”内容中详细讲解。
我们可以在代码中指定 Flink 系统使用的时间类型为 EventTime：
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //设置时间属性为 EventTime env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStream&amp;lt;MyEvent&amp;gt; stream = env.addSource(new FlinkKafkaConsumer09&amp;lt;MyEvent&amp;gt;(topic, schema, props)); stream .keyBy( (event) -&amp;gt; event.getUser() ) .</description>
    </item>
    
    <item>
      <title>第07讲：Flink 常见核心概念分析</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-08/</link>
      <pubDate>Mon, 20 Jul 2020 10:16:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-08/</guid>
      <description>在 Flink 这个框架中，有很多独有的概念，比如分布式缓存、重启策略、并行度等，这些概念是我们在进行任务开发和调优时必须了解的，这一课时我将会从原理和应用场景分别介绍这些概念。
分布式缓存 熟悉 Hadoop 的你应该知道，分布式缓存最初的思想诞生于 Hadoop 框架，Hadoop 会将一些数据或者文件缓存在 HDFS 上，在分布式环境中让所有的计算节点调用同一个配置文件。在 Flink 中，Flink 框架开发者们同样将这个特性进行了实现。
Flink 提供的分布式缓存类型 Hadoop，目的是为了在分布式环境中让每一个 TaskManager 节点保存一份相同的数据或者文件，当前计算节点的 task 就像读取本地文件一样拉取这些配置。
分布式缓存在我们实际生产环境中最广泛的一个应用，就是在进行表与表 Join 操作时，如果一个表很大，另一个表很小，那么我们就可以把较小的表进行缓存，在每个 TaskManager 都保存一份，然后进行 Join 操作。
那么我们应该怎样使用 Flink 的分布式缓存呢？举例如下：
public static void main(String[] args) throws Exception { final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.registerCachedFile(&amp;#34;/Users/wangzhiwu/WorkSpace/quickstart/distributedcache.txt&amp;#34;, &amp;#34;distributedCache&amp;#34;); //1：注册一个文件,可以使用hdfs上的文件 也可以是本地文件进行测试  DataSource&amp;lt;String&amp;gt; data = env.fromElements(&amp;#34;Linea&amp;#34;, &amp;#34;Lineb&amp;#34;, &amp;#34;Linec&amp;#34;, &amp;#34;Lined&amp;#34;); DataSet&amp;lt;String&amp;gt; result = data.map(new RichMapFunction&amp;lt;String, String&amp;gt;() { private ArrayList&amp;lt;String&amp;gt; dataList = new ArrayList&amp;lt;String&amp;gt;(); @Override public void open(Configuration parameters) throws Exception { super.</description>
    </item>
    
    <item>
      <title>第06讲：Flink 集群安装部署和 HA 配置</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-07/</link>
      <pubDate>Mon, 20 Jul 2020 10:15:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-07/</guid>
      <description>我们在这一课时将讲解 Flink 常见的部署模式：本地模式、Standalone 模式和 Flink On Yarn 模式，然后分别讲解三种模式的使用场景和部署中常见的问题，最后将讲解在生产环境中 Flink 集群的高可用配置。
Flink 常见的部署模式 环境准备 在绝大多数情况下，我们的 Flink 都是运行在 Unix 环境中的，推荐在 Mac OS 或者 Linux 环境下运行 Flink。如果是集群模式，那么可以在自己电脑上安装虚拟机，保证有一个 master 节点和两个 slave 节点。
同时，要注意在所有的机器上都应该安装 JDK 和 SSH。JDK 是我们运行 JVM 语言程序必须的，而 SSH 是为了在服务器之间进行跳转和执行命令所必须的。关于服务器之间通过 SSH 配置公钥登录，你可以直接搜索安装和配置方法，我们不做过度展开。
Flink 的安装包可以在这里下载。需要注意的是，如果你要和 Hadoop 进行集成，那么我们需要使用到对应的 Hadoop 依赖，下面将会详细讲解。
Local 模式 Local 模式是 Flink 提供的最简单部署模式，一般用来本地测试和演示使用。
我们在这里下载 Apache Flink 1.10.0 for Scala 2.11 版本进行演示，该版本对应 Scala 2.11 版本。
将压缩包下载到本地，并且直接进行解压，使用 Flink 默认的端口配置，直接运行脚本启动：
➜ [SoftWare]# tar -zxvf flink-1.10.0-bin-scala_2.11.tgz 上图则为解压完成后的目录情况。</description>
    </item>
    
    <item>
      <title>第05讲：Flink SQL &amp; Table 编程和案例</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-06/</link>
      <pubDate>Mon, 20 Jul 2020 10:14:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-06/</guid>
      <description>我们在第 02 课时中使用 Flink Table &amp;amp; SQL 的 API 实现了最简单的 WordCount 程序。在这一课时中，将分别从 Flink Table &amp;amp; SQL 的背景和编程模型、常见的 API、算子和内置函数等对 Flink Table &amp;amp; SQL 做一个详细的讲解和概括，最后模拟了一个实际业务场景使用 Flink Table &amp;amp; SQL 开发。
Flink Table &amp;amp; SQL 概述 背景 我们在前面的课时中讲过 Flink 的分层模型，Flink 自身提供了不同级别的抽象来支持我们开发流式或者批量处理程序，下图描述了 Flink 支持的 4 种不同级别的抽象。
Table API 和 SQL 处于最顶端，是 Flink 提供的高级 API 操作。Flink SQL 是 Flink 实时计算为简化计算模型，降低用户使用实时计算门槛而设计的一套符合标准 SQL 语义的开发语言。
我们在第 04 课时中提到过，Flink 在编程模型上提供了 DataStream 和 DataSet 两套 API，并没有做到事实上的批流统一，因为用户和开发者还是开发了两套代码。正是因为 Flink Table &amp;amp; SQL 的加入，可以说 Flink 在某种程度上做到了事实上的批流一体。</description>
    </item>
    
    <item>
      <title>第04讲：Flink 常用的 DataSet 和 DataStream API</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-05/</link>
      <pubDate>Mon, 20 Jul 2020 10:13:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-05/</guid>
      <description>本课时我们主要介绍 Flink 的 DataSet 和 DataStream 的 API，并模拟了实时计算的场景，详细讲解了 DataStream 常用的 API 的使用。
说好的流批一体呢
现状 在前面的课程中，曾经提到过，Flink 很重要的一个特点是“流批一体”，然而事实上 Flink 并没有完全做到所谓的“流批一体”，即编写一套代码，可以同时支持流式计算场景和批量计算的场景。目前截止 1.10 版本依然采用了 DataSet 和 DataStream 两套 API 来适配不同的应用场景。
DateSet 和 DataStream 的区别和联系 在官网或者其他网站上，都可以找到目前 Flink 支持两套 API 和一些应用场景，但大都缺少了“为什么”这样的思考。
Apache Flink 在诞生之初的设计哲学是：用同一个引擎支持多种形式的计算，包括批处理、流处理和机器学习等。尤其是在流式计算方面，Flink 实现了计算引擎级别的流批一体。那么对于普通开发者而言，如果使用原生的 Flink ，直接的感受还是要编写两套代码。
整体架构如下图所示：
在 Flink 的源代码中，我们可以在 flink-java 这个模块中找到所有关于 DataSet 的核心类，DataStream 的核心实现类则在 flink-streaming-java 这个模块。
在上述两张图中，我们分别打开 DataSet 和 DataStream 这两个类，可以发现，二者支持的 API 都非常丰富且十分类似，比如常用的 map、filter、join 等常见的 transformation 函数。
我们在前面的课时中讲过 Flink 的编程模型，对于 DataSet 而言，Source 部分来源于文件、表或者 Java 集合；而 DataStream 的 Source 部分则一般是消息中间件比如 Kafka 等。</description>
    </item>
    
    <item>
      <title>第03讲：Flink 的编程模型与其他框架比较</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-04/</link>
      <pubDate>Mon, 20 Jul 2020 10:12:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-04/</guid>
      <description>本课时我们主要介绍 Flink 的编程模型与其他框架比较。
本课时的内容主要介绍基于 Flink 的编程模型，包括 Flink 程序的基础处理语义和基本构成模块，并且和 Spark、Storm 进行比较，Flink 作为最新的分布式大数据处理引擎具有哪些独特的优势呢？
Flink 的核心语义和架构模型 我们在讲解 Flink 程序的编程模型之前，先来了解一下 Flink 中的 Streams、State、Time 等核心概念和基础语义，以及 Flink 提供的不同层级的 API。
Flink 核心概念  Streams（流），流分为有界流和无界流。有界流指的是有固定大小，不随时间增加而增长的数据，比如我们保存在 Hive 中的一个表；而无界流指的是数据随着时间增加而增长，计算状态持续进行，比如我们消费 Kafka 中的消息，消息持续不断，那么计算也会持续进行不会结束。 State（状态），所谓的状态指的是在进行流式计算过程中的信息。一般用作容错恢复和持久化，流式计算在本质上是增量计算，也就是说需要不断地查询过去的状态。状态在 Flink 中有十分重要的作用，例如为了确保 Exactly-once 语义需要将数据写到状态中；此外，状态的持久化存储也是集群出现 Fail-over 的情况下自动重启的前提条件。 Time（时间），Flink 支持了 Event time、Ingestion time、Processing time 等多种时间语义，时间是我们在进行 Flink 程序开发时判断业务状态是否滞后和延迟的重要依据。 API：Flink 自身提供了不同级别的抽象来支持我们开发流式或者批量处理程序，由上而下可分为 SQL / Table API、DataStream API、ProcessFunction 三层，开发者可以根据需要选择不同层级的 API 进行开发。  Flink 编程模型和流式处理 我们在第 01 课中提到过，Flink 程序的基础构建模块是流（Streams）和转换（Transformations），每一个数据流起始于一个或多个 Source，并终止于一个或多个 Sink。数据流类似于有向无环图（DAG）。
在分布式运行环境中，Flink 提出了算子链的概念，Flink 将多个算子放在一个任务中，由同一个线程执行，减少线程之间的切换、消息的序列化/反序列化、数据在缓冲区的交换，减少延迟的同时提高整体的吞吐量。
官网中给出的例子如下，在并行环境下，Flink 将多个 operator 的子任务链接在一起形成了一个task，每个 task 都有一个独立的线程执行。</description>
    </item>
    
    <item>
      <title>第02讲：Flink 入门程序 WordCount 和 SQL 实现</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-03/</link>
      <pubDate>Mon, 20 Jul 2020 10:11:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-03/</guid>
      <description>本课时我们主要介绍 Flink 的入门程序以及 SQL 形式的实现。
上一课时已经讲解了 Flink 的常用应用场景和架构模型设计，这一课时我们将会从一个最简单的 WordCount 案例作为切入点，并且同时使用 SQL 方式进行实现，为后面的实战课程打好基础。
我们首先会从环境搭建入手，介绍如何搭建本地调试环境的脚手架；然后分别从DataSet（批处理）和 DataStream（流处理）两种方式如何进行单词计数开发；最后介绍 Flink Table 和 SQL 的使用。
Flink 开发环境 通常来讲，任何一门大数据框架在实际生产环境中都是以集群的形式运行，而我们调试代码大多数会在本地搭建一个模板工程，Flink 也不例外。
Flink 一个以 Java 及 Scala 作为开发语言的开源大数据项目，通常我们推荐使用 Java 来作为开发语言，Maven 作为编译和包管理工具进行项目构建和编译。对于大多数开发者而言，JDK、Maven 和 Git 这三个开发工具是必不可少的。
关于 JDK、Maven 和 Git 的安装建议如下表所示：
工程创建 一般来说，我们在通过 IDE 创建工程，可以自己新建工程，添加 Maven 依赖，或者直接用 mvn 命令创建应用：
mvn archetype:generate \ -DarchetypeGroupId=org.apache.flink \ -DarchetypeArtifactId=flink-quickstart-java \ -DarchetypeVersion=1.10.0 通过指定 Maven 工程的三要素，即 GroupId、ArtifactId、Version 来创建一个新的工程。同时 Flink 给我提供了更为方便的创建 Flink 工程的方法：
curl https://flink.apache.org/q/quickstart.sh | bash -s 1.</description>
    </item>
    
    <item>
      <title>开篇词：实时计算领域最锋利的武器 Flink</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-01/</link>
      <pubDate>Mon, 20 Jul 2020 10:10:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-01/</guid>
      <description>你好，欢迎来到 Flink 专栏，我是王知无，目前在某一线互联网公司从事数据平台架构和研发工作多年，算是整个大数据开发领域的老兵了。
我最早从 Release 版本开始关注 Flink，可以说是国内第一批钻研 Flink 的开发者，后来基于 Flink 开发过实时计算业务应用、实时数据仓库以及监控报警系统，在这个过程中积累了大量宝贵的生产实践经验。
面试是开发者永远绕不过去的坎 由于项目需要，我在工作中面试过很多 Flink 开发工程师，并且发现了一些普遍性问题，比如：
 对常用的 Flink 核心概念和原理掌握不牢，一旦参与到实战业务中必将寸步难行，一面直接被刷掉； 能够通过简历筛选的人基本都有实时流计算开发的经验，可以从容应对典型场景下的问题，但对于非典型但常见的业务场景问题就会支支吾吾，无从应答； 有些面试者自称参与过实时计算平台的架构设计、开发、发布和运维等全流程的工作，但稍微追问就会发现他在项目中的参与度其实很低，暴露出在上一家公司只是开发团队的一个“小透明”； 我们现在招聘其实是偏向招有相关经验并熟悉底层原理的人，曾经有面试者能熟练回答在项目中是如何应用 Flink 的，但是不知道底层源码级别的实现。  上面列举的这四个问题看似不同，但本质上都是在全方位考察你对技术原理的理解深度，以及在实际工作中解决问题的能力。
当然还有一类人，他们具备深厚的理论基础和丰富的实战经验，却往往因为缺乏面试经验，依然屡屡与大厂擦肩而过。很多开发者在学习完一个框架后，可以熟练地开发和排查问题，但是在面试的过程中却无法逻辑清晰地表述自己的观点。想象一下，当你在面试中被问到以下三个问题：
 Flink 如何实现 Exactly-once 语义？ Flink 时间类型的分类和各自的实现原理？ Flink 如何处理数据乱序和延迟？  你将如何作答？面试官满意的答案究竟长什么样？上述问题的答案，你都可以在这个专栏中找到。
想进大厂，必须掌握 Flink 技术 随着大数据时代的发展、海量数据的实时处理和多样业务的数据计算需求激增，传统的批处理方式和早期的流式处理框架也有自身的局限性，难以在延迟性、吞吐量、容错能力，以及使用便捷性等方面满足业务日益苛刻的要求。在这种形势下，Flink 以其独特的天然流式计算特性和更为先进的架构设计，极大地改善了以前的流式处理框架所存在的问题。
越来越多的国内公司开始用 Flink 来做实时数据处理，其中阿里巴巴率先将 Flink 技术在全集团推广使用，比如 Flink SQL 与 Hive 生态的集成、拥抱 AI 等；腾讯、百度、字节跳动、滴滴、华为等众多互联网公司也已经将 Flink 作为未来技术重要的发力点。在未来 3 ~ 5 年，Flink 必将发展成为企业内部主流的数据处理框架，成为开发者进入大厂的“敲门砖”。
反观国外，在 2019 年 Flink 已经成为 Apache 基金会和 GitHub 社区最为活跃的项目之一。在全球范围内，越来越多的企业都在迫切地进行技术迭代和更新，无论是更新传统的实时计算业务，还是实时数据仓库的搭建，Flink 都是最佳之选。</description>
    </item>
    
    <item>
      <title>第01讲：Flink 的应用场景和架构模型</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-02/</link>
      <pubDate>Mon, 20 Jul 2020 10:10:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-20-the-lessons-of-flink-02/</guid>
      <description>你好，欢迎来到第 01 课时，本课时我们主要介绍 Flink 的应用场景和架构模型。
实时计算最好的时代 在过去的十年里，面向数据时代的实时计算技术接踵而至。从我们最初认识的 Storm，再到 Spark 的异军突起，迅速占领了整个实时计算领域。直到 2019 年 1 月底，阿里巴巴内部版本 Flink 正式开源！一石激起千层浪，Flink 开源的消息立刻刷爆朋友圈，整个大数据计算领域一直以来由 Spark 独领风骚，瞬间成为两强争霸的时代。
Apache Flink（以下简称 Flink）以其先进的设计理念、强大的计算能力备受关注，如何将 Flink 快速应用在生产环境中，更好的与现有的大数据生态技术完美结合，充分挖掘数据的潜力，成为了众多开发者面临的难题。
Flink 实际应用场景 Flink 自从 2019 年初开源以来，迅速成为大数据实时计算领域炙手可热的技术框架。作为 Flink 的主要贡献者阿里巴巴率先将其在全集团进行推广使用，另外由于 Flink 天然的流式特性，更为领先的架构设计，使得 Flink 一出现便在各大公司掀起了应用的热潮。
阿里巴巴、腾讯、百度、字节跳动、滴滴、华为等众多互联网公司已经将 Flink 作为未来技术重要的发力点，迫切地在各自公司内部进行技术升级和推广使用。同时，Flink 已经成为 Apache 基金会和 GitHub 社区最为活跃的项目之一。
我们来看看 Flink 支持的众多应用场景。
实时数据计算 如果你对大数据技术有所接触，那么下面的这些需求场景你应该并不陌生：
 阿里巴巴每年双十一都会直播，实时监控大屏是如何做到的？
  公司想看一下大促中销量最好的商品 TOP5？
  我是公司的运维，希望能实时接收到服务器的负载情况？
  &amp;hellip;&amp;hellip;
 我们可以看到，数据计算场景需要从原始数据中提取有价值的信息和指标，比如上面提到的实时销售额、销量的 TOP5，以及服务器的负载情况等。
传统的分析方式通常是利用批查询，或将事件（生产上一般是消息）记录下来并基于此形成有限数据集（表）构建应用来完成。为了得到最新数据的计算结果，必须先将它们写入表中并重新执行 SQL 查询，然后将结果写入存储系统比如 MySQL 中，再生成报告。
Apache Flink 同时支持流式及批量分析应用，这就是我们所说的批流一体。Flink 在上述的需求场景中承担了数据的实时采集、实时计算和下游发送。</description>
    </item>
    
    <item>
      <title>15 | 用户增长：用户增长的本质是什么？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-16/</link>
      <pubDate>Thu, 16 Jul 2020 23:10:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-16/</guid>
      <description>本课时内容分为三部分：
 用户增长模型； 国内用户增长现状； 增长案例解析。  用户增长模型 用户增长的基本模型就是大家熟知的 AARRR，如下图所示。
实际上，这套模型看似很完美，然而很有问题。因为这套模型是从拉新的角度出发，对一款 APP 来说，拉新要花很多钱。如果从这个角度出发就是在不断烧钱，随着现在资本越来越理性，野蛮增长已经是过去式。
如果是从留存这个角度来出发，就会好很多，如下图所示。
先把产品打磨好，运营服务好，然后在留存的基础上进行变现，挣钱后再投入到渠道去拉新。这样会更加靠谱，毕竟人傻钱多的时代早已过去。如果一款产品在中期还要靠不断注水才能保持规模，那这样的产品肯定有极大问题，这样的团队也是非常不靠谱，只有早点转型做好留存才有希望。
先做留存，再做变现，然后做推荐、拉新、激活。现阶段大部分的产品都是这样一套玩法，而不是从拉新开始，因为拉新对渠道的要求太高。渠道思维和产品思维是两种思维，我个人觉得产品思维、用户思维会更加靠谱。
另外未来也有可能是这样，先变现，然后到推荐，再到拉新、激活、留存，如下图所示。
第一考虑因素是变现，你先说出你这产品能带来多少收入，净利润是多少。那么就先变现和推荐，然后再拿这些钱去拉新、激活、留存。因为随着资本越来越理性，普遍会从流量思维切换到 ROI 思维，毕竟活下去才是最重要的指标。
这三种不同模型的侧重点都不同，企业的打法也完全不一样。你可以想想，这套模型还可以怎么样？你可以在下方留言与我交流。
在我本人的工作中，接触到很多人，他们去做产品或者做数据分析时，都会看网上的模型 ，一直都在往某一套模型上去套，最后效果也不是很好。所以我建议你不要去纠结什么模型，也不要指望通过数据分析突然找到一个很厉害增长点，带来大量用户增长。如果有大腿可以抱，一定要坚决抱大腿。平时要学会研究自己的产品、用户，找到当前产品真正存在的问题，慢慢去解决它，建立自己的产品壁垒。你也可以去学习优秀产品的玩法，思考他们能成功的本质，找到真正的用户痛点，比如 QQ 浏览器和腾讯视频为何能后来居上？
其实分析师的任务就是做规模和带收入，一直都没变，所以一定要独立思考，不要被各种风带偏。
然后我们来看一个招聘解读，如下图所示。
职责描述中关键点就是指标体系、数据监控、数据分析和建模、A/B 测试。这些关键点与我前面讲的课时是一致的。
国内的用户增长现状 第二部分是国内用户增长的现状，先说一下关于用户增长的书和资料。
关于书，国内主要有这三本书：《增长黑客》《增长黑客实战》《引爆用户增长》。关于用户增长的大会有 Growing IO，每年一次，在会上可以推广自己的 Growing IO 大数据平台。现阶段也有些公司专门成立了用户增长小组。
对于以上信息，我都接触过。我个人觉得书确实很火，内容很有料，唯一瑕疵的就是可落地的干货不多。大会上的干货也不多，更多是以品牌宣传为主，千万不要指望参加一次大会就能学到很多黑科技。如果一个公司没有好好思考就单独成立了用户增长小组，这完全没必要。因为无论是职责还是 KPI 都会与其他组产生很大重合，所以成立用户增长小组要谨慎。
我相信你一定了解一些用户增长的方法，我这里要特意说下一些看似很管用，但落地很难导致效果不好的用户增长方法，如下所示。
 第一是魔法数字，假设我们发现一个用户阅读篇数超过 3 篇，留存将大大提升。基于这个数据，产品就会想让所有用户阅读篇数超过 3 篇，这个结论没有问题，但是在落地时却非常困难。因为这本身是用户的一种很主动的行为，单独让阅读篇数小于 3 篇的人多阅读，本身就非常难。如果做一些活动让他们多去做一些其他行为，到后来你会发现这些用户会流失。 第二是优化渠道结构来提升新增用户留存，数据分析师想要优化渠道结构，这也很难。因为用户量大、质量高的渠道总是有限，其实渠道人员在开始的时候就一直在想这件事。并且渠道链路非常长，很多因素控制不了，反馈周期也需要很久。因为渠道涉及与外部公司合作，所以很多抉择不是你直接能控制的，所以用优化渠道结构对方法，来提升新增用户留存也不靠谱。你如果真的想通过渠道提升新用户留存，最直接的方法就是把低质渠道给砍掉。 第三是流失用户召回，我经常看到很多人进行流失用户分析，然后通过一些手段召回。因为召回的手段很有限，除了 Push 也没有其他手段，Push 还经常被用户吐槽。与其把精力放在召回，还不如放在分析用户流失原因上。  其实有两个很好的增长思维，我也介绍一下。
 北极星指标：一定要找到最核心的指标。  你做产品时，北极星指标一定要找对，找到后需对北极星指标进行拆解，拆解后的指标需与每个团队的 KPI 挂钩。如果每个人都能够知道自己做的每件事是正向还是负向，那 KPI 完成程度就会很直观。
 A/B 测试：目的是公正性和快速反馈性。  A/B 测试有两个原则，第一要基于数据分析来做 A/B 测试，第二 A/B 测试不只是看结果数据，还要看过程数据，排坑是第一步。</description>
    </item>
    
    <item>
      <title>14 | 营销活动：日常运营活动的分析模板</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-15/</link>
      <pubDate>Thu, 16 Jul 2020 23:09:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-15/</guid>
      <description>本课时内容分为三部分：
 营销活动当前现状； 营销活动具体分析； 案例讲解——百度 APP。  营销活动当前现状 我之前在国企工作时，公司经常会做线上和线下活动，所以每天都会看到各种活动捷报。活动结束后，钱是花完了，真实用户数却没涨多少，大多数都被薅羊毛了。营销活动每年都会花很多钱，因此必须要找一个公正的第三方——数据分析师，来做这件事。而数据分析师既然要做，就一定要发挥出自己的专业性，大家都是罗列数字，为何你就是不一样，你的强大逻辑性在哪？
在这种背景下，我们看一下营销活动的运营人员现状。运营人员比较关注活动的三个维度：带来多少用户量的增长，拉来多少新增用户，外界传播量能覆盖多少人。而数据分析师只需在活动期间每天进行效果播报，活动后 1 ~ 2 周内产出活动报告即可。活动报告包括活动参与人数、拉新数、用户画像三部分内容。
数据分析师与营销活动运营人员相比，数据分析师的优势在于快和维度拆解性，劣势在于细节性。因为数据分析师在做分析时，只是开一个大树，在很多具体业务的细节上面，毕竟不是专门做营销活动出身，所以不是特别了解，也可能没去问，导致最后报告结论的解读可能多多少少有些问题。而对于营销活动人员来说，每过一段时间，就要搞各种活动，所以很清楚活动细节。
其实营销活动应该是一件长期的事件，不可能通过某一次活动就能够带来大量的用户增长，因此数据分析师在做这件事时，要保持以下特性。
 分析的连贯性：在活动前、活动中、活动后都要进行分析。 分析的对比性：不要单看活动本身，活动要与活动之间对比，这样才能更好分析什么样的活动更适合产品本身。 分析的公正性：该怎么样就怎么样，拉新、促活、品牌的评判都应该有一套商定好的标准。  营销活动分析无非就两件事：活动效果评估（本活动和活动对比）和活动优化建议。活动之后要对活动进行复盘，那些做得不好，之后可以避免。
营销活动具体分析 第二部分，我们来看营销活动具体应该怎么分析？我们先理一理，实际上在做任何活动之前，活动运营方都花了很多心思。活动之前必然会出文案，找开发，然后跟外面的合作方进行研讨，所有的这一切都会发生得很早。因此分析师要想做好活动分析，在这个时候就要与活动运营方多沟通，知道活动整体是怎么回事。
比如，第一要了解是谁来开发，靠不靠谱？第二要知道活动形式及测试体验，文案可能存在哪些问题。第三要想好活动大概有哪些指标。这些都要提前想一想。
活动前好好准备——前 1 ~ 2 周 在活动前，要好好准备这几件事。
 活动前和运营方商定本次活动的目标。一定要有目标，没有目标的运营不是一个好运营，没有目标的运营绝对不会使出 100% 的力气。这里能很好地培养你业务的敏感性。 活动前和研发沟通好埋点。不是每个研发都很靠谱，即使很靠谱也可能会犯错误。在埋点这件事上，分析师应该是主导地位，包括字段名、埋点位置、上报方式等。 提前搭建好指标体系和报表。一定要提前准备，活动前 1 天才发现问题，这样的情况太常见。 定好输出格式。要想好活动中、活动后每天输出哪些数据，什么形式展现，这些要与业务绑好。  正式活动前一定要好好地准备，对于一般中型的活动一般是提前 1～2 周。如果是大公司的活动，可能前一个月就要好好准备。像双 11 这种特大活动就不是前一个月才准备，可能在活动的前三个月，所有的数据分析师都在准备这件事了。
活动中好好观察——期间每一天，包括预热
其实正式活动，都有预热期，比如双 11 活动是 11 月 11 日，但 11 月 1 日起就已经很热闹了，甚至更早。所以在活动中，应注意以下事项。
 观察第 1 天的数据，这个非常关键。详细看指标体系的报表数据，查看是否有异常。因为前期修改成本非常小，对于负责人的研发也很乐意去解决这件事。 观察 1 ~ 3 天数据，预估活动目标的完成度，活动目标在前期一定是确定好的，这里要看是否要做适当调整。 定时输出活动战报，每天早上输出，让所有人都知道情况。实际上管理层都有一个比较好的心法，就是早上看数据，你不要以为他在群里面没回，实际上都会看。在工作中真实情况可能真的只有运营人员自己知道数据，有一些数据很可能还藏着掖着。这里的数据要注意真实性，该怎么样就怎么样，要敢于暴露问题，这里问题不会很大，因为所有人的目标就是希望把这个活动做好。 活动 1 周后数据复盘，1 周后进行一次详细复盘，并同步给管理层，让更高视野的人来给建议。  活动后好好复盘——公正性 到活动后期，需要好好复盘，其中最关键的是公正性，比如以下几点。</description>
    </item>
    
    <item>
      <title>13 | 竞品分析：教你如何做竞品分析</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-14/</link>
      <pubDate>Thu, 16 Jul 2020 23:08:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-14/</guid>
      <description>今天我主要讲解如何做竞品分析。
本课时内容主要分为三部分。
 为什么要做竞品分析； 竞品分析的步骤； 爱奇艺与优酷的竞品分析。  为什么要做竞品分析 在前面的课时，我介绍的分析方法都是针对自身 APP，像指标体系、流量分析、路径分析。
假设你当前所处的行业是老大，这个时候肯定要防止外来者，警惕行业老二和老三；假设你是当前行业老二，肯定要看老大最近在做什么，从而模仿超越；假设你当前是行业老三或老三以后，一方面肯定要紧跟老大老二，另一方面要放大招，弯道超车。所以在行业中无论处于什么位置都要去分析竞品。
在实际工作中，做竞品分析有以下几种情况：
 当你的产品准备进入某个行业时，需要先把该行业的竞品分析清楚，主要侧重行业规模和前景的分析； 当你的产品发展处于下降阶段，需要看竞争对手在做什么，主要侧重头部玩家的玩法分析； 当你的产品发展处于瓶颈阶段，需要看竞争对手的数据和功能迭代，持续监控对手数据，从中寻找突破； 当你的产品发展处于快速上升期，一般不会做竞品分析。  对于一款 APP ，分析师在初期就要监控好竞品的各项数据，只有这样才能保持对竞品数据的敏感性，同时跟自身 APP 数据结合起来思考优化迭代。
到底什么是竞品分析？
网上写的很多分析报告，把竞品的功能罗列太多，这其实是最初级的产品体验分析。竞品分析绝不是大而全地把竞品的功能罗列一遍，也不是日常的竞品数据监控，配置一张报表就完事。
竞品分析包含两个点：
 竞品的选择：哪些才是竞品？不要小看这件事，很多产品经理都没想清楚。并不是所有的头部行家都是你的竞品，而是要根据你做竞品分析的目的来选择。 分析什么点，这就需要你知道分析的背景是什么，从而有针对性切入。  其中分析什么点最关键，你需要知道你的 leader 想做什么，如果这件事他自己也说不太清楚，那最好先别投入大量时间去做，不是你做得对或不对，而是问题都没搞清楚，即使他是 leader。
竞品分析的步骤 竞品分析分为三步。
第一步：确定分析目的 所有数据分析的第一件事，就是要搞清楚分析的目的是什么？做分析时一定是要带着某种商业意图来做，不要忘记初心。分析目的分以下几种情况：
 当你尝试进入某个新的行业，需要评估可行性。比如唯品突然想做唯品金融，这个时候肯定就要评估可行性。这种竞品分析更加偏行业趋势、市场规模，财务收入，看大数不拘于小节。比如说看一下目前整个金融行业的情况，另外看一下电商类公司的发展，比如京东金融是怎么做的。 纯粹看竞品的功能、玩法和数据，学习优点。人无我有，人有我优，主要以学习为主。这种分析目的比较常见，主要以功能体验、运营手法、具体数据为主，落地性非常强。 通过看竞品的不同版本迭代的功能、玩法和数据，揣摩竞品想干什么，目的以预防为主。看竞品的版本迭代，思考竞品最近的战略中心在哪，这种情况往往是为了满足管理层的需要。  第二步：挑选 1～2 家竞品，进行对比分析 我们知道分析目的之后，下一步需要拉竞品跟我们自身进行对比。这时候首先挑选核心功能一样的 1～2 两家竞品，其次是功能体验分析，再就是运营手法分析。分析竞品的功能是怎么运用的，侧重对比运营手法。最后看宏观和微观的数据分析，也就是数据源，竞品数据源很关键，比如基础数据、财务数据、市场数据。
整个的过程是一项由分析师牵头与产品、运营协助的团队任务，同时可能还需要财务、市场部的参与才能完成。
第三步：给初步分析结论 第三步就是在第二步的基础之上给出一些初步结论。
比如你尝试进入某个新的行业需要评估可行性，那你最终的结论就是回答这个问题。是否可以进入这个行业？如果可以进入，步骤是什么？
比如你纯粹看竞品的功能玩法和数据，学习优点。那你最终的结论要落到竞品的什么功能好？接下来产品和运营如何去做？这样做预计能带来多少收益？
比如你只是看竞品不同版本迭代的数据，揣摩竞品想干什么，那你最终的结论就是竞品的下一步战略是什么？我们要不要也做某种尝试？实际上这种非常难，需要管理人去拍板。
优酷爱奇艺会员案例分享 以上是一些方法论，现在我以优酷、爱奇艺为例，具体来说下竞品分析（可能双方 APP 版本有更新，例子请以我截图为准）。
案例背景：
当下小 A 是负责优酷 APP 会员模块的数据分析师，Q3 季度优酷会员增长乏力，而爱奇艺会员仍处于高速增长阶段。管理层希望对爱奇艺会员进行一次分析，学习爱奇艺会员的优点，从而提升优酷会员数。
从案例背景可以看出分析目的是提升优酷会员数，分析对象为爱奇艺。我们来看具体怎么做？
第一步先看基础数据，如果公司有内部数据，直接用即可。如果没有可以使用外部数据源，外部数据源基本上可以从 Google、百度、Questmoblie、百度指数获取，但数据源只是一个参考。对于爱奇艺，其实我们能够拿到很多数据，但不是每一个数据都需要看，关键是你要想清楚看过这些数据后，对后面分析有什么用。
既然是分析会员数，我们首先要知道爱奇艺会员数跟优酷会员数的差距，如图所示。</description>
    </item>
    
    <item>
      <title>12 | 路径分析：用户的使用路径网络分析</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-13/</link>
      <pubDate>Thu, 16 Jul 2020 23:07:48 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-13/</guid>
      <description>今天我主要讲解路径分析。
本课时内容分为三部分：
 路径分析定义； 路径分析案例——以美团 APP 为例； 路径分析思考。  路径分析定义 我们在讲前面的案例中多次提到了漏斗模型，漏斗模型是非常经典的一种分析方法，但所有的漏斗都是人为假设的，也就是事前假设一条关键路径，事后看关键路径的转化数据。
随着各类 APP 的功能模块、坑位越来越多，用户的行为越来越分散化，比如很多 APP 不止有一个核心功能，可能有若干个核心功能。这个时候就要在用户的所有操作行为中，发现一些产品设计之初可能不知道、但非常有意思的用户前后行为，这就是路径分析。也就是说，路径分析是基于数据本身发现的，产品可能不太清楚，但是符合用户习惯的路径。
 漏斗分析：人为设定一条或者若干条漏斗。先有假设再数据验证。 路径分析：基于用户的所有行为，去挖掘出若干条重要的用户路径，通过优化界面交互让产品用起来更加流畅和符合用户习惯，产生更多价值。先有数据再验证假设。跟漏斗分析刚好相反。  我们举个例子。
比如对于美团 APP 来说，我们发现它有很多功能，比如“搜索”“美食”“电影演出”“酒店住宿”“休闲娱乐”“外卖”这 5 个 tab，下面还有“家居”等。然后下面有 4 个小模块：“很优惠”“有格调”“秒杀”“周末去哪儿”，再往下翻是“猜你喜欢”，也就是个人推荐，而在底部 button 有“附近”“发现”“订单”“我的”，各个 button 里面又有很多子模块。
基本上目前市面上大多数 APP 都是这种多坑位，把能做的都做了。在这种情况下，漏斗分析确实完全满足不了日常分析需求，因为漏斗分析相对来说都是人为事先假定的，而且内容比较符合大众认知的习惯，这个时候就要路径分析派上用场了。
这就是路径分析的背景。
路径分析过程 那么我们对路径分析的过程进行一个详细的说明。大家在听第二小模块的时候，一定要把美团 APP 多体验几次，后面会涉及大量的界面交互和路径使用，所以各个模块都要认真看几遍。
日志介绍 我们先说一下日志，因为路径分析实际上都是基于底层日志来做，有些同学可能没有看过公司本身的日志，用户在端内（ APP 内），所有的行为都是以表或者文件存储的，其中记录了用户最详细的行为信息，这就是日志。比如，你打开 APP，实际上在日志里面是有一条记录的，一般都是一行，格式如下图所示：
首先，对于日志我们怎么看？我们看到中间行与行之间是分段的，这个分段就代表每一条记录。这个日志首先是 Key-Value 格式，就以第一条记录来说，imei 和 ip 中间是以逗号分隔，每一条记录与记录间是行的分割。然后有哪些字段呢？我们有用户的设备号 imei、IP、内存、分辨率、机型、系统，event 和 active 事件、版本、子版本、操作时间 Unix time。
所以对于分析师来说，要知道自己公司本身底层日志是以什么格式来存储的，不一定是 Key-Value 格式，这块操作更多偏 Linux 命令，基本上分析师都会一些基本的命令，比如查今天的日志大小或者日志的一些字段，那么在这块查一下就行了。如果一个分析师每天大部分时间跟这种底层日志打交道，那一定是有问题的，因为这一块相对比较独立，更多的时候是数据研发工程师或者后台研发本身就应该做的事情。
日志分析步骤 当我们熟悉了日志的字段以及格式之后，就可以进行路径分析了，因为路径分析本身就是啃日志。路径分析一共分为四个步骤：
 筛选。第一步是对所有功能用户的量级进行查看，筛选出重要功能，因为当前 APP 可能有 100 个子功能，那么到底要看哪些呢？这个时候就要用用户量级来评判了，首先选出这样的功能，找到切入点。 日志关联（抽样）。第二步，就是对筛选出的功能进行时间序列的排序，比如对于一个用户来说，一天可能有 10 个重要功能，那这 10 个重要功能的先后顺序是什么样子？你要先排序，既然是路径分析，肯定有先后，排完序之后就是日志与日志之间的功能数据的匹配，比如用了 A 功能之后有多少用户用了功能 B？这个就是同一份日志相互间匹配，但是一定是先排序好。然后就是关联，对于所有的路径分析，日志关联都是抽样，因为公司的日志可能非常大，如果要做这一步关联，资源是跟不上的，所以抽样就可以了，基本上抽 10 万、 20 万数据就可以。 标准化及画图。第三步就是数据的标准化以及路径画图，因为第二步是相对绝对的数据，比如使用功能 A 的用户是 100 万，然后这 100 万里面有 80 万用了功能 B，实际上还是这种绝对量级的数据，而第三步是让第二步更加可视化以及标准化。 启发。第四步就是在第三步的基础之上，看有没有比较有启发性的路径，大部分公司做路径分析都要从第一步开始，一步一步来做。但是有一些公司在技术层面一二三步已经帮你搞定了，就是底层研发把一二三都已经做了，然后分析师相对来说要轻松，看第四步就可以了，这就比较好。  整个过程不是特别难，但是非常考验耐心，大家可以这样理解，就是你沉浸在日志当中，然后去干各种各样的行为。我们举例子来说，这样比较生动。</description>
    </item>
    
    <item>
      <title>11 | 流量分析：如何分析数据的波动？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-12/</link>
      <pubDate>Thu, 16 Jul 2020 23:07:47 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-12/</guid>
      <description>今天我主要讲解流量分析。
本课时内容分为四部分：
 背景； 渠道分析； 转化与价值分析； 流量波动逻辑性分析。  背景 上课时我讲解了指标体系，建立产品指标体系和报表之后，分析师和业务方最重要的事情就是每天看各种数据，而这个看数据的过程就是流量分析。这里的流量指广义的流量，并不一定单指日活。它是指所有的流量，比如用户从哪儿来，经过什么过程，产生什么价值，如果流量波动了，为何波动。
从流量分析的定义来看，可以分为以下四部分：
 渠道分析——从哪来； 转化分析——经过什么过程； 价值分析——产生什么价值； 波动分析——日常的监控分析。  下面我逐一来讲。
渠道分析 我们先来看渠道分析，渠道分析包含三部分：
 常见渠道及渠道分类； 渠道推广的整个过程； 渠道的关键指标及分析方法。  常见渠道及渠道分类 我们常见的渠道，如下图所示。
渠道分为内部渠道和外部渠道，内部渠道包括内部的产品矩阵。比如，今日头条会给抖音带量，内部渠道往往都是免费使用。外部渠道往往需要付费，包括多个渠道。第一是搜索引擎，比如打开百度会看到有很多广告的推广；第二是 App 内的广告，比如今日头条里会经常看到京东或双 11 的链接；第三是社交媒体，比如微信朋友圈的广告；第四是软件市场，比如应用宝、华为手机市场、豌豆荚等。
内部渠道和外部渠道都是为了拉新、拉增而用。对于一款健康的 App，前期靠渠道（特别是外部渠道）的品牌带量，后期靠自传播或者免费推广。大多公司都会单独设有渠道运营经理岗位，分析师在这部分的价值体现不会很大。
我们再看下渠道的分类，如下图所示。
横坐标是渠道给我们产品带来的量级，从左到右是越来越高。纵坐标是渠道本身的质量，一般我们可以用留存来看，有些同学可能会用收入衡量渠道质量，其实本质一样，因为留存跟收入本身就是高度相关性的指标。假设我们就是用留存来衡量，那么按照量级和质量画这样一个四象限，可以把渠道分为 4 类。
 量级少，但是质量比较高。对于这种渠道，需要扩量，需在扩量的基础上仔细观察数据。 量级多，同时质量也高。代表渠道非常好，这部分渠道需加快变现能力。 量级多，质量不太好。说明用户不匹配，交互有问题，这部分渠道需要拆解，精细化运营。 量级少，质量也差，这部分渠道直接放弃即可。  其实所有的分析都是先分析一级渠道，然后在此基础上进行拆解。比如，一级渠道的 A 渠道留存很差，我们要进一步对 A 渠道进行二级渠道拆解，看是所有二级渠道差还是部分二级渠道差，这里所有质量的好与差都是相对大盘来说。
渠道推广的整个过程 有些同学对渠道的整个推广过程理解不深刻，我举个例子来说明渠道推广的整个流程，如下图所示。
第一步是外部渠道，然后在外部渠道里，放了一个文案展示，点击文案展示之后会有一个落地页，落地页里面会提示用户进行下载，用户下载之后是打开 App ，打开 App 浏览一遍之后就是注册 App，然后直到最后一步退出。从外部渠道到注册的整个过程实际上是一个漏斗，在这里面分析师可以提供一些优化建议。
以百度搜索举例，如下图所示。
假设我搜索“外卖”两个字，出现的第一个链接是“外卖送药 京东到家”，这链接下方有个广告的标签，这是“京东到家”在百度搜索投放的外部渠道。当我点击进去之后，文案展示如上图所示，有“15元优惠券”的文案提示，下方有个横框，你有没有觉得这儿有一点不合理，因为没有提示输入什么。有些用户第一次打开会犹豫这里是不是输入手机号码，当我把手机号码输入进去之后，点击“立即领取”，就会出现一个下载页面。在下载的过程中又有很多弹窗，比如说外部风险提示等。
当 App 下载完成后，用户打开后在里面浏览或者注册，所有的这些步骤就是渠道推广的整个过程。虽然看似很简单的例子，但在页面的交互还是设计又或是产品的文案方面，都有很多优化点。
渠道的关键指标及分析方法 第三是渠道的关键指标以及分析方法。
 关键指标指前期看有效用户数和次留，中期看次日、7 日、30 日留存，后期看 ROI。  其中要注意，由于渠道都是收费的，所以有效用户数会有刷量的嫌疑，所以除了看直接量级，还要看有主动行为的用户数，比如上节课里面的停留大于 3 秒的用户数。所有渠道最终的目的还是商业变现，所以一定要计算每个渠道的 ROI，及时把 ROI 小于 1 的渠道砍掉。</description>
    </item>
    
    <item>
      <title>10 | 指标体系搭建：指标体系的经典四步</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-11/</link>
      <pubDate>Thu, 16 Jul 2020 23:06:47 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-11/</guid>
      <description>从本课时开始，我们正式进入微观模块的学习，其中包含指标体系、流量分析、路径分析、竞品分析、营销活动分析、用户增长分析。这些分析在工作过程中都非常实用，比如营销活动分析，我相信每个公司都会做活动，但具体如何评价活动，很多人都非常困惑。而在所有模块当中，最基本的是指标体系，所以我先讲解指标体系。
本课时内容分为三部分：
 指标体系的定义及选取原则； 建立指标体系的四个步骤； 知乎 App 指标体系实操。  指标体系的定义及选取原则 实际工作过程会出现令人不悦的两种情况。第一种是对于某核心数据，如日活，只知道数据在变化，但是不知道为何变化，特别是处于一个较大跌幅时，产品为了解释这种现象，就会向数据分析师要各种维度的数据。比如，年底汇报时，产品跟数据要各种各样的数据，要了之后发现跟现有数据对不上，数据内部要花费大量时间对各种各样的口径。第二种情况是每隔一段时间，产品都会拉上数据、研发一起对埋点，总是觉得当前的字段不够用，底层日志越来越大，数仓修改的也越来越多，取数越来越慢，错误越来越多。
这两种情况的根本原因都在于缺少指标体系的建设、宣贯（宣传）以及实施。其中宣贯和实施更为关键，因为有一些公司有指标体系的建设，但宣贯不到位，所以实施的时候就更不到位。比如，开会时决定要做哪些指标体系，大家都拍手叫好，但在落地时，很多动作没有做到位，这些都屡见不鲜。
业务方不重视指标体系是感觉指标体系是基建活，离完成 KPI 太远，只有出问题时才会临时重视。数据方不重视指标体系是因为这是一个吃力不讨好的工作，做好了可以，做不好就背锅。甚至有些分析师认为，指标体系只是一个思维导图而已。
要想把指标体系真正说明白并不容易，但如果你都说不明白，你怎么判断你自己真的很懂呢？作为埋点、取数、分析的一切前提，指标体系如果做不好，始终会很乱。
指标体系到底是什么？指标体系是在业务的不同阶段，分析师牵头与业务方协助，制定的一套能从各维度去反映业务状况的待实施框架。
这里面有几个关键点：
 在业务的前期、中期、后期，指标体系不一样； 一定是由分析师牵头与业务方协助，而不是闭门造车； 从各维度去反映业务的核心状况，指标有很多维度； 最后就是一个大实施框架，一定要实施，否则就是浪费大家时间。  而在指标选取时要注重几个原则：根本性、可理解性、结构性。
 根本性：对于核心数据一定要理解到位和准确，如果这里错了，后面基本不用看。 可理解性：所有指标要配上业务解释性，如日活的定义是什么，是产品的打开还是内容的点击还是后台进程在就行。 结构性：能够充分对业务进行解读，如新增用户只是一个大数，我们还需要知道每个渠道的新增用户、每个渠道的新增转化率、每个渠道的新增用户价值等。  建立指标体系的四个步骤 第二个模块是指标体系建立，知道了指标选取原则后，具体如何建立指标体系？可以分为四个步骤。在讲解指标体系建立的过程之前，我们先看一下所有指标的构成，如下图所示。
我们工作过程中遇到的指标都是派生性指标，派生性指标等于原子性指标加修饰词加时间段。修饰词本身是可选项，而原子性指标和时间段是必选项。原子性指标是最基础的，不可拆分的指标，比如交易额、支付金额，下单数。而修饰词往往是基于某种场景，注意它是一个可选的指标，比如是通过搜索带来的交易。时间段是一个必选的指标，比如，时间周期。我们选的是双 11 这一天，通过 1 加 2 加 3 就衍生出一个派生性指标——双 11 这一天通过搜索带来的交易额。如果不需要修饰词，那就是双 11 这一天的交易额。同样像次日留存、日活、月活、日转化率都是派生指标，这就是所有指标的构成。实际上它是由原子性指标加修饰词加时间段组成，这些知识在大家做数据仓库时，非常有用。
具体的指标体系建立分为四个步骤。
第一步：厘清业务阶段和方向 你要知道当前业务处于什么阶段，具体的业务方向是什么。对于一家公司往往会有三个阶段。
 第一阶段：业务前期（创业期），在业务前期更多是想快速抢占市场份额，看公司盘子大小。所以在业务前期最关注用户量，此时的指标体系应该紧密围绕用户量提升做各种维度的拆解，比如说渠道。 第二阶段：业务中期（快速发展期），在业务中期，除了关注盘子的大小，还要看产品的健康度，除了关注前面的用户量走势，更重要的是优化当前的用户量结构。如果留存偏低，必然跟产品模块有关系，是不是某功能流量承接效果太差。 第三阶段：业务后期（成熟发展期），在成熟期看变现能力以及市场份额，整个行业市场格局已定，一定要看收入指标，各种商业化模式的收入，同时做好市场份额和竞品监控，防止后来者居上。  第二步：确定核心指标 第二步最重要的是找到正确的核心指标，相信我，这可不是一件容易的事，不是因为这件事很难，而是所有人重新接受一些客观事实很难。
举个例子，某款产品的日活口径是打开 App，通过不断买量、外部刷量，日活也一直在上升。业务方觉得挺好，但分析师发现，打开 App 的用户中，3 秒跳出率达 30%，这非常不健康。这说明当前的核心指标（日活）有问题，更好的核心指标是停留时长大于 3 秒的用户数。
每个 App 的核心指标都不太一样，所以一定要多花时间去考虑这件事，这个非常重要，不只是看日活和留存。核心指标确定好之后，更重要的是对核心指标进行维度拆解。
第三步：指标核心维度拆解 核心指标的波动必然是由某种维度的波动引起，所以要监控核心指标，本质上还是要监控维度核心指标。通用的拆解方法是先对核心指标进行公式计算，再按照业务路径或者业务模块进行拆解。
比如，当前的核心指标是停留时长大于 3 秒的用户数。那么停留时长大于 3 秒的用户数等于打开进入 App 的用户数乘以停留时长大于 3 秒的占比。</description>
    </item>
    
    <item>
      <title>09 | 销售：传统行业如何做好交易额提升？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-10/</link>
      <pubDate>Thu, 16 Jul 2020 23:05:47 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-10/</guid>
      <description>今天我主要讲解传统销售行业的数据分析案例。
可能很多同学没有接触过销售行业，那如果接到一个陌生行业的数据分析需求，该如何去入手呢？我举个实际案例。
需求分析 这是我之前接到一个 case 的原始需求：
对方还提供了对应的四张表（门店信息表、产品信息表、销售经理表、销售数据表），如下图所示：
前面三张表可以理解为维度表，最后一张表为具体订单表。
 门店信息表：省区、城市、大区、门店编号、门店销售目标。 产品信息表：产品编码、产品名称、产品单价、产品经理、产品销售目标。 销售经理表：销售经理名称、大区，以及销售目标。 销售数据表：以上 3 个表的综合数据，具体的销售数据。  这四个表的需求是什么呢？
 产出 2016 年全国销售状况报告（维度多，包括时间、地域、产品、人等）。 产出 2016 年全国销售状况框架（结构化表现，X-mind 形式）。  针对这样的原始需求，该如何做呢？
实际工作过程中，原始需求往往比较模糊，数据分析师要跟业务方良好地沟通，因为有些业务方表达能力可能真的不太好。
核心指标分析 销售额完成率 实际上销售行业的核心指标是销售额完成率，所以我们按照正常业务理解进行维度拆解即可。
首先以 Boss 看报告的角度，去找到一条可以把所有的数据联系起来的清晰路径，一层一层剖析分解即可。分析路径如下图所示：
首先是总体的销售额完成率，假设上年末定的 7 月份预期完成目标是 50% 。我们看一下具体的数据，截止 7 月份目标完成情况，如下图所示：
上图左侧显示总目标为 60.5 亿，目前已完成 32.1 亿，完成率是 53%，预期目标是 50%，实际上已经完成了目标。因为这仅仅体现的是一个大数字，所以要分析总结因为哪些点做得好，哪些点做得不好，从而超额完成目标，这也是数据分析的价值。
我们前面说了总体完成率之后，再看了区域完成率，如下图所示：
从图中我们发现排名前三是华中、西南、东北，相对华南、华东、华北就差一点。
这里面西南地区虽然经济收入相对来说不高，但业绩却排在第 2 。而华北地域经济收入较高，但完成率不到 50%。实际上到了区域完成率还是比较抽象，在抽象化的基础之上，我们要想获得一些有价值的数据，必须要进行一个具体案例的分析。因此可以针对这两个地区，挑选门店单独进行分析。
我们看下门店完成率的排序，如下图所示：
Top 10 完成率最好的是门店 58，已经完成了 90%，而对于完成率最差的十家门店，最后一名是门店 3， 只完成了 37%。Top 10 的门店必然是有一些做得好的点，所以要进一步挖掘，比如以门店 58 为例，它哪里做得好，它肯定有一些可以借鉴的地方。而对于最差的十家门店，我们也要分析差在哪里，提升空间大不大。如果提升空间不大，从减少支出的角度来算，是不是可以建议直接关闭门店。接下来我会以这两个门店为例，进行详细地分析。
先看门店 58 ，我们现在手里有产品、订单、时间段的数据。因为统计的时间是 1 到 7 月份，所以会有一个持续的数据，经过这种数据处理之后，我拉了一张图，如下图所示：</description>
    </item>
    
    <item>
      <title>08 | 游戏：游戏行业的 ROI 和付费率是怎么算的？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-09/</link>
      <pubDate>Thu, 16 Jul 2020 23:04:47 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-09/</guid>
      <description>今天我以欢乐斗地主游戏为例介绍一下游戏行业的数据分析。
本节课内容分为四部分：
 背景； 指标口径； 用户流失分析； 用户付费分析。  背景 在前面两课时，我介绍了互联网和金融的数据思维，其实游戏行业兼具互联网与金融数据思维。我之前体验了狼人杀和欢乐斗地主两款 App，因为个人抵抗力比较差，每天都玩到很晚，对我的工作和生活都有一定影响，最后决定把它们删了，不玩了。所以游戏行业用户两极分化比较严重：要么快速流失，要么玩的时间就很长。
因此本节课重点围绕两个目标：
 尽量让用户晚点流失——流失分析 让花时间的用户多变现——商业分析  我们先来看看欢乐斗地主 App 的界面，如下图所示。
在上图界面的右侧，有经典、排位、残局、比赛的坑位，点进去之后是对应玩法。最右下角的有一个“商城” button （小人推着小推车），点击进去之后如下图所示。
这是欢乐斗地主 App 最重要的界面，后面的讲解我会围绕这些图片来展开。
指标口径 第二个模块是指标口径，指标口径包括常规指标和商业化指标。
常规指标 常规指标包含四类：
常规指标包含四类：
1、 DAU、WAU、MAU
DAU、WAU、MAU 分别指产品的日活、周活以及月活。对于任何产品，首先要看用户规模，用户规模是一个亿还是五千万，要有具体的数值。
2、 留存率
留存率一般是看次留、7 留、30 留存率。留存率是一个比例，反映产品的联系程度。以次留率为例，次留率等于第一天打开欢乐斗地主并且第二天也打开欢乐斗地主的人数除以第一天打开欢乐斗地主的人数。
比如，第一天打开欢乐斗地主 App 的有 1000 人，第二天这 1000 人里面又有 800人打开了App，那么它的次留率为：800/1000 * 100=80%。
3、 渗透率
每个产品都有很多子功能，每个子功能的渗透率等于该模块的使用人数除以该产品的日活。
比如，欢乐斗地主 App 的昨日 DAU 为 1000 人，昨日有 900 人点击“商城”。
那么“商城”渗透率为： 900 / 1000 * 100=90%。
4、 转化率
转化率针对某个连贯路径。它等于使用下一个节点的用户数除以使用上一个节点的用户数。</description>
    </item>
    
    <item>
      <title>07 | 互联网金融：芝麻信用分的建模过程是怎样的？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-08/</link>
      <pubDate>Thu, 16 Jul 2020 23:04:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-08/</guid>
      <description>今天我以芝麻信用为例介绍一下互联网金融行业的数据模型。
本节课内容分为三部分：
 背景； 授信模型； 模型落地。  背景 互联网金融的本质是风控，数据分析师在金融行业基本上有两种角色：
 数据建模师，要求对算法的理解较深，相对来说对行业经验要求不是很高； 风控分析师，除了一定的模型理解能力，还需要大量的行业和法律法规经验。  互联网金融与其他行业不太一样，互联网金融在产品对象上分为 to B 和 to C。
 to B：对企业整体的信用进行评估做整体授信。 to C：对个人的个人信用分。  而无论是 to B 还是 to C，在决策上都是依赖央行征信报告（数据最全）。
数据建模师工作内容 数据建模师平时工作的主要内容是什么呢？我找了一家国内大型的互联网金融公司的职位描述，如下图所示。
关键词为数据源、信用评分卡模型、模型上线监控维护、其他数据挖掘。与其他行业数据分析师差异比较大的是数据源，因为互联网金融行业很多时候要规避风险，怎么去规避风险呢？基于大数据，所以数据源越多越好，因此数据建模师平时要与其他公司进行数据合作或数据采购。
总的来说，数据建模师偏算法，但要很懂业务，不是纯算法分析师。
授信模型 接下来我重点说一下授信模型。模型具体是什么呢？以芝麻信用分来为例，如下图所示。
芝麻信用分是由五大维度构成：
 身份特质：你的学历（高中毕业还是博士毕业），表示人本身的稳定性，长时间改变不了的特质。 履约能力：看你消费后按时还款的能力（是否有房有车），表示人消费的兜底性。 信用历史：看你历史的信用（信用卡有无逾期），表示人本身的诚信。 人脉关系：看你支付宝好友的信用分是不是都很高，表示个人身份的稳定性及弱价值性。 行为偏好：看你是喜欢买价格高的还是低的，这部分数据最重要，表示人本身的当前信息,对产品后续决策有非常大的价值。  芝麻信用能够很好地判断一个人的信用到底好不好，另外一个潜在的价值是可以结合人的行为偏好来做更精准的推荐。
数据源 数据建模授信模型的第一步是数据源，同样以芝麻信用为例（如下图所示）。
从图中可以看出这里有一级分类：身份特质、行为偏好、履约能力、信用历史、人脉关系。这五大维度实际上有很多字段的中文名，每一个维度大概用了哪些字段，这些就是数据源。
这里真实的数据变量有上千个，为什么会有这么多变量呢？实际上数据变量分为原始变量和衍生变量。
 原始变量：是直接存储在数据库里的最基础变量，如每天的交易额（你今天花了多少钱）。 衍生变量：衍生变量是在基础变量的基础之上进行的，因为金融的本质是风险，所以都要对原始变量进行加工转化，一般是三种。  时间维度衍生，最近 1 个月交易额、最近 3 个月交易额。 函数衍生，最大交易额、最小交易额、交易额方差。 比率衍生，最近 1 个月交易额/最近 3 个月交易额。    基于这三种变换就可以对原始变量进行扩充，所以最终的数据模型里的数据变量非常多。在选择变量的时候，基于 RFM 原则，即最近、频次、钱。所有跟这三个属性相关的变量都要先保留，因为金融行业本身就是在和钱打交道。</description>
    </item>
    
    <item>
      <title>06 | 电商数据分析：京东 App 的详细产品分析</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-07/</link>
      <pubDate>Thu, 16 Jul 2020 23:03:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-07/</guid>
      <description>今天我讲一下京东 App 的详细产品分析。
目前电商数据分析比较成熟，整个分析脉络非常庞大，如果我单纯地讲一些分析方法可能会大同小异，所以本节课以零售的北极星指标（关键性指标）交易额为切入点，针对交易额的核心转化率做一些分析，并且对新用户的获取进行一些场景分析。
本课时分为四部分：
 如何看待京东 App； 整体数据的分发效率； 漏斗分析； 新用户分析。  如何看待京东 App 当你拿到一个 App 时，首先是从用户视角去体验，以京东 App 用户视角为例（如下图所示）。
App 的主界面主要分为以下几个模块。
 搜索：流量最大的入口； 广告 Banner：用于各种活动宣传； 导航：十宫格（超市、数码、美妆等十大类主要产品），受众覆盖广，分类相对比较稳定； Feeds 流：电商+内容； 个性化推荐：实现千人千面； 底部 Button：五大主模块（我的、购物车、发现、分类、首页）方便快速查看。  首先作为分析师，视角要高于普通用户，除了知道有哪些功能模块以及所在位置以外，还要更深入且有层次性地去看 App 。比如，当前产品有什么痛点，怎么样去优化？这里有三个问题需要你思考。
 引流（场）：首页作为最大的带量位，分发效率怎么评估。 漏斗（货）：北极星指标交易额只是一个数字，更加重要的是理解这个数字转化的过程。 用户（人）：作为一款非常成熟的 App，老用户相对比较稳定，但新用户获取应该怎么优化。  其中引流是对整个 App 的整体分析，漏斗是对核心路径的分析，用户是对产品当前痛点进行分析。
整体数据分发效率 对于分发效率的评估除了要关注日活、留存、渗透率等常规指标外，还要找到能反映产品问题的指标。比如 CTR 和人均访问页面数，这两个指标就能很好反映产品问题。
 CTR：CTR = 点击 UV / 曝光 UV，反映用户点击欲望的指标。  点击 UV：每天有多少用户点击进入到页面。
曝光 UV：每天有多少用户看到了页面。
这个非常重要，因为只有点击才能产生交易，如果较小，首页问题较大。
 人均访问页面数：总访问页面数（PV）/ 总访问 UV。  总访问页面数 PV：点击所有页面的次数总和是多少。
总访问 UV：点击所有页面的人数总和是多少。</description>
    </item>
    
    <item>
      <title>05 | 多元思维模型：数据分析需要具备的四大能力？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-06/</link>
      <pubDate>Thu, 16 Jul 2020 23:02:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-06/</guid>
      <description>今天我讲一下多元思维模型。
本节课内容一共分为四部分：
 背景； 中观能力； 微观能力； 宏观能力。  背景 目前在整个数据分析行业中，大部分同学偏数据库和机器学习，又或是学计算机专业出身，所以会造成一种行业错觉——会点技术、会点 PPT 就可以做数据分析，感觉门槛比较低；有时候觉得自己做出的分析报告，别人能很快的发现问题，自己也认可，但就是不知道如何避免；有时候针对某个问题，有些人总是能有很多想法，但自己却不知道怎么理解；有些人职业发展的很顺利，而自己却始终有瓶颈。
其实这些困惑都非常正常，所以这一节课的目的就是告诉你，到底掌握好哪些能力才能够成为一名优秀的数据分析师，这也是后面所有课程的一个基础。这一节课会有一点点抽象，但是你要好好学。
先给出数据分析的多元思维模型，就是从中观、微观、宏观三个角度去出发。
 中观能力：中观能力是真正的专业度，看你是否能够发现其他数据分析师在分析中的问题。这个专业度不单指你的技术，而是需要你长期总结和思考。 微观能力：微观能力指有效沟通力+快速发散收敛力，看你是否能够从业务的交流中发现问题，找到方向，很多同学都还没意识到这一点。 宏观能力：宏观能力是洞见性的全局观，能够从社会事件和整个行业发展中找到业务的决策方向，这是极难的能力，同时平台和天赋缺一不可。 怎么获得这些能力呢？我逐一来讲。  中观能力 中观能力指专业度，包括技术理解、逻辑性、价值点三个点。中观能力是反映分析师基本功怎么样、套路熟不熟练、思考到不到位的一种标准。
 技术理解：指对分析时用到的技术理解是否到位，是停留在理论阶段还是在实践阶段。很多同学看了很多数据分析的书籍，理论说起来无所不知，但在实践过程中还是遇到很多坑； 逻辑性：指对整体思考的逻辑性是否欠缺； 价值点：强调价值，你做出来的分析价值在哪。如果现在你是决策者，你敢不敢立马规划落地。  中观能力的提升相对比较容易，基本上就是从他人那里获得有效反馈，所以你做出的分析一定要获得高手的反馈，让他指出一些不足或建议，然后多实践。
（1）技术理解 在数据处理中经常用到数据标准化方法，比如常见的 MAX-MIN （最大最小值）方法、Z-score （z 分数）方法、指数对数法。但这只是理论上的方法，你需要理解到数据标准化的本质目的是去除量纲、量级的差异性，才能在业务中有效地利用。
举例：对于 to B 的金融公司来说，除了头部的大客户（前期资源），剩下的都是中小客户（后期拓展）。大客户和小客户需按照每天的交易额来区分，所以需要我们对交易额及用户进行建模。这时就要用到数据标准化，以 MAX-MIN 方法为例，如果直接用这种方法，会造成除了头部几个数据有数值外，其他基本都是 0。到与业务方沟通时，业务方 leader 会觉得你这个方法很有问题。数据非常稀疏，无论是可用性还是理解性都很困难。
所以，有两个解法：
第一对客户进行先分群，再用 MAX-MIN 进行标准化。
第二以 90% 中位数替代 MAX，消除头部影响，让数据变得不那么稀疏。
实际上所有的技术都是为了让业务更加方便，更加高效，而不是让人很困惑，这就是技术理解。
（2）逻辑性 关于逻辑性，我举一个资讯类 App 的真实案例：数据分析师在研究最近一个月的数据，发现所有与留存相关的因素中，留存和自媒体文章下发的占比存在高度相关性。于是就建议业务方多下发自媒体文章，业务方觉得这个点很好，还真的做了。结果是刚开始几天留存是微涨，后续却大跌。
你觉得这个案例的问题出在哪 ？
其实相关性是一种基于向量的伴随关系，不代表直接的因果关系（但确实是因果关系的一种可能性），也就是说留存和自媒体文章下发的占比是一种伴随关系，而不是因果关系。留存的影响因素非常多，不仅仅是因为某一两个指标就能很好地提升留存。
上述例子后来复盘发现，最近刚好是因为有一些重大热点导致留存提升，而这些重大热点文章是以自媒体文章为主，最终造成了自媒体文章下发占比能提升留存的假象。
所以分析师的逻辑性是非常重要的，每一环节的推导必须要讲究严谨性，不能有侥幸心理。
（3）价值点 第三价值点，强调你做的所有分析一定要有价值点。
我曾经见到一个同学，竟然在业务分析报告里面用到了大量的复杂公式，最后还画了一张非常复杂的技术图，在会议上大家都不好意思反驳他。后来问了业务方，得到的反馈是，整个分析报告用了两个月时间，看似解决了很多技术痛点，但是对业务提升没意义，因为听完不知道怎么去落地。
在数据分析过程中，有些是避免不了的描述性统计，你要快速解决，切记不要耽误时间。而对于指导性、预测性的分析，最花时间也是价值最大。你一定要利用好有效时间找到价值点，即使这个价值点只有 1 个。请注意：有没有价值不是分析师说了算，而是业务方说了算，有些点很好但暂时无法落地，就先不要管它。
中观能力能体现分析师的专业度，基本上就是多沉淀、多思考、多反馈、多总结。
微观能力 第二个是微观能力，我们先看下微观能力的背景。</description>
    </item>
    
    <item>
      <title>04 | 如何挑选适合项目场景的数据分析工具？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-05/</link>
      <pubDate>Thu, 16 Jul 2020 23:01:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-05/</guid>
      <description>今天我讲一下各种数据分析工具的使用场景对比。
本节课内容分为四大部分：
 数据分析整体流程； Excel 常用操作； SQL 常见问题； R 语言以及 Python 脚本案例。  数据分析整体流程 数据分析有一套标准化流程，很多人在做数据分析的时候，不知道怎么去开展或者怀疑自己做出的分析报告是否合理、是否全面，这是因为你对这一套标准化流程没有真正的理解。
来看一下标准化流程的九个步骤：
 明确问题，先把问题定义清楚，因为很多人还没理清问题就直接去看数据了； 搭建框架，定义问题之后再把问题考虑全面、找到一条分析主线； 数据提取，用 MySQL、Hive 等工具提取相关数据； 数据处理，用 Excel、R、Python 处理数据； 数据分析，以数据分析方法论为主来分析数据； 数据展现，用 Tableau、Excel、R、Python 工具把你的数据展现出来； 撰写报告，考验你的文笔功底以及整体逻辑性； 报告演讲，考验你沟通能力，表达能力，被提问能力。所有的报告撰写完成之后不要直接去讲，还是要和业务方进行大量的沟通，如果不提前做好沟通，你在会议或公众场合上讲时很容易被别人挑战； 报告闭环，这是最难也是最大价值的地方。  这里面的数据提取、数据处理、数据展现是数据分析师前期的基本功，以工具为主，都是比较容易学到的，也比较容易完成。而明确问题、搭建框架、撰写报告、报告演讲、报告闭环更多是考验分析师的综合能力以及智商、情商，所以这块往往需要很多时间去沉淀。基于数据分析这一套标准化流程，其中涉及一些你必须要学的工具，下面我简单讲一下这些工具。
 MySQL、Hive：基本上所有的数据获取方式都是通过 MySQL、Hive 这两种语言来实现，同时你要学习一些 Linux 命令，因为在排查数据异常时会用到。你需要对这两门工具超级熟练，因为数据提取环节是不能出错的，这一步有问题，后面就都有问题。 Excel：Excel 是最高频的数据处理工具。工作中你经常遇到的一种情况，你的 leader 直接让你现场画个图，这时你最有可能用 Excel 而不是 R、Python。 R：R 是一门统计型语言，专门为数据分析而生，简单易学，但缺点是计算能力确实比较差，你导入两个 GB 数据就有可能导致死机。 Python：Python 是一门真正的脚本语言，可扩展性极强，算法研发同学必备。而数据分析以 Pands 包为主，其他常用包含爬虫、文本挖掘。  Excel 常用操作 先看 Excel 常用操作，一般通过 SQL 在数据库中提取数据，保存到本地 Excel，所以 Excel 是最基础也是最重要的一个数据分析工具，能用 Excel 坚决不用其他工具。
Excel 对比分析（筛选和色阶功能） 对比分析是数据分析中常见的一种分析方法。所有的数据只有对比才有意义，比如：每年的双 11 都会与之前的双 11 进行消费额对比。在工作中最常见的对比对象就是大盘，比如：新上线一个功能，怎么样评估这个功能效果，除了看功能使用人数，更要做这个功能和大盘的留存对比，如果高于大盘留存，代表这个功能有非常好的正向效应。</description>
    </item>
    
    <item>
      <title>03 | 怎样才更好地转型或成功跳槽？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-04/</link>
      <pubDate>Thu, 16 Jul 2020 22:59:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-04/</guid>
      <description>今天我讲一下中小企业的数据分析工作。
本节课内容一共分为三部分：
 对上节课三个问题进行解答； 日常工作分析； 转型四步法。  三个问题的解答 Q1. 流量波动，数据突然涨了怎么分析——考察分析师的经验怎么样 这个问题非常经典，因为它能够很好地考察分析师的经验，虽然很多分析师工作了很长时间，但依然解答不好这个问题。
举个例子：美团外卖近期的订单量突然下降 5%，需要分析师给出解释并提供下一步建议。
一些经验不足的分析师遇到这个问题时可能会盲目地检查原因，比如是否由口径问题、数据存储问题、产品变化问题等原因导致。但这样的回答都是单点分析，缺少全面性，我们看一下参考答案。
先对命题进行解析，订单量下降 5% 属于什么水位，影响范围有多大。如果发现对收入有重大影响，这个时候 CEO 都可能会关注这件事情，所以要更全面地分析原因。
具体的分析模块包括以下几点（如图所示）。
 常识判断：最近是否有比较重大的节日，用户外出度假旅游导致订单量有所下降。 竞品数据：竞品最近的数据有没有大涨，最近有没有做一些营销活动导致我们的订单量下降。 外部事件：社会上有没有针对外卖的一些负面事件，是否对品牌本身造成影响。 产品变化：用户订单下降的产品本身有没有发布最新的版本，部分功能有缺陷导致用户无法下单。 用户行为：整体的订单量下降是因为全国的订单量均普降，还是因为部分区域的订单量下降。 数据问题：是否是因为采集数据的统计口径发生变化。  当你从这六大模块去分析这件事，面试官会觉得你具有条理性，有架构。最主要是能很快定位到问题，这个非常关键。所以一定要有组织有架构地回答问题，而不是单点进行分析。
Q2. 常用三个 App-考察分析师的思考深度怎么样 这个问题我建议你回答跟应聘岗位相关的 App，比如：你应聘公司的产品是 QQ 音乐。
这个时候你可以说常用的三款 App 是网易云音乐、微信读书、知乎。为什么这里会说微信读书和知乎，是为了给面试官衬托你的亮点——网易云音乐。
面试官进一步问：“就以网易云来说，能不能说下你对这个产品最喜欢的点，以及最想吐槽的点。” 请注意这个问题考察你是不是高于普通用户。
你可以这样回答：“最喜欢网易云的每日推荐，最想吐槽的是很多时候通过搜索来选择听某歌，但是搜索栏里面没有语音输入，同时下面的热门搜索跟我的画像非常不准，都不是我喜欢的，那些歌、明星可能我都没听过”。这个回答代表你已经有了自己的见解，不只是简单使用产品而已。
面试官再问：“好的，那么如果你是产品经理，你会怎么样解决这个问题？”
你要回答：&amp;ldquo;先看热门搜索的点击率多少，如果较低说明确实有问题 ，可以把热门搜索功能与用户的画像匹配，实现千人千面，同时在搜索栏增加语音输入功能。&amp;rdquo;
面试官再问：“怎么样评估这样做就能够带来指标的提升？ ”
你可以回答：“这种先开始小流量 A/B 测试，然后再慢慢放量，如果效果还不错，就全量 。”
面试官问：“能不能说下你对 A/B 测试的理解，如正交性，A/B 测试的设计……”
问到这儿表示面试官对你已经非常感兴趣啦。
这就是面试官问你常用三个 App 问题，如果你回答微信、淘宝、B 站之类，你就不能衬托你的亮点，让面试官接着往下问。
Q3:商业化变现—考察对商业的最终目的是否敏感 实际上分析师日常所有的 PV、UV、MAU、DAU 等都是围绕最终的本质目的——商业变现。
举例：面试官让你举例一款工具类产品，说说如何商业化。
你可以这样回答：“以墨迹天气 App 为例，首页&amp;quot;天气&amp;rdquo; Button 是主流量入口，进行底部下拉时会出现资讯，而在资讯里面有较多广告 App 下载链接， 所以这是一个 App 带量商业化时景 Button，内部有较多旅游景点、住宿类 App下载推荐，所以这里是一个自身 App 高相关商业化推荐 。&amp;ldquo;Me&amp;rdquo; 这个 Button，分生活、娱乐、休闲、游戏四大板块，每个模块都有自己的商业化坑位 。”</description>
    </item>
    
    <item>
      <title>02 | 如何搞定 BAT 大厂的数据分析项目？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-03/</link>
      <pubDate>Thu, 16 Jul 2020 22:58:46 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-03/</guid>
      <description>今天我讲一下 BAT 的数据分析工作。
BAT 招聘解析 通过招聘解析的讲解，一是希望你能了解互联网大厂数据分析师的日常工作内容是什么，揭开大厂招聘的神秘面纱；二是通过他们的岗位要求，了解到自身的差距，并对不足有针对性的面试准备，或提升自己在行业内的技术能力。
阿里 先看下阿里的岗位职责描述，关键词有“代理”“方向”“痛点”“转化”“风险”“创新”“落地”“合作”，岗位要求的关键词有“敏感度”“方法论”“管理整合”。由此可以看出，阿里的分析师岗位对技术要求不是很高，但对人的综合能力要求非常高，需要具备一定的数据敏感度和方法论。之所以没有特别要求技术能力是因为市场上工作三五年的分析师基本上都具备了一定的技术能力，但仍有很多人缺少宏观、中观与微观意识，这也是大部分人的职业瓶颈。
腾讯 再来看下腾讯的岗位职责描述，关键词有“埋点”“异常检测”“决策”“A/B 测试”等，岗位要求包括“海量数据处理”“用户增长”等，可以看出腾讯的岗位职责中规中矩，基本涵盖了 80% 的互联网公司数据分析师的日常工作内容。值得关注的是腾讯特别提到了增长黑客的经验，目前独角兽企业都会要求有这样的经验。
百度 再来看下百度的岗位职责描述，关键词有“分析体系”“专项策略”“用户增长”“逻辑思维”“敏感度”等，可以看出百度对个人的“发散思维”和“策略研究”要求非常高，同时在用户运营领域也要有自己的体系。
我们可以看出对于 BAT，他们会特别关注用户增长，毕竟独角兽企业只有持续增长才能获得发展，而在实际工作中，你也会感同身受，所有项目都是为了用户增长而构建。
日常工作 第二部分就是日常工作，日常工作主要包括数据异常的排查和融入专项，做专项分析并负责 KPI、埋点，指标体系等。
数据异常排查 我们先看下数据异常排查的背景，一般情况下，BAT 的数据产品 DAU 都比较大，动辄几百万上千万，甚至过亿，因此业务方和管理层每天都会盯着核心数据，而在这些核心数据中肯定会有一些数据是波动比较大的。这时，分析师需要对这些波动进行排查并解释原因，如果没有一套方法论面对问题就会很头痛，你可以回想下自己是否面对过这种情况，每天早上面对波动数据无从下手，找不到原因，解决不了，进而浪费很多时间做了很多无用功，感觉永无出头之日。
实际上，数据有较大波动，无非就两个原因：一是目前数据本身有问题；二是业务本身有问题。
如果能够透过问题看本质，你就可以在数据波动方面成为专家。当然数据异常排查是需要一些前期准备的：
 业务理解； 指标口径； 当前数据产出过程。  第一个业务理解，比如某个 App 的 DAU 低于1000w，那么请问这个 DAU 代表的是什么行动的DAU，是在进程中还是需要打开 App，还是必须有主动行为，这理解起来是不一样的。第二个指标口径，同样是 DAU 一千万，是 Android 还是所有系统。第三个产出过程，对于 DAU 一千万目前是由哪份日志做了哪些数据清洗计算出来的，只有了解清楚这些才能够开始异常排查。
举个例子，市场部领导看了某一张日活数据和你提供的数据相差较大，就来询问是怎么回事。实际上这时你首先需要弄清楚他看到的数据表是怎样产出的，然后指标口径是什么，指标的业务含义是什么，只有熟悉这些情况后才能分析出产生差异的原因。
实际工作中，有些分析师在进入一家公司时产品已经比较成熟，但指标口径没有文档化，所以可能对业务理解不深，这个时候面对领导的提问就会手足无措，一旦不能解决问题就会失去信任，所以前期准备工作一定要做好。
有了前期准备工作，接下来就是异常排查步骤了，异常排查主要分三步：
 判断是否异常； 最大概率法则归类； 闭环。  第一步判断是否异常，有四个关键点：
 亲自去看数据准确性，不要人云亦云，比如业务方说 DAU 下降了就立马去调查，这是不对的，而是应该亲自查看数据是否真实，有时候业务方不一定多专业，也会出现错误。 时间轴拉长，看是近期异常（3 个月）还是历史异常，一般分析师看数据时习惯看近一两周或一个月的数据，然后突然出现波峰或波谷就认为数据异常了，但实际上往往不是。我们一定要拉长时间轴，如果仍出现波峰或波谷可能就真的出现异常了。 看和该指标关联的其他指标或其他核心指标是否也异常，比如 DAU 异常时，需要查看自流、渗透率是否异常，如果也异常就需要一起解决，而不是按下葫芦浮起瓢，反复做无用功。 找到一个关键人物（产品/数据），提前沟通，也就是当我们确认是数据异常后，找经验丰富的人提前沟通，看他们对此是否有什么见解，往往经验能够快速的定位问题。  第二步就是最大概率法则原因归类，很多分析师遇到异常时无从下手，抓不到问题主线，无法对问题进行有效分类，而我把异常问题分为了六大类，基本上所有的异常问题都归属于这六大类。
 假期效应：开学季、暑假、四大节、当地节日； 热点事件：常规热点（世界杯）、突发热点（爆款 IP）； 活动影响：双 11、618，公司层面活动； 政策影响：互联网金融监管，快递实名； 底层系统故障：数据传输、存储、清洗有无问题； 统计口径：业务逻辑更改、指标计算方式更改。  所以当我们遇到问题时，就可以按照降序在这六大类中逐一排查找到问题原因。</description>
    </item>
    
    <item>
      <title>01 | 如何解决临时提数需求？</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-02/</link>
      <pubDate>Thu, 16 Jul 2020 22:58:42 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-02/</guid>
      <description>今天主要讲解国企的数据分析工作是什么样的。
国企招聘解析 我们先看第一部分招聘解析。学习招聘解析，我们的主要目的有两个：
 通过对比分析工行、招行、广发、电信四家公司的数据分析岗位，找出不同类型企业对应聘者的具体要求，或者说共性及差异点，从而能够有所准备的去面试。 对岗位职责及相关要求进行提炼和总结，从而让你能够匹配出适合自己的企业。 刚参加工作的人往往比较看重薪酬，但随着工作年限的增长，你会发现自身能力是否可以在工作中得到提升至关重要。  刚参加工作的人往往比较看重薪酬，但随着工作年限的增长，你会发现自身能力是否可以在工作中得到提升至关重要。
而想要找到适合的工作，同时自身能力又能够得到提升，首先需要明确数据分析师发展的三个方向。
 业务型：业务型对分析师的业务理解能力要求非常高。 数据研发型：这个发展方向要求你对技术非常熟练，能够通过技术对数据进行分析。 算法型：算法型要求你数学功底要好，而且课题的研究能力非常强。  了解发展方向后，数据分析师一定要结合自身情况在工作三年后找到自己的定位。
当然，如果在投递简历前就能够从 JD（岗位职责）上看出是否适合自己，从而决定是否投递简历，这对于最终是否能成功进入心仪公司来说事半功倍。
工行 我们先来看工行的数据分析师岗位职责：
你说工行好不好，肯定非常好，但是它对数据挖掘岗位的职责是非常模糊的，而且要求一定的技术基础。如果你不是一个对技术感兴趣的人，而又在一个错误的时间进去，那么你可能会很快跳槽。
招行 下面我们来看下招行的数据分析岗位职责：
包括分析大零售客户群、标签、客户细分及客户画像等，要求非常具体。在专业上也提出了具体要求：金融、统计、经济、管理和计算机专业。如果你符合以上专业，那么就有很大的机会进入招行。而招行作为后起之秀，往往追求一定的创新，所以如果你思维发达，喜欢挑战，招行还是比较适合的。
广发 下面我们再来看下广发银行的数据分析岗位职责：
从它的岗位职责描述来看，偏向业务型，事情繁杂，这时你就需要看是否适合自身的发展了。但它的应聘要求并不十分明确，优秀的数据分析能力，对数据敏感，有一定的市场敏感度，这些都是无法准确衡量的，这时候面试时的临场表现就至关重要了。
电信 最后看下电信的数据分析岗位职责：
相比于银行来说，电信的岗位职责中规中矩，日常工作主要是统计报表、专题分析，要求你具备一定的独立思考能力。
以上是大部分国有企业对数据分析师的岗位要求，你在看招聘信息时，一定要从岗位职责及能力要求进行分析，综合考虑企业目前所处状态及日常工作内容，这对于此岗位是否适合你起关键作用，同样在面试中也非常重要。
国企常规工作 接下来，看下国企数据分析日常工作都做些什么，主要包括三个方面。
 日/周/月报； 临时数据； 常规工作的优化。  日/周/月报 作为一名分析师，日报是每天都需要关注的，但是在日常工作中往往得不到足够的重视。而分析日报主要有三个目的：
 了解业务现状； 培养数据敏感性； 提供业务发展建议。  先说了解业务现状，我问过很多同学：“你为什么不愿意看日报？”很多人会说：“每天数据就那样，没什么好看的。”实际上，这个理解是非常浅层次的，如果数据一直就那样，那就说明公司业务出问题了，而这时如果你能够指出问题，一定能够得到展现自己的机会。
然后是培养数据的敏感性，很多公司在招聘时都要求具有良好的敏感性，但实际中敏感性都是慢慢培养出来的，没有人天生就对具体业务敏感。如果你可以每天关注数据的波动，潜移默化的便会培养一定的敏感性，比如之前每日收入在 500 ~ 600 万之间，突然变成了 650 万，这个时候你能够发现便会并去寻找原因，这便是数据敏感性。
最后就是为业务提供发展建议，因为数据波动时肯定需要寻找波动原因，为何涨跌，久而久之你就会发现产品或运营在做出何种调整后数据会出现涨跌，进而能够为业务发展提供更合理的发展建议。
在日报的基础上，周报就可以看作是一个短期趋势了，因为很多公司的发版周期往往是一周。新版本的效果能够直接体现在周报中，同时一周的数据会更加稳定、更具说服力。
而月报的周期就比较长了，基本上所有公司每个月都会进行一次例会，而在月例会中会对业务数据进行分析，也更能够为接下来的业务提供更合理的发展建议。
数据敏感性和业务的发展建议都是从日报/周报/月报的分析总结中不断积累的，而对于大部分公司来说日/周/月报只是常规性的经营分析，罗列数字，很多时候失去了数据分析的本质意义。
作为一名优秀的数据分析师一定要经常看三种报，培养自身的敏感性，从而找到获取业务增长的发力点。
临时数据 第二块就是临时数据，这是大部分数据分析师职业生涯的第一个痛点。
我面试过很多工作时间较长的同学“你工作中最主要的事情是什么？”，很多人的回答都是在不断地满足临时提数需求，如果你也是这样就非常危险了，一定要想办法进行优化。
目前我将临时提数需求分为两种，一种是管理层的需求，另一种是业务执行人员需求。
 对于高管层的临时提数需求，优先级肯定是最高。但此时一定不能立马去做，而是需要思考为何需要这个数据，通过这个数据能进行什么决策。举个例子，CEO 现在正在与外部公司商谈合作事宜，需要了解用户人均时长，如果你不知道这个背景前提，给出的数据往往有误，这时需要及时的与领导进行沟通。 而业务线的临时提数需求是非常繁多的，比如数据指标口径、数据增长计算，等等，这时就需要你根据自身的实际工作情况合理安排，学会合理地拒绝一些业务线可以自己解决的事情，不断提升自己的工作价值，而现实情况往往是大部分数据分析师都陷在临时数据需求的泥潭。  以“掌上生活” App 最近上线的一个线下餐饮优惠券功能为例，产品经理需要快速得知优惠券的使用人数，如果你拿到这个需求后，立马分析使用人数字段口径是哪个日志，然后写 SQL 把结果反馈给产品经理，那么他可能会发现使用人数不够广，接下来便会要求你提供优惠券下发人数，再从头做一遍，周而复始，你会发现后面还会有使用频次、消费金额，等等需求在等着你，面对这样的窘况如何解决？</description>
    </item>
    
    <item>
      <title>开篇词：数据分析能力，是每个职场人必备的核心竞争力</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-01/</link>
      <pubDate>Thu, 16 Jul 2020 22:58:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-16-the-lagou-data-analysis-01/</guid>
      <description>你好，我是花木，你也可以叫我 Spring。
我本人是学计算机的，毕业后因为机缘巧合入了数据分析这行，如今也有近 10 年时间了。这期间，我在各种类型的公司都待过，一路从数据分析小白到数据分析工程师、高级数据分析工程师，最后到行业数据分析专家。因为非专业出身，我遇到了大多数人都会遇到的问题，踩了大多数人都会踩的坑，走了不少弯路。但也正因为不懂，对于各种业务问题，我都会从不同层次和视角出发去思考和验证，因而对各种层面的问题和解决方案也都有更切实的体会和深入思考。
我平时喜欢和同行业的人沟通交流，而越来越发现，很多新人甚至从业者都对数据分析岗位或者自身职业发展存在困惑，我总结了以下几点：
 做不好：学了很多工具，看了很多学习资料，却还是做不好数据分析？ 没经验：明明精通各类算法，但到了分析数据的时候，却还是败给了懂业务的公司同事！ 没想法：看到的数据只是数据，而领导看到的是机遇和方案！ 低薪水：前同事每两年换次工作，已经薪水翻倍，而自己面试时却节节失利？薪水原地踏步。  我经历过自己摸索的迷茫期，也在项目中体会过灵光乍现的喜悦，你经历的问题，我都经历过。
工作中，我前前后后负责过很多项目，一方面在职场上为公司做了贡献，一方面也成就了自己。
我打造过一款从 0 到 1 的信用分产品，目前仍在金融领域被千万级用户的公司持续使用，而我也是依靠该项目拿到了大厂的 Offer；我也从 0 到 1 摸索过电商领域的亿级用户精细化运营，并且沉淀了一套从渠道到流失用户运营的方法论，目前仍然在多家公司被持续使用；后来，我还负责过工具产品的规模增长和变现，带来规模的 2 倍和收入的 3 倍增长，在业内增长大会上有过多次分享。
工作之余，我经常思考如何能够在“数据为王”的时代，帮助更多的人了解数据分析工作和提升数据分析能力，思索再三，我决定通过专栏的形式，体系化地输出自己的方法和经验。同时，我也希望这个课程可以帮助你以终为始，更好地规划自己的成长路径。
数据分析 = 分析工具 + 分析思维 工作以后我们发现，数据分析行业不乏因为好奇和薪酬而毕业入行的从业者。
然而进入职场后，却发现现实与理想相差甚远，原本非常“有意思”的工作现在变得&amp;quot;让人恼火&amp;rdquo;：学了各种工具，写了很多代码，工作中最常用的却还是按照条件导出数据，自嘲像个&amp;quot;提数机器&amp;rdquo;，没有成就感&amp;hellip;&amp;hellip; 工作热情也一天天消失。
我认为，上面的问题可以归为以下三类。
问题1：缺乏对业务的理解，更多的是被动做事。
很多分析师在做事的时候，充当老好人，别人提什么都做，上班非常忙，成为别人口中的好好先生。自己也想自我提升，但没时间，因为你的时间掌握在了别人手中。
 简单测试一下你：今年产品团队的目标具体是多少，有哪些战略打法，你看过这份报告吗？
 问题2：对数据分析的理解片面化，更多的还是停留在工具层面。
现在很多高校都在开数据分析这个专业，我看过学生们学的课程，像 Python、Tableau、Java 这些工具都有，实际上是有问题的。大数据是很好，但如果不解决业务问题，你玩得再花，都是空架子。现在很多公司面试的时候都写了一大堆代码要求，这本身就是外行人。
问题3：缺乏引路人，想提升但找不到导师
很多悟性不错的同学，在思考自身能力提升的同时，会去和职场上的老同事或者领导沟通，希望得到一些指导。但沟通后发现他们更多还是一些工具层面的指导，对思维的解惑不多，或者也是在重复类似的工作场景，没有更深入地思考问题。以专题报告为例，你发给对方希望他们给些意见，得到的却更多是报告格式、文字描述、图形可视化上的一些意见，对分析套路的建议却非常少。
我想说的是，数据分析不是简单的工具使用和重复的数据处理，数据分析的本质是：从大量事物中发现关键信息，用于直接决策，而不是辅助。
但是，市面上的数据分析资料也多以各类技术工具讲解为主，部分人对数据分析的认知还停留在 SQL 和其他工具操作阶段；很多中小企业的数据分析从业者，又限于自身业务场景问题，无法在本职工作中得到锻炼和成长；想要提升能力、求职体验新鲜的工作内容，却又不了解心仪岗位的要求，不知从哪下手，面试求职又屡屡败北。
怎么更好地解决这些问题呢？
课程设计 在这个课程中，我会依循大多数人学习新技能的方法路径，通过“找定位、扩思维、精方法、知流程、找不足、寻突破”这样一个流程，来带你全方位掌握数据分析。
具体来说，课程分为 5 个模块，23 篇文章：
 模块一，数据分析的行业需求与要求。我会从不同企业的业务类型着手分析，带你掌握不同企业要求的数据分析基本技能。学完这个部分，不论你在从事什么类型的业务，都能找到属于自己的数据化思维与方法。 模块二，拓展你的宏观视野：通过 4 大行业（电商、互联网金融、游戏、传统行业）的知名案例，讲述数据分析思维模型。该模块最大的亮点就是案例实战，比如电商是怎么做数据分析的，游戏又是怎么做的，很多案例你都可以直接去套。同时，我还会给出优秀数据分析人员的能力模型和 4 个评价指标。对应能力模型，你很容易知道一个人处于什么段位。 模块三，聚焦微观方法论：聚焦不同业务分析的分析框架，讲解关键阶段动作，比如流量分析、路径分析、竞品分析、活动分析、用户增长分析等核心操作，带你掌握数据分析的微观方法论。 模块四，知流程，找不足。因为专题报告就是分析师对外推广自己的产品，而一份专题报告实际上是有一套标准化流程的，像问题的定义与拆解、数据的获取与拆解、专题报告的撰写与落地，以及AB测试等。学完后，你就知道怎么写完美的专题报告了。 模块五，人人都是数据分析师：除了以上数据分析的思维与方法，你还需要提升比如行业分析、数据仓库研究、用户研究、时间管理等专业素养，这些可以解决你对于数据分析至关重要的一些大问题，让你具备一个优秀数据分析者的专业素养。  可以看到，这门课基本上没有工具的讲解，都是针对一些具体产品，结合案例来说明数据分析是怎么帮助产品进行优化的。只有这样，每个数据分析从业者才能掌握这个职业的精髓。</description>
    </item>
    
    <item>
      <title>数据仓库之范式理论</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-13-the-concept-of-fanshi/</link>
      <pubDate>Mon, 13 Jul 2020 17:02:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-13-the-concept-of-fanshi/</guid>
      <description>范式概念 关系型数据库设计时，遵照一定的规范要求，目的在于降低数据的冗余性，目前业界范式有：第一范式(1NF)、 第二范式(2NF)、 第三范式(3NF)等。
使用范式的根本目的是：
 减少数据冗余，尽量让每个数据只出现一次 保证数据一致性  缺点是获取数据时，需要通过 join 拼接出最后的数据。
函数依赖    学号 姓名 系名 系主任 课程 分数     1 小明 经济系 王强 高等数据 95   1 小明 经济系 王强 大学英语 87   1 小明 经济系 王强 普通化学 76   2 小莉 经济系 王强 高等数据 72   2 小莉 经济系 王强 大学英语 98   2 小莉 经济系 王强 计算机 88   3 小芳 法律系 刘玲 高等数学 82   3 小芳 法律系 刘玲 法学基础 82    完全函数依赖 (学号，课程)推出分数，但是单独用学号推断不出来分数，那么就可以说：分数完全依赖于(学号，课程)。</description>
    </item>
    
    <item>
      <title>B站优秀Up主记录</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-10-the-up-of-bilibili/</link>
      <pubDate>Fri, 10 Jul 2020 23:11:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-10-the-up-of-bilibili/</guid>
      <description>高校类 华中科技大学  首页 2018-12-17 致敬所有在考研路上的逐梦人 2018-06-15 2018华中科技大学原创毕业主题曲MV —— 年少路远，远方再见 2018-06-10 这一次，笑着离开【2018华中科技大学毕业季大电影】《同学，帮帮忙》首映预告出炉！ 2018-03-07 2018女生节 | 看完这个采访的女生都哭了 2018-03-02 【IN HUST】——情深不自知 2018-02-28 【华中科技大学2018毕业电影】花絮|二轮试镜 2018-02-28 【2018华中科技大学毕业电影】花絮|演员大骗局 2017-11-11 一首《华科男，别哭》送给大家 | 双十一献礼 2017-09-15 「授权填词翻唱」愿得一人心HUST版 毕业献礼，祝愿你半生归来还是那个人 2017-09-15 江城几处清佳，喻山共赏「玉兰辞」 2017-09-15 妈妈，我想对你说…… 2017-09-15 「小科街采」震惊！HUSTer了解华师居然是因为…… 2017-09-15 高考加油，我们在HUST等你  </description>
    </item>
    
    <item>
      <title>中华经典唐诗之王维</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-11-the-poem-of-wangwei/</link>
      <pubDate>Fri, 10 Jul 2020 23:11:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-11-the-poem-of-wangwei/</guid>
      <description>王维（701－761年，一说699－761年），字摩诘，号摩诘居士。河东蒲州（今山西运城）人，祖籍山西祁县。唐朝诗人、画家。
送 别  下马饮君酒，问君何所之？
君言不得意，归卧南山陲。
但去莫复问，白云无尽时。
 山居秋暝  空山新雨后，天气晚来秋。
明月松间照，清泉石上流。
竹喧归浣女，莲动下渔舟。
随意春芳歇，王孙自可留。
 参考文献  诗词名句网——王维  </description>
    </item>
    
    <item>
      <title>英语单词记忆</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-09-the-english-new-words/</link>
      <pubDate>Thu, 09 Jul 2020 15:20:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-09-the-english-new-words/</guid>
      <description>A开头的 abode n. 房屋，家，住所 
搭 right of abode 居住权 
eg. I do not dare to linger in that gloom-hidden abode.
我不敢在那个隐没于黑暗中的房屋里逗留。 
E开头的 exotic n. 外来的，奇异的，醒目的，吸引人的 
搭 exotic experience 异国体验 
同 foreign(a. 外国的，外来的)
反 native(a. 本国的，本地的); indigenous(a. 本土的)
 evaporate v. (使)蒸发，消失，不复存在 
P开头的 privilege n. 特权，优惠 vt. 给予特权，特别优待 
eg. Education is not a privilege of some people, but a right of all citizens.
受教育不是某些人的特权，而是所有公民的权利。</description>
    </item>
    
    <item>
      <title>数据开发岗位面试准备</title>
      <link>https://xiaohao890809.github.io/2020/2020-07-04-the-jd-of-data-analysis/</link>
      <pubDate>Sat, 04 Jul 2020 21:01:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-07-04-the-jd-of-data-analysis/</guid>
      <description>存储过程 什么是存储过程？  存储过程是一个预编译的 SQL 语句 优点是允许模块化的设计，就是说只需创建一次，以后在该程序中就可以调用多次 如果某次操作需要执行多次 SQL，使用存储过程比单纯 SQL 语句执行要快  用什么来调用？  可以用一个命令对象来调用存储过程 可以供外部程序调用，比如：java 程序  存储过程的优缺点？ 优点：
 存储过程是预编译过的，执行效率高 存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯 安全性高，执行存储过程需要有一定权限的用户 存储过程可以重复使用，可减少数据库开发人员的工作量  缺点：
 移植性差 过程化编程，复杂业务处理的维护成本高 调试不便  hive 分区和分桶的区别 分区 partition
 划分数据集，通过分区减少每次扫描的总数据量 分区使用 hdfs 的子目录功能实现，每个子目录都包含了分区对应的列名和每一列的值，但是 hdfs 并不支持大量的子目录，所以分区的数量是有限制的，要先对表中分区数量进行预估，从而避免分区数量过大带来的问题 分区的划分是非随机的  分桶 bucket
 在分区数量过于庞大以至于可能导致文件系统崩溃时使用分桶来解决问题 分桶是通过对指定列进行哈希计算来实现，使用列的哈希值对数据打散，然后分发到不同的桶中从而完成数据的分桶 在数据量够大的情况下，分桶比分区更有查询效率 分桶的划分是随机的  </description>
    </item>
    
    <item>
      <title>牛客网错题集</title>
      <link>https://xiaohao890809.github.io/2020/2020-06-30-the-wrong-problems-of-nowcoder/</link>
      <pubDate>Tue, 30 Jun 2020 21:43:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2020/2020-06-30-the-wrong-problems-of-nowcoder/</guid>
      <description>知识类 夏天，打开冰箱冷冻室的门，常常看到冷冻室中冒出一股白雾，这是
A. 冰箱里原有水蒸气凝结成小水滴
B. 冰箱里的冰升华后凝结成小水滴
C. 冰箱里的水变成水蒸气
D. 冰箱外部空气中的水变成小水滴
正确答案：D 
打开冰箱冷冻室的门时，冰箱内的低温气体飘散到冰箱外。使周围空气中平时看不见的水蒸气迅速冷却液化，成为很多微小水珠形成了&amp;quot;白雾&amp;rdquo;。
 关于宇航员在太空中的生活，下列说法中不正确的是：
A. 宇航员可以使用特定的加热器对食品加热
B. 宇航员从太空返回地面之后，由于失重，质量会有所增加
C. 宇航员应该睡在固定的睡袋中，以免被气流推动误碰仪器设备开关
D. 在同一航空器中的宇航员可以直接交谈，无需借助无线电通信设备
正确答案：B 
质量是物体的本质属性，在哪都不会改变 
技术类 请问下面哪个赋值语句不是合法的()
A. float a = 3.0
B. int c = 3
C. long d = 3
D. double b = 3.0
正确答案：A  `float a = 3.0f` 
 请阅读下面代码
public class HelloWorld { public static void main(String[] args) { Integer f1 = 100, f2 = 100, f3 = 150, f4 = 150; } } 请问以下哪些判断会返回false()</description>
    </item>
    
    <item>
      <title>居士自习室作业第五周</title>
      <link>https://xiaohao890809.github.io/2019/2019-08-18-the-data-science-learning_5/</link>
      <pubDate>Sun, 18 Aug 2019 12:51:41 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2019/2019-08-18-the-data-science-learning_5/</guid>
      <description>最常用的统计量与抽样分布
统计量
  样本均值：样本均值(sample mean)又叫样本均数，即为样本的均值。均值是表示一组数据集中趋势的量数，是指在一组数据中所有数据之和再除以这组数据的个数。它是反映数据集中趋势的一项指标。
  样本方差：先求出总体各单位变量值与其算术平均数的离差的平方，然后再对此变量取平均数，就叫做样本方差。样本方差用来表示一列数的变异程度。
  变异系数：在概率论和统计学中，变异系数，又称“离散系数”(coefficient of variation)，是概率分布离散程度的一个归一化量度，其定义为标准差与平均值之比。变异系数也被称为标准离差率或单位风险。
  样本矩：有一类常用的统计量是样本的数字特征，他们是模拟总体数字特征构造的，称为样本矩，看看以下两种：
 样本 $k$ 阶原点矩：$\bar{\alpha}_{k}=\frac{1}{n}\sum_{i=1}^{n}x_{i}^{k}$ 样本 $k$ 阶中心矩：$\bar{\beta}_{k}=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^{k}$    样本偏度：样本偏度(sample skewness)一种基本统计量，是样本三阶中心矩除以样本二阶中心矩的3/2次幂的商。
  样本峰度：样本峰度(sample kurtosis)一种基本统计量，样本的峰度和偏度都是作为检验总体分布正态性的统计量。是样本四阶中心矩除以样本二阶中心矩平方的商再减去3。
  抽样分布
  卡方分布：若 $n$ 个相互独立的随机变量 $\xi_{1},\xi_{2},&amp;hellip;,\xi_{n}$ 均服从标准正态分布(也称独立同分布于标准正态分布)，则这 $n$ 个服从标准正态分布的随机变量的平方和构成一新的随机变量，其分布规律称为卡方($\chi^{2}$)分布(chi-square distribution)。
  T分布：假设 $X$ 服从标准正态分布$N(0,1)$，Y服从 $\chi^{2}(n)$ 分布，那么 $Z=\frac{X}{\sqrt{Y/n}}$ 的分布称为自由度为 $n$ 的 T分布,记为$Z\sim t(n)$。
  F分布：若总体$X\sim N(0,1)$，$(X_{1},X_{2},&amp;hellip;,X_{n_{1}})$与$(Y_{1},Y_{2},&amp;hellip;,Y_{n_{2}})$来自 $X$ 的两个独立样本，设统计量$F=\cfrac{\sum_{i=1}^{n_{1}}X_{i}^{2}}{N_{1}}/\cfrac{\sum_{i=1}^{n_{2}}Y_{i}^{2}}{N_{2}}$，则称统计量 $F$ 服从自由度 $n_{1}$ 和 $n_{2}$ 的 F分布，记为$F\sim F(n_{1},n_{2})$。</description>
    </item>
    
    <item>
      <title>居士自习室作业第二周</title>
      <link>https://xiaohao890809.github.io/2019/2019-07-28-the-data-science-learning_2/</link>
      <pubDate>Sun, 28 Jul 2019 17:42:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2019/2019-07-28-the-data-science-learning_2/</guid>
      <description>在用 pandas 处理报表邮件的时候，有时会遇到处理二级表头，并进行合并的例子。例如：
研究了一下 pandas 的多级表头功能，发现它有一个 columns 的设置参数，可以新增两列参数，作为二级表头。
data = [[&amp;#39;2016-07-23&amp;#39;,&amp;#39;整体&amp;#39;,4540,557.34,1235,469,69.23,334], [&amp;#39;2016-07-22&amp;#39;,&amp;#39;整体&amp;#39;,4410,567.34,1135,459,68.23,324], [&amp;#39;2016-07-21&amp;#39;,&amp;#39;整体&amp;#39;,4380,564.34,1115,439,67.23,314]] data_pd = pd.DataFrame(data) data_pd.columns = [[&amp;#39;日期&amp;#39;, &amp;#39;品类&amp;#39;, &amp;#39;下单&amp;#39;, &amp;#39;下单&amp;#39;, &amp;#39;下单&amp;#39;, &amp;#39;支付&amp;#39;, &amp;#39;支付&amp;#39;, &amp;#39;支付&amp;#39;], [&amp;#39;日期&amp;#39;, &amp;#39;品类&amp;#39;, &amp;#39;下单笔数&amp;#39;, &amp;#39;下单金额&amp;#39;, &amp;#39;下单人数&amp;#39;, &amp;#39;成功笔数&amp;#39;, &amp;#39;成功金额&amp;#39;, &amp;#39;成功人数&amp;#39;]] 二级表头处理
发现第一列和第二列并没有合并，下单和支付的大类也没有居中显示。那么怎么样才能实现这样的功能呢？其实 html 里的表格有一个 colspan 参数，我们可以对这个参数进行修改即可，比如，我们可以手动给第二行表头的日期和品类改为删除标识，然后对其进行删除，最后把第一行表头的 colspan 扩展为2，就可以进行合并了，也不影响整体表格的功能，至于居中样式可以设置表格的 style 样式。
# 重新自定义html的格式 def get_type_html(df_html): html = str(df_html).replace(&amp;#39;&amp;lt;table border=&amp;#34;1&amp;#34; class=&amp;#34;dataframe&amp;#34;&amp;gt;&amp;#39;, &amp;#39;&amp;lt;table border=&amp;#34;1&amp;#34; style=&amp;#34;font-family: verdana,arial,sans-serif;font-size:11px;\ color:#333333;border-width: 1px;border-color: #666666;border-collapse: collapse;&amp;#34;&amp;gt;&amp;#39;) html = html.replace(&amp;#39;&amp;lt;td&amp;gt;&amp;#39;, &amp;#39;&amp;lt;td colspan=&amp;#34;1&amp;#34; rowspan=&amp;#34;1&amp;#34; style=&amp;#34;background-color:#FFFFFF;color:#000000;font-weight:normal;\ padding:10px;text-align:center;white-space:pre&amp;#34;&amp;gt;&amp;#39;) &amp;#34;&amp;#34;&amp;#34; html = html.</description>
    </item>
    
    <item>
      <title>居士自习室作业第一周</title>
      <link>https://xiaohao890809.github.io/2019/2019-07-20-the-data-science-learning_1/</link>
      <pubDate>Sat, 20 Jul 2019 22:42:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2019/2019-07-20-the-data-science-learning_1/</guid>
      <description>数据的集中趋势 众数 一组数据中出现次数最多的数值，叫众数。
 注意：有时众数在一组数中有1个或多个，也可能不存在。
 中位数 中位数，又称中点数，中值。
中位数是按顺序排列的一组数据中居于中间位置的数，即在这组数据中，有一半的数据比他大，有一半的数据比他小。
平均数   算数平均数
 算术平均数是指在一组数据中所有数据之和再除以数据的个数。 它是反映数据集中趋势的一项指标。    加权平均数
 加权平均数即将各数值乘以相应的权数，然后加总求和得到总体值，再除以总的单位数。 加权平均数也称加权平均值。    几何平均数
 几何平均数是n个变量值连乘积的n次方根。    分位数 分位数(Quantile)，亦称分位点，是指将一个随机变量的概率分布范围分为几个等份的数值点。
常用的有中位数(即二分位数)、四分位数、百分位数等。
数据的离中趋势 数值型数据   方差
 统计中的方差(样本方差)是每个样本值与全体样本值的平均数之差的平方值的平均数。    标准差
 标准差是方差的算术平方根。    极差
 极差又称范围误差或全距(Range)，以R表示，是用来表示统计资料中的变异量数(measures of variation)。 其最大值与最小值之间的差距，即最大值减最小值后所得之数据。    平均差
 平均差(Mean Deviation)是表示各个变量值之间差异程度的数值之一。 指各个变量值同平均数的离差绝对值的算术平均数。    顺序数据  四分位差  四分位差(quartile deviation)，它是上四分位数(Q3，即位于75%)与下四分位数(Q1，即位于25%)的差。    分类数据  异众比率  异众比率(variation ratio)是统计学名词，是统计学当中研究现象离中趋势的指标之一。 异众比率指的是总体中非众数次数与总体全部次数之比。 换句话说，异众比率指非众数组的频数占总频数的比例。    相对离散程度 离散系数 在概率论和统计学中，离散系数(coefficient of variation)，是概率分布离散程度的一个归一化量度，其定义为标准差 $\sigma$ 与平均值 $\mu$ 之比。</description>
    </item>
    
    <item>
      <title>机器学习基础之概率论</title>
      <link>https://xiaohao890809.github.io/2018/2018-03-07-the-note-of-probability-theory/</link>
      <pubDate>Wed, 07 Mar 2018 23:54:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2018/2018-03-07-the-note-of-probability-theory/</guid>
      <description>机器学习算法中有很多用到概率论知识的地方，比如贝叶斯定理，频繁挖掘，条件概率等，掌握好概率论的基础知识有利于更好的理解机器学习中的相关算法。
三箱零件，其中第一箱 10 个零件，第二箱 20 个零件，第三箱 15 个。检验结果表明第一箱有 1 个不合格，第二箱有 3 个不合格，第三箱有 2 个不合格，从中抽取一个零件，合格的概率有多少？
 记 $A_i$ 为从第 $i$ 个箱子拿的零件，拿到正品的事件记为B。
 $$ \begin{align} P(B)&amp;amp;=P(A_1)\cdot P(B|A_1)+P(A_2)\cdot P(B|A_2)+P(A_3)\cdot P(B|A_3)\\\
&amp;amp;=P(A_1)+P(A_2)\cdot P(B|A_2)+P(A_3)\cdot P(B|A_3)\\\
&amp;amp;=\frac{1}{3}\cdot \frac{9}{10}+\frac{1}{3}\cdot \frac{17}{20}+\frac{1}{3}\cdot \frac{13}{15}\\\
&amp;amp;=0.872 \end{align} $$
问题：求逆向概率$P(A_{1}|B)$，抽到的这个合格品来自箱子 $A_{1}$ 的概率。
$$ \begin{align} P(A_{1}|B)&amp;amp;=\frac{P(A_1\cdot B)}{P(B)}\\\
&amp;amp;=\frac{P(A_1)\cdot P(B|A_1)}{P(B)}\\\
&amp;amp;=\frac{\frac{1}{3}\cdot \frac{9}{10}}{P(B)}\\\
&amp;amp;=\frac{0.3}{0.872}=0.344 \end{align} $$
注意条件概率 $P(B|A_1)$ 和联合概率 $P(A_1\cdot B)$ 的使用。
分类问题概述： 通过对已知类别信息的数据进行学习后获得分类模型(classifier)，利用分类模型对未知类别信息的数据进行分类(classification)。
朴素贝叶斯 分类算法举例——朴素贝叶斯(Naive Bayesian Model, NBM)
$$ \begin{cases} P(C_i|X)=\frac{P(X|C_i)\cdot P(C_i)}{P(X)}\\
P(X|C_i)=\prod_{K=1}^{n}P(X_k|C_i)\\
P(AB)=P(A)\cdot P(B)\\</description>
    </item>
    
    <item>
      <title>VBA知识点总结</title>
      <link>https://xiaohao890809.github.io/2018/2018-03-01-the-note-of-vba/</link>
      <pubDate>Thu, 01 Mar 2018 22:53:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2018/2018-03-01-the-note-of-vba/</guid>
      <description>VBA是一门比较早的语言了，用于处理 Office 办公软件的数据，不过最近听说以后会被 Python 代替（笑脸），现在总结一些使用 VBA 的过程遇到的一些问题以及解决办法。
常见错误 问题1  此文件正由应用程序或另一用户使用。
 解决方案：打开后记得做退出关闭操作。
Set wdApp = GetObject(,&amp;#34;word.application&amp;#34;) If wdApp is Nothing Then Sef wdApp = CreatObject(&amp;#34;word.application&amp;#34;) wdApp.Visible = True End If wdApp.NormalTemplate.Saved = True wdApp.Quit Set wdApp = Nothing 常用函数 获取最大有效行数 Public Function GetLastRow(theSheet As Worksheet, ByVal col As Integer) As Integer Dim findreg As Range, ret As Range Set findreg = theSheet.Columns(col) Set ret = findreg.Find(what:=&amp;#34;*&amp;#34;, searchDirection:=xlPrevious) If Not ret Is Nothing Then GetLastRow = ret.</description>
    </item>
    
    <item>
      <title>正则表达式知识积累</title>
      <link>https://xiaohao890809.github.io/2018/2018-02-27-the-study-of-reg-expression/</link>
      <pubDate>Tue, 27 Feb 2018 23:07:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2018/2018-02-27-the-study-of-reg-expression/</guid>
      <description>众所周知，正则表达式是非常重要的一个模块，在爬虫中用的好的话可以事半功倍，从复杂文本中根据规则去筛选数据等，掌握常用的一些正则通配符，从此刻开始。
正则表达式概念  使用单个字符串来描述匹配一系列符合某个句法规则的字符串 是对字符串操作的一种逻辑公式 应用场景：处理文本和数据  举例分析 大括号 匹配中括号中有任何一个字符，匹配里面的或者情况
ma = re.match(r&amp;#39;\[[\w]\]&amp;#39;,&amp;#39;[a]&amp;#39;) ret = re.findall(r&amp;#39;[abc]f&amp;#39;,&amp;#39;afufobfidlfodcfr&amp;#39;) # [&amp;#39;af&amp;#39;, &amp;#39;bf&amp;#39;, &amp;#39;cf&amp;#39;] 含有换行 正则修饰符re.S可以匹配包括换行在内的所有字符
import re content = &amp;#39;&amp;#39;&amp;#39;Hello 1234567Word-This is a Regex Demo&amp;#39;&amp;#39;&amp;#39; # 非贪婪匹配 result = re.match(&amp;#39;^He.*?(\d+).*?Demo$&amp;#39;, content, re.S) print(result.group(1)) # 1234567 贪婪模式和非贪婪模式 import re # 贪婪模式，最大范围的匹配标准 ret = re.findall(r&amp;#39;&amp;lt;div&amp;gt;(.*)&amp;lt;/div&amp;gt;&amp;#39;,&amp;#39;&amp;lt;div&amp;gt;hello&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;hello&amp;lt;/div&amp;gt;&amp;#39;) print(ret) # [&amp;#39;hello&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;hello&amp;#39;] # 非贪婪模式 ret = re.findall(r&amp;#39;&amp;lt;div&amp;gt;(.*?)&amp;lt;/div&amp;gt;&amp;#39;,&amp;#39;&amp;lt;div&amp;gt;hello&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;hello&amp;lt;/div&amp;gt;&amp;#39;) print(ret) # [&amp;#39;hello&amp;#39;, &amp;#39;hello&amp;#39;] ma = re.match(r&amp;#39;[0-9][a-z]*&amp;#39;,&amp;#39;1bc&amp;#39;) #全部匹配 print(ma.group()) #1bc ma = re.</description>
    </item>
    
    <item>
      <title>VBA实现Excel的笛卡尔积</title>
      <link>https://xiaohao890809.github.io/2018/2018-01-12-the-decare-by-excel/</link>
      <pubDate>Fri, 12 Jan 2018 23:40:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2018/2018-01-12-the-decare-by-excel/</guid>
      <description>最近有一个任务需求，就是把 Excel 里的两列元素，里面的元素有多个信息，将其分别拆分，然后取笛卡尔积，写到一个新的工作簿中。刚开始准备用 Python 实现的，后来想了下，VBA作为 Office 工具的原生态语言，何不用 VBA 实现呢，于是整理了下便写出来了，下面介绍下过程，为方便起见，弄了一些简易的数据，针对不同的数据需要做一些相应的调整。
Excel的原始数据如下：
整个 VBA 的代码框架如下：
类模块 原始数据对应的首行信息，每一列对应一个元素，将其列出，作为一个新的数组。
Public a As String Public b As String Public c As String Public d As String 常量 Public Const maxNum = 100000 Public MyArr(maxNum) As New MyAttr Public MyNum As Integer 主函数 思路是分别拆分每一行的那两列元素，然后将得到的元素做两个循环，遍历写入新的数组中，然后将新的数组传到工作簿中。
获取最大行数 Public Function GetLastRow(theSheet As Worksheet, ByVal col As Integer) As Integer Dim findreg As Range, ret As Range Set findreg = theSheet.</description>
    </item>
    
    <item>
      <title>经典排序法之Python版</title>
      <link>https://xiaohao890809.github.io/2018/2018-01-11-the-classic-sorts/</link>
      <pubDate>Thu, 11 Jan 2018 23:11:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2018/2018-01-11-the-classic-sorts/</guid>
      <description>大学的时候学习的经典算法忘记差不多了，现在赶紧拾起来，基本算法往往是复杂算法的基础，牢记掌握才是王道。
冒泡排序 def BubbleSorted(nums): length = len(nums) for i in range(length-1): for j in range(length-i-1): if nums[j]&amp;gt;nums[j+1]: nums[j],nums[j+1] = nums[j+1],nums[j] return nums 思考
如果原来的列表是有序列表呢，能否优化一下呢？
def bubble_sort(input_list): &amp;#34;&amp;#34;&amp;#34;冒泡排序&amp;#34;&amp;#34;&amp;#34; length = len(input_list) for j in range(length - 1): # 当列表已经是有序列表的，节省空间 count = 0 for i in range(length - j - 1): if input_list[i] &amp;gt; input_list[i+1]: input_list[i], input_list[i+1] = input_list[i+1], input_list[i] count += 1 if count == 0: return 属性：
 最优时间复杂度：$O(n)$ (表示遍历一次没有发现任何可以交换的元素，排序结束) 最坏时间复杂度：$O(n^2)$ 稳定性：稳定  插入排序 def InsertSorted(nums): # 从第二元素开始直到最后一个元素 for i in range(1,len(nums)): tmp = nums[i] print(nums) j = i-1 while j &amp;gt;= 0 and nums[j] &amp;gt; tmp: nums[j+1] = nums[j] j = j - 1 nums[j+1] = tmp return nums 选择排序 def SelectSorted(nums): for i in range(len(nums)-1): minIndex = i for j in range(i+1,len(nums)): if nums[j] &amp;lt; nums[minIndex]: minIndex = j nums[i],nums[minIndex] = nums[minIndex],nums[i] return nums 图片来源：常用算法js版</description>
    </item>
    
    <item>
      <title>保持某些好的习惯</title>
      <link>https://xiaohao890809.github.io/2017/2017-09-27-some-good-habits-need-to-insist/</link>
      <pubDate>Tue, 26 Sep 2017 10:45:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2017/2017-09-27-some-good-habits-need-to-insist/</guid>
      <description>优秀是一种习惯，保持一个良好的习惯往往能引导一种健康的生活姿态，现在记录一些自己认为是比较好的一些习惯，自己经常去看看，用于监督和督促自己。
生活  一周至少运动两次（跑步，爬山，打球等） 晚上11点半准备看书，然后睡觉 一天至少三大杯水 平时多吃点水果 一个月清理一次房间 晚上睡前刷牙  技术  代码写完后多检查下注释有没有写全，没写的补上，以免以后都不知道自己当初写的啥 LeetCode过三遍 js和go语言了解下 正则表达式多熟悉  反思  每天问一遍自己，想进BAT吗，以你现在的能力能进BAT吗，不能的话哪些地方需要加强呢？  工作  平时多看下面试相关的信息，刷刷题 把跟自己业务相关的数据库表多检查下  英语  熟读新概念英语第三册前30篇  </description>
    </item>
    
    <item>
      <title>记录博客的第一天</title>
      <link>https://xiaohao890809.github.io/2015/2015-08-14-first-day/</link>
      <pubDate>Fri, 14 Aug 2015 22:47:23 +0000</pubDate>
      
      <guid>https://xiaohao890809.github.io/2015/2015-08-14-first-day/</guid>
      <description>好久不见，大家好！很高兴，今天终于开通了这个博客，这期间也得到了不少网友的帮助。之前在网上找了很多模板，但是选来选去，最终还是定了这个模板，因为这个模板比较简洁，而且也符合我预期的效果。
首先要感谢 enml，我是引用了他的模板。
先把他的模板下载到本地，然后传到自己建立的仓库下就可以进行修改模板了。将一些基本的信息，比如名字，多说的 id 以及一些图标都加以修改即可。刚开始的模板的导航栏上没有留言板和关于我这一栏，我后来自己补上的。之前模板的一个出口的 page 被我删掉了，因为感觉那个功能不太实用。还有原来的评论系统貌似用的是国外的一个系统，我将其改为了多说的评论系统，非常好用！
其次是感谢 liberize 的帮助
在他的帮助下，我在评论框的头像加上了炫酷的旋转效果，看起来非常漂亮。只是原作者是圆形的图标，而我这个背景如果用了圆形的头像，会有一个阴影在下面，看起来非常不舒服，于是我改为了方形的效果，默认下的头像旋转是没有阴影的，这一点我至今还是很疑惑。
再者感谢 tk 域名
博客搭建好了，但是域名太长，不太方便随时输入。所以也百度了很多资料，看到网上都是说加一个 CNAME 文件，然后把域名加上去，在去域名管理页面加一个 A 地址。折腾了半天也没有成功。最后发现前提应该是自己得有一个 .com 或者 .me 域名，但是我木有。于是乎这条路便走不通了。后来无意间看到 tk 域名的网址，听说进去可以免费注册域名，于是抱着试一试的态度进去了，后来发现其实根本不用那么麻烦，只有把自己的博客地址指向到你要注册的 .tk 域名就 ok 了。省去了很多步骤。而且最后的网页比之前的网页更加完整了，之前没显示出来额头像图标和 github 图标都显示出来了，太惊喜了。只是某些字体不知道怎么回事，还是显示不完美。不过这样已经很满足了。
最后感谢 liberize 和百度文库的帮助
模板本来已经弄得差不多了，后来想来想去看了别人的很多模板，都在首页有分页的功能（PS：分类和标签是没有这个功能的），于是自己也想加上，所以结合了好几个人的模板，最后把每一个标签换了个颜色，显得不是那么单调。最后文章也要截取部分显示在首页中，这部分调式了好久，最后用了下面这个代码搞定了。
post.content | truncate:300 其实就是把中间一部分舍掉了，因为加上中间那一部分的话，是按照文本进行分割的，而我想保留自己原来的格式，于是只取后面一部分就搞定了。当理想的界面出现在 html 上的时候，那个感觉非常好。也许博客到现在格式方面已经差不多了。不过到后面估计还得修改一些小细节，不断地进步才有动力前进！
总结
github 真是个不错的平台，幸好天朝没有进行封杀，里面的资源和牛人特别多。他们秉着开源的精神，无私地分享了自己得许多经验和代码。所以，学无止境，多学习，多总结肯定是没有错的。
但无论如何，万事开头难，希望自己能够坚持下去，记录生活点滴，同时也写一些技术类的博客，虽然比较菜，但是进步空间比较大，加油，耗子！
另外本博客已经使用 hexo 主题，之前的 jekyll 已经不用了。因为这个框架的主题配置起来比较容易，不用修改很多地方。</description>
    </item>
    
  </channel>
</rss>