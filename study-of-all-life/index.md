# 数仓之学无止境


本文主要摘抄数仓面试中的一些常见问题，以便后续查阅复习。
<!--more-->

## 数仓是什么？

数据仓库是一个**面向主题**的、**集成**的、**相对稳定**的、**反映历史变化**的数据集合，用于支持管理决策。

## Hive 是什么？

1. 是基于 Hadoop 的一个数据仓库工具；
1. 可以将结构化的数据映射为一张数据库表；
1. 并提供 HQL(Hive SQL) 查询功能；
1. 底层数据是存储在 HDFS 上；
1. Hive 的本质是将 SQL 语句转换为 MapReduce、Tez 或者 spark 等任务执行；
1. 适用于离线的批量数据计算。

## ETL 是哪三个单词的缩写

- `Extraction` 提取
- `Transformation` 转换
- `Loading` 加载

## 知道笛卡尔积吗？

根据笛卡尔积的定义，JOIN 的两表中的任意一行都会形成一组关系对，如果 A 表有 N 条记录，B 表有 M 条记录，A X B 会生产 N*M 条数据。 [^1]

1. 可 以 通 过 `CROSS JOIN` 来实现笛卡尔积 `select * from A cross join b`
1. 如果不支持 `CROSS JOIN` 的情况下，可以采用 `select * from A join B on 1 = 1` 的方式实现。
1. 第二种方法可能在语法检测阶段就报错不支持，可以转成如下语法 `select * from (select * , '1' as flag from A) t1 on (select *, '1' as flag from B) t2 where t1.flag = t2.flag`

## shuffle 用来干什么？

为了让相同的 key 都到一个 reduce 中进行处理，reduce 要去每个 map 中拉取数据，all-to-all 的过程，跨分区聚集相同的 key。

## 数据质量

### 定义

> 数据质量管理作为数据仓库的一个重要模块，主要可以分为数据的健康标准量化、监控和保障。

数据质量管理是对数据产生、加工、消费的整个数据生命周期的质量管控，具体的维度包括：

- 准确性
- 完整性
- 一致性
- 及时性
- 有效性
- 唯一性

数据生产阶段：由于系统异常或系统流程等问题，导致的数据缺失或数据的不准确。

数据加工和消费阶段：加工过程中，数据**抽取**的**完整**性能否与系统产生的数据保持**一致**、数据**产出**是否**及时**等质量问题。

### 目标

针对数仓体系中的表建立一套质量评估体系，从数据的完整性、准确性、⼀致性、有效性、及时性、唯一性等维度进行评价，去引导对数表的建设和对数表准确性进行合理评估。

### 实施

简单来说就是通过一系列规则，从全链路、多角度去监测一些指标，形成质量报告，对质量进行评价。这里举一些需要监控的指标的例子：

- 表：主键、数据量（行数、占用磁盘大小）；
- 字段：空值行数的占比、重复行数、固定值行数、枚举个数、枚举范围、长度；
- SLA：对外承诺的最晚产出时间（任务延迟时报警）；

每周要进行复盘，对事故、破线、报警个数、报警率、起夜次数进行记录，分析原因，对任务进行优化。

### 数据质量标准分类

- 数据完整性：数据不存在大量的缺失值、不缺少某一日期/部门/地点等部分维度的数据，同时在 ETL 过程当中应保证数据的完整不丢失。验证数据时总数应符合正常规律时间推移，记录数总数的增长符合正常的趋势。
- 数据一致性：数仓各层的数据，应与上一层保持数据一致，最终经过数据清洗转化（ETL）的宽表/指标能和数据源保持一致。

### 数据质量校验

#### 单表数据量监控

一张表的记录数在一个已知的范围内，或者上下浮动不会超过某个阈值。

- 方法：`select count（*）from` 表 `where` 时间等过滤条件。
- 报警触发条件设置：如果数据量不在 [数值下限, 数值上限]， 则触发报警。
- **同比**增加：如果 ((本周的数据量 - 上周的数据量) / 上周的数据量 * 100) 不在 [比例下限，比例上限]，则触发报警。
- **环比**增加：如果 ((今天的数据量 - 昨天的数据量) / 昨天的数据量 * 100) 不在 [比例下限，比例上限]，则触发报警。
- 报警触发条件设置一定要有。如果没有配置的阈值，不能做监控。

监控的指标：日活、周活、月活、留存（日周月）、转化率（日、周、月）、GMV（日、周、月）。

{{< admonition example "例如" true>}}
平常复购率（日周月）大约 30%，某天检测发现只有 20%。
{{< /admonition >}}

#### 单表空值检测

某个字段为空的记录数在一个范围内，或者占总量的百分比在某个阈值范围内。

- 目标字段：选择要监控的字段。
- 方法：`select count(*) from` 表 `where` 目标字段 `is null`。

单次检测：如果（异常数据量）不在 [数值下限, 数值上限]，则触发报警。

#### 单表重复值检测

一个或多个字段没有重复记录。

- 目标字段：选择要监控的字段。
- 第一步先正常统计条数；`select count(*) form` 表；
- 第二步，去重统计；`select count(*) from` 表 `group by` 目标字段；
- 第一步的值和第二步的值做减法，看是否在上下线阀值之内；
- 单次检测：如果（异常数据量）不在 [数值下限, 数值上限]， 则触发报警。

#### 单表值域检测

一个或多个字段是否满足某些规则。

- 目标字段：选择要监控的字段，支持多选。
- 检测规则：填写“目标字段”要满足的条件。
- 阈值配置与“空值检测”相同。

#### 跨表数据量对比

主要针对同步流程，监控两张表的数据量是否一致

- 方法：`count`（本表） - `count`（关联表）
- 阈值配置与“空值检测”相同。

## 数仓架构与分层，这样分层的好处

## 数仓维度建模思想，PK维度建模异同与利弊

## 数仓有哪些主题域，如何划分

## 讲一下你们公司的业务，说一下最复杂的业务场景

## 你们制定了哪些数仓规范，分别说一下

## HDFS的读写过程（client，namenode，datanode）

## 数据倾斜的场景与解决方法

## Spark宽依赖，窄依赖区别

Spark 中 RDD 的高效与 DAG（有向无环图）有着莫大的关系，在 DAG 调度中我们需要对计算过程划分 stage，而划分依据就是 RDD 之间的依赖关系。

1. 宽依赖：是指 1 个父 RDD 分区对应多个子 RDD 的分区
1. 窄依赖：是指一个或多个父 RDD 分区对应一个子 RDD 分区

宽依赖就是 1 对多，窄依赖就是一对一或者多对一。如图：

{{<image src="/images/kuan1.png" caption="窄依赖" width="350">}}

{{<image src="/images/kuan2.png" caption="窄依赖" width="350">}}

{{<image src="/images/kuan3.png" caption="宽依赖" width="350">}}

{{<image src="/images/kuan4.png" caption="宽窄依赖" width="450">}}

窄依赖是将其聚合到一起，收拢数据，这样我们就可以考虑到我们的一些算子就做此功能比如：

- map
- filter
- union
- join(父RDD是hash-partitioned )
- mapPartitions
- mapValues

宽依赖将其数据进行打散分开，走 `shuffle` 机制与 `mapreduce` 相同。他主要将一些数据进行洗牌和重新分组发牌。这里也有一些算子做此功能：

- groupByKey
- join(父RDD不是hash-partitioned )
- partitionBy
- sort

## join的实现原理

### Join 背景

当前 `SparkSQL` 支持三种 `join` 算法：

- Shuffle Hash Join
- Broadcast Hash Join
- Sort Merge Join

其中前两者归根到底都属于 `Hash Join`，只不过在 `Hash Join` 之前需要先 `Shuffle` 还是先 `Broadcast`。其实，`Hash Join` 算法来自于传统数据库，而 `Shuffle` 和 `Broadcast` 是大数据在分布式情况下的概念，两者结合的产物。因此可以说，大数据的根就是**传统数据库**。`Hash Join`是**内核**。


## 离线数仓最大的挑战是什么，如何克服的，是否沉淀方法论

## 实时数仓如何保障数据质量，有哪些手段和方式

[^1]: 参考链接：[大数据SQL如何实现笛卡尔积](https://blog.csdn.net/firenet1/article/details/125268142)













