[{"categories":["刷题"],"content":"力扣-反转链表 II","date":"2022-12-19","objectID":"/reverse-linked-list-ii/","tags":["力扣","链表"],"title":"力扣-反转链表 II","uri":"/reverse-linked-list-ii/"},{"categories":["刷题"],"content":"🔗 题目链接 给你单链表的头指针 head 和两个整数 left 和 right，其中 left \u003c= right。请你反转从位置 left 到位置 right 的链表节点，返回反转后的链表。 示例1 示例 1： 输入：head = [1,2,3,4,5], left = 2, right = 4 输出：[1,4,3,2,5] 示例 2： 输入：head = [5], left = 1, right = 1 输出：[5] 提示： 链表中节点数目为 $n$ $1 \u003c= n \u003c= 500$ $-500 \u003c= Node.val \u003c= 500$ $1 \u003c= left \u003c= right \u003c= n$ ","date":"2022-12-19","objectID":"/reverse-linked-list-ii/:0:0","tags":["力扣","链表"],"title":"力扣-反转链表 II","uri":"/reverse-linked-list-ii/"},{"categories":["刷题"],"content":"前言 链表的操作问题，一般而言面试（机试）的时候不允许我们修改节点的值，而只能修改节点的指向操作。1 思路通常都不难，写对链表问题的技巧是：一定要先想清楚思路，并且必要的时候在草稿纸上画图，理清「穿针引线」的先后步骤，然后再编码。 ","date":"2022-12-19","objectID":"/reverse-linked-list-ii/:1:0","tags":["力扣","链表"],"title":"力扣-反转链表 II","uri":"/reverse-linked-list-ii/"},{"categories":["刷题"],"content":"穿针引线 我们以下图中黄色区域的链表反转为例。 链表反转 使用「206. 反转链表」的解法，反转 left 到 right 部分以后，再拼接起来。我们还需要记录 left 的前一个节点，和 right 的后一个节点。如图所示： 链表反转 算法步骤： 第 1 步：先将待反转的区域反转； 第 2 步：把 pre 的 next 指针指向反转以后的链表头节点，把反转以后的链表的尾节点的 next 指针指向 succ。 链表反转 说明：编码细节我们不在题解中介绍了，请见下方代码。思路想明白以后，编码不是一件很难的事情。这里要提醒大家的是，链接什么时候切断，什么时候补上去，先后顺序一定要想清楚，如果想不清楚，可以在纸上模拟，让思路清晰。 def reverseBetween(head: ListNode, left: int, right: int) -\u003e ListNode: def reverse_linked_list(head: ListNode): # 也可以使用递归反转一个链表 pre = None cur = head while cur: next = cur.next cur.next = pre pre = cur cur = next # 因为头节点有可能发生变化，使用虚拟头节点可以避免复杂的分类讨论 dummy_node = ListNode(-1) dummy_node.next = head pre = dummy_node # 第 1 步：从虚拟头节点走 left - 1 步，来到 left 节点的前一个节点 # 建议写在 for 循环里，语义清晰 for _ in range(left - 1): pre = pre.next # 第 2 步：从 pre 再走 right - left + 1 步，来到 right 节点 right_node = pre for _ in range(right - left + 1): right_node = right_node.next # 第 3 步：切断出一个子链表（截取链表） left_node = pre.next curr = right_node.next # 注意：切断链接 pre.next = None right_node.next = None # 第 4 步：同第 206 题，反转链表的子区间 reverse_linked_list(left_node) # 第 5 步：接回到原来的链表中 pre.next = right_node left_node.next = curr return dummy_node.next 复杂度分析 时间复杂度：$O(N)$，其中 $N$ 是链表总节点数。最坏情况下，需要遍历整个链表。 空间复杂度：$O(1)$。只使用到常数个变量。 反转链表 II ↩︎ ","date":"2022-12-19","objectID":"/reverse-linked-list-ii/:2:0","tags":["力扣","链表"],"title":"力扣-反转链表 II","uri":"/reverse-linked-list-ii/"},{"categories":["刷题"],"content":"力扣-反转链表","date":"2022-12-18","objectID":"/reverse-linked-list/","tags":["力扣","链表","双指针"],"title":"力扣-反转链表","uri":"/reverse-linked-list/"},{"categories":["刷题"],"content":"🔗 题目链接 给你单链表的头节点 head，请你反转链表，并返回反转后的链表。 示例1 示例 1： 输入：head = [1,2,3,4,5] 输出：[5,4,3,2,1] 示例2 示例 2： 输入：head = [1,2] 输出：[2,1] 示例 3： 输入：head = [] 输出：[] 提示： 链表中节点的数目范围是 $[0, 5000]$ $-5000 \u003c= Node.val \u003c= 5000$ ","date":"2022-12-18","objectID":"/reverse-linked-list/:0:0","tags":["力扣","链表","双指针"],"title":"力扣-反转链表","uri":"/reverse-linked-list/"},{"categories":["刷题"],"content":"双指针迭代 我们可以申请两个指针，第一个指针叫 pre，最初是指向 null 的。1 第二个指针 cur 指向 head，然后不断遍历 cur。 每次迭代到 cur，都将 cur 的 next 指向 pre，然后 pre 和 cur 前进一位。 都迭代完了(cur 变成 null 了)，pre 就是最后一个节点了。 动画演示如下： 动画演示 动画演示中其实省略了一个 tmp 变量，这个 tmp 变量会将 cur 的下一个节点保存起来。 def reverseList(head): \"\"\" :type head: ListNode :rtype: ListNode \"\"\" # 申请两个节点，pre和 cur，pre指向None pre = None cur = head # 遍历链表，while循环里面的内容其实可以写成一行 # 这里只做演示，就不搞那么骚气的写法了 while cur: # 记录当前节点的下一个节点 tmp = cur.next # 然后将当前节点指向pre cur.next = pre # pre和cur节点都前进一位 pre = cur cur = tmp return pre 复杂度分析2 时间复杂度：$O(n)$ 空间复杂度：$O(1)$ 动画演示+多种解法 206. 反转链表 ↩︎ [视频图解]206. 反转链表，迭代 + 递归双解法 ↩︎ ","date":"2022-12-18","objectID":"/reverse-linked-list/:1:0","tags":["力扣","链表","双指针"],"title":"力扣-反转链表","uri":"/reverse-linked-list/"},{"categories":["学习"],"content":"Sqoop or Datax","date":"2022-12-15","objectID":"/sqoop/","tags":["数仓"],"title":"Sqoop or Datax","uri":"/sqoop/"},{"categories":["学习"],"content":"sqoop 和 datax 作为 2 款优秀的数据同步工具，备受数据开发人员喜爱。 ","date":"2022-12-15","objectID":"/sqoop/:0:0","tags":["数仓"],"title":"Sqoop or Datax","uri":"/sqoop/"},{"categories":["学习"],"content":"sqoop ","date":"2022-12-15","objectID":"/sqoop/:1:0","tags":["数仓"],"title":"Sqoop or Datax","uri":"/sqoop/"},{"categories":["学习"],"content":"定义 sqoop 是 apache 旗下一款“Hadoop中的各种存储系统（HDFS、HIVE、HBASE） 和关系数据库（mysql、oracle、sqlserver等）服务器之间传送数据”的工具。 sqoop ","date":"2022-12-15","objectID":"/sqoop/:1:1","tags":["数仓"],"title":"Sqoop or Datax","uri":"/sqoop/"},{"categories":["学习"],"content":"底层工作机制 将导入或导出命令翻译成 MapReduce 程序来实现，在翻译出的 MapReduce 中主要是对 InputFormat 和 OutputFormat 进行定制。 ","date":"2022-12-15","objectID":"/sqoop/:1:2","tags":["数仓"],"title":"Sqoop or Datax","uri":"/sqoop/"},{"categories":["学习"],"content":"datax ","date":"2022-12-15","objectID":"/sqoop/:2:0","tags":["数仓"],"title":"Sqoop or Datax","uri":"/sqoop/"},{"categories":["学习"],"content":"简介 DataX 是一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。 DataX ","date":"2022-12-15","objectID":"/sqoop/:2:1","tags":["数仓"],"title":"Sqoop or Datax","uri":"/sqoop/"},{"categories":["学习"],"content":"核心架构 核心架构 DataX 本身作为离线数据同步框架，采用 Framework + plugin 架构构建。将数据源读取和写入抽象成为 Reader/Writer 插件，纳入到整个同步框架中。 ","date":"2022-12-15","objectID":"/sqoop/:2:2","tags":["数仓"],"title":"Sqoop or Datax","uri":"/sqoop/"},{"categories":["学习"],"content":"核心模块介绍 举例来说，用户提交了一个 DataX 作业，并且配置了 20 个并发，目的是将一个 100 张分表的 mysql 数据同步到 odps 里面。 DataX 的调度决策思路是： DataXJob 根据分库分表切分成了 100 个 Task。 根据 20 个并发，DataX 计算共需要分配 4 个 TaskGroup。 4 个 TaskGroup 平分切分好的 100 个Task，每个 TaskGroup 负责以 5 个并发共计运行 25 个 Task。 ","date":"2022-12-15","objectID":"/sqoop/:2:3","tags":["数仓"],"title":"Sqoop or Datax","uri":"/sqoop/"},{"categories":["学习"],"content":"总结 对于 sqoop 和 datax，如果只是单纯的数据同步，其实两者都是 ok 的，但是如果需要集成在大数据平台，还是比较推荐使用 datax，原因就是支持流量控制，支持运行信息收集，及时跟踪数据同步情况。 ","date":"2022-12-15","objectID":"/sqoop/:3:0","tags":["数仓"],"title":"Sqoop or Datax","uri":"/sqoop/"},{"categories":["刷题"],"content":"力扣-两数相加","date":"2022-12-15","objectID":"/add-two-numbers/","tags":["力扣"],"title":"力扣-两数相加","uri":"/add-two-numbers/"},{"categories":["刷题"],"content":"🔗 题目链接 给你两个非空的链表，表示两个非负的整数。它们每位数字都是按照逆序的方式存储的，并且每个节点只能存储一位数字。 请你将两个数相加，并以相同形式返回一个表示和的链表。 你可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例1 示例 1： 输入：l1 = [2,4,3], l2 = [5,6,4] 输出：[7,0,8] 解释：342 + 465 = 807. 示例 2： 输入：l1 = [0], l2 = [0] 输出：[0] 示例 3： 输入：l1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9] 输出：[8,9,9,9,0,0,0,1] 提示： 每个链表中的节点数在范围 $[1, 100]$ 内 $0 \u003c= Node.val \u003c= 9$ 题目数据保证列表表示的数字不含前导零 整体思路 依次遍历 L1 跟 L2 链表相加，相当于是个位十位百位依次相加 如： $L1 = [1,4,2], L2 = [4,6,5]$1 可看成：$241 + 564 = 805$ 结果为：$[8,0,5]$ 加法 我们将每位数相加后除以 10 的余数记为 val，我们将每位数相加后除以 10 的商记为 tmp # Definition for singly-linked list. # class ListNode: # def __init__(self, val=0, next=None): # self.val = val # self.next = next class Solution: def addTwoNumbers(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u003e Optional[ListNode]: # 初始化链表 head = tree = ListNode() val = tmp = 0 # 当三者有一个不为空时继续循环 while tmp or l1 or l2: val = tmp if l1: val = l1.val + val l1 = l1.next if l2: val = l2.val + val l2 = l2.next tmp = val // 10 val = val % 10 # 实现链表的连接 tree.next = ListNode(val) tree = tree.next return head.next 两数相加python实现 ↩︎ ","date":"2022-12-15","objectID":"/add-two-numbers/:0:0","tags":["力扣"],"title":"力扣-两数相加","uri":"/add-two-numbers/"},{"categories":["刷题"],"content":"力扣-二分查找","date":"2022-12-14","objectID":"/binary-search/","tags":["力扣","二分查找"],"title":"力扣-二分查找","uri":"/binary-search/"},{"categories":["刷题"],"content":"🔗 题目链接 给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。 示例 1： 输入：nums = [-1,0,3,5,9,12], target = 9 输出：4 解释：9 出现在 nums 中并且下标为 4 示例 2： 输入：nums = [-1,0,3,5,9,12], target = 2 输出：-1 解释：2 不存在 nums 中因此返回 -1 提示： 你可以假设 nums 中的所有元素是不重复的。 $n$ 将在 $[1, 10000]$之间。 $nums$ 的每个元素都将在 $[-9999, 9999]$ 之间。 ","date":"2022-12-14","objectID":"/binary-search/:0:0","tags":["力扣","二分查找"],"title":"力扣-二分查找","uri":"/binary-search/"},{"categories":["刷题"],"content":"二分查找 在升序数组 $nums$ 中寻找目标值 $target$，对于特定下标 $i$，比较 $nums[i]$ 和 $target$ 的大小：1 如果 $nums[i]=target$，则下标 $i$ 即为要寻找的下标； 如果 $nums[i]\u003etarget$，则 $target$ 只可能在下标 $i$ 的左侧； 如果 $nums[i]\u003ctarget$，则 $target$ 只可能在下标 $i$ 的右侧。 基于上述事实，可以在有序数组中使用二分查找寻找目标值。 二分查找的做法是，定义查找的范围 $[left,right]$，初始查找范围是整个数组。每次取查找范围的中点 $mid$，比较 $nums[mid]$ 和 $target$ 的大小，如果相等则 $mid$ 即为要寻找的下标，如果不相等则根据 $nums[mid]$ 和 $target$ 的大小关系将查找范围缩小一半。 由于每次查找都会将查找范围缩小一半，因此二分查找的时间复杂度是 $O(log⁡n)$，其中 $n$ 是数组的长度。 二分查找的条件是查找范围不为空，即 $left≤right$。如果 $target$ 在数组中，二分查找可以保证找到 $target$，返回 $target$ 在数组中的下标。如果 $target$ 不在数组中，则当 $left\u003eright$ 时结束查找，返回 $−1$。 def search(nums: list[int], target: int) -\u003e int: l, r = 0, len(nums) - 1 while l \u003c= r: mid = (l + r) // 2 if nums[mid] == target: return mid elif nums[mid] \u003c target: l = mid + 1 else: r = mid - 1 return -1 复杂度分析 时间复杂度：$O(log⁡n)$，其中 $n$ 是数组的长度。 空间复杂度：$O(1)$。 二分查找 ↩︎ ","date":"2022-12-14","objectID":"/binary-search/:1:0","tags":["力扣","二分查找"],"title":"力扣-二分查找","uri":"/binary-search/"},{"categories":["刷题"],"content":"力扣-爬楼梯","date":"2022-12-14","objectID":"/climbing-stairs/","tags":["力扣","动态规划"],"title":"力扣-爬楼梯","uri":"/climbing-stairs/"},{"categories":["刷题"],"content":"🔗 题目链接 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 示例 1： 输入：n = 2 输出：2 解释：有两种方法可以爬到楼顶。 1. 1 阶 + 1 阶 2. 2 阶 示例 2： 输入：n = 3 输出：3 解释：有三种方法可以爬到楼顶。 1. 1 阶 + 1 阶 + 1 阶 2. 1 阶 + 2 阶 3. 2 阶 + 1 阶 提示： $1 \u003c= n \u003c= 45$ 解题思路 $n\u003e2$ 时，第 $n$ 层为 $(n-1)$ 与 $(n-2)$ 层之和1 代码 def climbStairs(n: int) -\u003e int: if n \u003c= 2: return n dp = [0] * n dp[0] = 1 dp[1] = 2 for i in range(2, n): dp[i] = dp[i-2] + dp[i-1] return dp[-1] 爬楼梯 ↩︎ ","date":"2022-12-14","objectID":"/climbing-stairs/:0:0","tags":["力扣","动态规划"],"title":"力扣-爬楼梯","uri":"/climbing-stairs/"},{"categories":["刷题"],"content":"力扣-打家劫舍 III","date":"2022-12-14","objectID":"/house-robber-iii/","tags":["力扣","打家劫舍"],"title":"力扣-打家劫舍 III","uri":"/house-robber-iii/"},{"categories":["刷题"],"content":"🔗 题目链接 小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为 root。 除了 root 之外，每栋房子有且只有一个“父”房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果两个直接相连的房子在同一天晚上被打劫，房屋将自动报警。 给定二叉树的 root 。返回在不触动警报的情况下，小偷能够盗取的最高金额。 示例 1： 输入：root = [3,2,3,null,3,null,1] 输出：7 解释：小偷一晚能够盗取的最高金额 3 + 3 + 1 = 7 示例1 示例 2： 输入：root = [3,4,5,1,3,null,1] 输出：9 解释：小偷一晚能够盗取的最高金额 4 + 5 = 9 示例2 提示： 树的节点数在 $[1, 10^4]$ 范围内 $0 \u003c= Node.val \u003c= 10^4$ 解题思路 递归处理，返回当前节点偷与不偷的两个结果，取其中最大的一个。1 代码 # Definition for a binary tree node. # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: def rob(self, root: TreeNode) -\u003e int: def _rob(root): if not root: return 0, 0 # 偷，不偷 left = _rob(root.left) right = _rob(root.right) # 偷当前节点, 则左右子树都不能偷 v1 = root.val + left[1] + right[1] # 不偷当前节点, 则取左右子树中最大的值 v2 = max(left) + max(right) return v1, v2 return max(_rob(root)) 递归处理-返回偷与不偷两种结果（Python3） ↩︎ ","date":"2022-12-14","objectID":"/house-robber-iii/:0:0","tags":["力扣","打家劫舍"],"title":"力扣-打家劫舍 III","uri":"/house-robber-iii/"},{"categories":["刷题"],"content":"力扣-打家劫舍 II","date":"2022-12-14","objectID":"/house-robber-ii/","tags":["力扣","打家劫舍","动态规划"],"title":"力扣-打家劫舍 II","uri":"/house-robber-ii/"},{"categories":["刷题"],"content":"🔗 题目链接 你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都围成一圈，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，今晚能够偷窃到的最高金额。 示例 1： 输入：nums = [2,3,2] 输出：3 解释：你不能先偷窃 1 号房屋（金额 = 2），然后偷窃 3 号房屋（金额 = 2）, 因为他们是相邻的。 示例 2： 输入：nums = [1,2,3,1] 输出：4 解释：你可以先偷窃 1 号房屋（金额 = 1），然后偷窃 3 号房屋（金额 = 3）。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 3： 输入：nums = [1,2,3] 输出：3 提示： $1 \u003c= nums.length \u003c= 100$ $0 \u003c= nums[i] \u003c= 1000$ ","date":"2022-12-14","objectID":"/house-robber-ii/:0:0","tags":["力扣","打家劫舍","动态规划"],"title":"力扣-打家劫舍 II","uri":"/house-robber-ii/"},{"categories":["刷题"],"content":"前言 这道题是「198. 打家劫舍」的进阶，和第 198 题的不同之处是，这道题中的房屋是首尾相连的，第一间房屋和最后一间房屋相邻，因此第一间房屋和最后一间房屋不能在同一晚上偷窃。1 ","date":"2022-12-14","objectID":"/house-robber-ii/:1:0","tags":["力扣","打家劫舍","动态规划"],"title":"力扣-打家劫舍 II","uri":"/house-robber-ii/"},{"categories":["刷题"],"content":"动态规划 首先考虑最简单的情况。如果只有一间房屋，则偷窃该房屋，可以偷窃到最高总金额。如果只有两间房屋，则由于两间房屋相邻，不能同时偷窃，只能偷窃其中的一间房屋，因此选择其中金额较高的房屋进行偷窃，可以偷窃到最高总金额。 注意到当房屋数量不超过两间时，最多只能偷窃一间房屋，因此不需要考虑首尾相连的问题。如果房屋数量大于两间，就必须考虑首尾相连的问题，第一间房屋和最后一间房屋不能同时偷窃。 如何才能保证第一间房屋和最后一间房屋不同时偷窃呢？ 如果偷窃了第一间房屋，则不能偷窃最后一间房屋，因此偷窃房屋的范围是第一间房屋到最后第二间房屋； 如果偷窃了最后一间房屋，则不能偷窃第一间房屋，因此偷窃房屋的范围是第二间房屋到最后一间房屋。 假设数组 $nums$ 的长度为 $n$。如果不偷窃最后一间房屋，则偷窃房屋的下标范围是 $[0,n−2]$；如果不偷窃第一间房屋，则偷窃房屋的下标范围是 $[1,n−1]$。在确定偷窃房屋的下标范围之后，即可用第 198 题的方法解决。对于两段下标范围分别计算可以偷窃到的最高总金额，其中的最大值即为在 $n$ 间房屋中可以偷窃到的最高总金额。 假设偷窃房屋的下标范围是 $[start,end]$，用 $dp[i]$ 表示在下标范围 $[start,i]$ 内可以偷窃到的最高总金额，那么就有如下的状态转移方程： $$dp[i]=max(dp[i−2]+nums[i],dp[i−1])$$ 边界条件为： $$ \\begin{cases} {dp}[{start}] = {nums}[{start}] \u0026 只有一间房屋，则偷窃该房屋 \\\\ {dp}[{start}+1] = max({nums}[{start}], {nums}[{start}+1]) \u0026 只有两间房屋，偷窃其中金额较高的房屋 \\end{cases} $$ 计算得到 $dp[end]$ 即为下标范围 $[start,end]$ 内可以偷窃到的最高总金额。 分别取 $(start,end)=(0,n−2)$ 和 $(start,end)=(1,n−1)$ 进行计算，取两个 $dp[end]$ 中的最大值，即可得到最终结果。 根据上述思路，可以得到时间复杂度 $O(n)$ 和空间复杂度 $O(n)$ 的实现。考虑到每间房屋的最高总金额只和该房屋的前两间房屋的最高总金额相关，因此可以使用滚动数组，在每个时刻只需要存储前两间房屋的最高总金额，将空间复杂度降到 $O(1)$。 def rob(nums: list[int]) -\u003e int: def robRange(start: int, end: int) -\u003e int: first = nums[start] second = max(nums[start], nums[start + 1]) for i in range(start + 2, end + 1): first, second = second, max(first + nums[i], second) return second length = len(nums) if length == 1: return nums[0] elif length == 2: return max(nums[0], nums[1]) else: return max(robRange(0, length - 2), robRange(1, length - 1)) 复杂度分析 时间复杂度：$O(n)$，其中 $n$ 是数组长度。需要对数组遍历两次，计算可以偷窃到的最高总金额。 空间复杂度：$O(1)$。 打家劫舍 II ↩︎ ","date":"2022-12-14","objectID":"/house-robber-ii/:2:0","tags":["力扣","打家劫舍","动态规划"],"title":"力扣-打家劫舍 II","uri":"/house-robber-ii/"},{"categories":["刷题"],"content":"力扣-打家劫舍","date":"2022-12-14","objectID":"/house-robber/","tags":["力扣","打家劫舍","动态规划"],"title":"力扣-打家劫舍","uri":"/house-robber/"},{"categories":["刷题"],"content":"🔗 题目链接 你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你不触动警报装置的情况下，一夜之内能够偷窃到的最高金额。 示例 1： 输入：[1,2,3,1] 输出：4 解释：偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2： 输入：[2,7,9,3,1] 输出：12 解释：偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。 提示： $1 \u003c= nums.length \u003c= 100$ $0 \u003c= nums[i] \u003c= 400$ ","date":"2022-12-14","objectID":"/house-robber/:0:0","tags":["力扣","打家劫舍","动态规划"],"title":"力扣-打家劫舍","uri":"/house-robber/"},{"categories":["刷题"],"content":"动态规划 首先考虑最简单的情况。如果只有一间房屋，则偷窃该房屋，可以偷窃到最高总金额。如果只有两间房屋，则由于两间房屋相邻，不能同时偷窃，只能偷窃其中的一间房屋，因此选择其中金额较高的房屋进行偷窃，可以偷窃到最高总金额。1 如果房屋数量大于两间，应该如何计算能够偷窃到的最高总金额呢？对于第 $k~(k\u003e2)$ 间房屋，有两个选项： 偷窃第 $k$ 间房屋，那么就不能偷窃第 $k−1$ 间房屋，偷窃总金额为前 $k−2$ 间房屋的最高总金额与第 $k$ 间房屋的金额之和。 不偷窃第 $k$ 间房屋，偷窃总金额为前 $k−1$ 间房屋的最高总金额。 在两个选项中选择偷窃总金额较大的选项，该选项对应的偷窃总金额即为前 $k$ 间房屋能偷窃到的最高总金额。 用 $dp[i]$ 表示前 $i$ 间房屋能偷窃到的最高总金额，那么就有如下的状态转移方程： $$dp[i]=max⁡(dp[i−2]+nums[i],dp[i−1])$$ 边界条件为： $$ \\begin{cases} {dp}[0] = {nums}[0] \u0026 只有一间房屋，则偷窃该房屋 \\\\ {dp}[1] = \\max({nums}[0], {nums}[1]) \u0026 只有两间房屋，选择其中金额较高的房屋进行偷窃 \\end{cases} $$ 最终的答案即为 $dp[n−1]$，其中 $n$ 是数组的长度。 def rob(nums: list[int]) -\u003e int: n = len(nums) if n == 1: return nums[0] dp = [0]*n dp[0] = nums[0] dp[1] = max(nums[0], nums[1]) for i in range(2, n): dp[i] = max(dp[i-2] + nums[i], dp[i-1]) return dp[n-1] 上述方法使用了数组存储结果。考虑到每间房屋的最高总金额只和该房屋的前两间房屋的最高总金额相关，因此可以使用滚动数组，在每个时刻只需要存储前两间房屋的最高总金额。 def rob(nums: list[int]) -\u003e int: if not nums: return 0 size = len(nums) if size == 1: return nums[0] first, second = nums[0], max(nums[0], nums[1]) for i in range(2, size): first, second = second, max(first + nums[i], second) return second 复杂度分析 时间复杂度：$O(n)$，其中 $n$ 是数组长度。只需要对数组遍历一次。 空间复杂度：$O(1)$。使用滚动数组，可以只存储前两间房屋的最高总金额，而不需要存储整个数组的结果，因此空间复杂度是 $O(1)$。 打家劫舍 ↩︎ ","date":"2022-12-14","objectID":"/house-robber/:1:0","tags":["力扣","打家劫舍","动态规划"],"title":"力扣-打家劫舍","uri":"/house-robber/"},{"categories":["学习"],"content":"driver 的功能是什么","date":"2022-12-14","objectID":"/driver/","tags":["数仓"],"title":"driver 的功能是什么","uri":"/driver/"},{"categories":["学习"],"content":"driver 的功能是什么？ 一个 Spark 作业运行时包括一个 Driver 进程，也是作业的主进程，具有 main 函数，并且有 SparkContext 的实例，是程序的人口点 功能：负责向集群申请资源，向 master 注册信息 负责了作业的调度和解析、生成 Stage 并调度 Task 到 Executor 上。包括 DAGScheduler，TaskScheduler ","date":"2022-12-14","objectID":"/driver/:0:0","tags":["数仓"],"title":"driver 的功能是什么","uri":"/driver/"},{"categories":["学习"],"content":"cache 和 persist 区别","date":"2022-12-14","objectID":"/persist/","tags":["数仓","spark"],"title":"cache 和 persist 区别","uri":"/persist/"},{"categories":["学习"],"content":"cache 和 pesist 的区别？ cache() 是 persist() 的特例，persist 可以指定一个 StorageLevel缓存级别 cache 的缓存级别是 memory_only cache 默认是在内存中存储的，而 persist 可以设置存储的级别 持久化级别 ","date":"2022-12-14","objectID":"/persist/:0:0","tags":["数仓","spark"],"title":"cache 和 persist 区别","uri":"/persist/"},{"categories":["刷题"],"content":"力扣-有效三角形的个数","date":"2022-12-12","objectID":"/valid-triangle-number/","tags":["力扣"],"title":"力扣-有效三角形的个数 ","uri":"/valid-triangle-number/"},{"categories":["刷题"],"content":"🔗 题目链接 给定一个包含非负整数的数组 nums，返回其中可以组成三角形三条边的三元组个数。 示例 1： 输入：nums = [2,2,3,4] 输出：3 解释：有效的组合是: 2,3,4 (使用第一个 2) 2,3,4 (使用第二个 2) 2,2,3 示例 2： 输入：nums = [4,2,3,4] 输出：4 提示： $1 \u003c= nums.length \u003c= 1000$ $0 \u003c= nums[i] \u003c= 1000$ ","date":"2022-12-12","objectID":"/valid-triangle-number/:0:0","tags":["力扣"],"title":"力扣-有效三角形的个数 ","uri":"/valid-triangle-number/"},{"categories":["刷题"],"content":"暴力求解法 class Solution: def is_triangle(self, a, b ,c): if a + b \u003e c and abs(a - b) \u003c c: return True elif a + c \u003e b and abs(a - c) \u003c b: return True else: return False def triangleNumber(self, nums: List[int]) -\u003e int: n = len(nums) ret_list = [] for i in range(n-2): for j in range(i+1, n): for k in range(j+1, n): ret = self.is_triangle(nums[i], nums[j], nums[k]) ret_list.append(ret) triangle_list = [item for item in ret_list if item is True] return len(triangle_list) 结果会超出时间限制 ","date":"2022-12-12","objectID":"/valid-triangle-number/:1:0","tags":["力扣"],"title":"力扣-有效三角形的个数 ","uri":"/valid-triangle-number/"},{"categories":["刷题"],"content":"排序 + 二分查找 思路与算法1 对于正整数 $a,b,c$，它们可以作为三角形的三条边，当且仅当： $$ \\begin{cases} a+b\u003ec \\\\ a+c\u003eb \\\\ b+c\u003ea \\end{cases} $$ 均成立。如果我们将三条边进行升序排序，使它们满足 $a≤b≤c$，那么 $a+c\u003eb$ 和 $b+c\u003ea$ 使一定成立的，我们只需要保证 $a+b\u003ec$。 因此，我们可以将数组 $nums$ 进行升序排序，随后使用二重循环枚举 $a$ 和 $b$。设 $a=nums[i],b=nums[j]$，为了防止重复统计答案，我们需要保证 $i\u003cj$。剩余的边 $c$ 需要满足 $c\u003cnums[i]+nums[j]$，我们可以在 $[j+1,n−1]$ 的下标范围内使用二分查找（其中 $n$ 是数组 $nums$ 的长度），找出最大的满足 $nums[k]\u003cnums[i]+nums[j]$ 的下标 $k$，这样一来，在 $[j+1,k]$ 范围内的下标都可以作为边 $c$ 的下标，我们将该范围的长度 $k−j$ 累加入答案。 当枚举完成后，我们返回累加的答案即可。 细节 注意到题目描述中 $nums$ 包含的元素为非负整数，即除了正整数以外，$nums$ 还会包含 $0$。但如果我们将 $nums$ 进行升序排序，那么在枚举 $a$ 和 $b$ 时出现了 $0$，那么 $nums[i]$ 一定为 $0$。此时，边 $c$ 需要满足 $c\u003cnums[i]+nums[j]=nums[j]$，而下标在 $[j+1,n−1]$ 范围内的元素一定都是大于等于 $nums[j]$ 的，因此二分查找会失败。若二分查找失败，我们可以令 $k=j$，此时对应的范围长度 $k−j=0$，我们也就保证了答案的正确性。 def triangleNumber(nums: List[int]) -\u003e int: n = len(nums) nums.sort() ans = 0 for i in range(n): for j in range(i + 1, n): left, right, k = j + 1, n - 1, j while left \u003c= right: mid = (left + right) // 2 if nums[mid] \u003c nums[i] + nums[j]: k = mid left = mid + 1 else: right = mid - 1 ans += k - j return ans 复杂度分析 时间复杂度：$O(n^{2}log⁡n)$，其中 $n$ 是数组 $nums$ 的长度。我们需要 $O(nlog⁡n)$ 的时间对数组 $nums$ 进行排序，随后需要 $O(n^{2}log⁡n)$ 的时间使用二重循环枚举 $a,b$ 的下标以及使用二分查找得到 $c$ 的下标范围。 空间复杂度：$O(log⁡n)$，即为排序需要的栈空间。 有效三角形的个数 ↩︎ ","date":"2022-12-12","objectID":"/valid-triangle-number/:2:0","tags":["力扣"],"title":"力扣-有效三角形的个数 ","uri":"/valid-triangle-number/"},{"categories":["学习"],"content":"left semi join 和 left join 区别","date":"2022-12-12","objectID":"/left-semi-join/","tags":["SQL"],"title":"left semi join 和 left join 区别","uri":"/left-semi-join/"},{"categories":["学习"],"content":"left semi join 和 left join 有啥区别呢？ ","date":"2022-12-12","objectID":"/left-semi-join/:0:0","tags":["SQL"],"title":"left semi join 和 left join 区别","uri":"/left-semi-join/"},{"categories":["学习"],"content":"使用对比 ","date":"2022-12-12","objectID":"/left-semi-join/:1:0","tags":["SQL"],"title":"left semi join 和 left join 区别","uri":"/left-semi-join/"},{"categories":["学习"],"content":"a表数据 select * from wedw_dw.t_user; 表A ","date":"2022-12-12","objectID":"/left-semi-join/:1:1","tags":["SQL"],"title":"left semi join 和 left join 区别","uri":"/left-semi-join/"},{"categories":["学习"],"content":"b表数据 select * from wedw_dw.t_order; 表B ","date":"2022-12-12","objectID":"/left-semi-join/:1:2","tags":["SQL"],"title":"left semi join 和 left join 区别","uri":"/left-semi-join/"},{"categories":["学习"],"content":"left join select * from wedw_dw.t_user t1 left join wedw_dw.t_order t2 on t1.user_id = t2.user_id; 如图所示：a 表和 b 表所有的字段都会展示出来 left join ","date":"2022-12-12","objectID":"/left-semi-join/:1:3","tags":["SQL"],"title":"left semi join 和 left join 区别","uri":"/left-semi-join/"},{"categories":["学习"],"content":"left semi join select * from wedw_dw.t_user t1 left semi join wedw_dw.t_order t2 on t1.user_id = t2.user_id; 如图所示：只能展示 a 表的字段，因为 left semi join 只传递表的 join key 给 map 阶段 left semi join ","date":"2022-12-12","objectID":"/left-semi-join/:1:4","tags":["SQL"],"title":"left semi join 和 left join 区别","uri":"/left-semi-join/"},{"categories":["学习"],"content":"in select * from wedw_dw.t_user t1 where t1.user_id in (select user_id from wedw_dw.t_order); 如图所示：发现效果和 left semi join 是一样的 in ","date":"2022-12-12","objectID":"/left-semi-join/:1:5","tags":["SQL"],"title":"left semi join 和 left join 区别","uri":"/left-semi-join/"},{"categories":["学习"],"content":"inner join select * from wedw_dw.t_user t1 inner join wedw_dw.t_order t2 on t1.user_id = t2.user_id; 如图所示：不会对 b 表有去重操作，会一直遍历 inner join ","date":"2022-12-12","objectID":"/left-semi-join/:1:6","tags":["SQL"],"title":"left semi join 和 left join 区别","uri":"/left-semi-join/"},{"categories":["学习"],"content":"总结 LEFT SEMI JOIN 是 IN/EXISTS子查询 的一种更高效的实现。 LEFT SEMI JOIN 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方都不行。 因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join 则会一直遍历。这就导致右表有重复值得情况下 left semi join 只产生一条，join 所以 left semi join 的性能更高。 left semi join 是只传递表的 join key 给 map 阶段，因此 left semi join 中最后 select 的结果只许出现左表。因为右表只有 join key 参与关联计算了，而 left join 默认是整个关系模型都参与计算了。 ","date":"2022-12-12","objectID":"/left-semi-join/:2:0","tags":["SQL"],"title":"left semi join 和 left join 区别","uri":"/left-semi-join/"},{"categories":["学习"],"content":"星型模型和雪花模型","date":"2022-12-11","objectID":"/star-snow/","tags":["数仓"],"title":"星型模型和雪花模型","uri":"/star-snow/"},{"categories":["学习"],"content":"星型模型和雪花模型的区别以及各自优点。 ","date":"2022-12-11","objectID":"/star-snow/:0:0","tags":["数仓"],"title":"星型模型和雪花模型","uri":"/star-snow/"},{"categories":["学习"],"content":"为什么叫星型模型和雪花模型 星型模型：多维表的数据关系，它由一个事实表和一组维表组成，每个维作为主键 雪花模型：当一个或多个维没有直接连接到事实表上，而是通过其他维表连接到事实表的时候，其图解就像雪花模型连接在一起 使用场景： 雪花模型使用维度分析更加容易，比如“针对特定的广告主，有哪些客户或者公司是在线的?” 星形模型使用指标分析更适合，比如“给定的一个客户他们的收入是多少?” ","date":"2022-12-11","objectID":"/star-snow/:1:0","tags":["数仓"],"title":"星型模型和雪花模型","uri":"/star-snow/"},{"categories":["学习"],"content":"各自的优点 星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一点的冗余。 星型模型 雪花模型是对星型模型的扩展，它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的“层次”区域，这些被分解的表都连接到主维度表而不是事实表。 雪花模型 星型模型因为数据的冗余所以很多统计查询不需要做外部的连接，因此一般情况下效率比雪花模型要高。 星型模型不用考虑很多正规化的因素，设计与实现都比较简单。 雪花模型由于去除了冗余，有些统计就需要通过表的连接才能产生，所以效率不一定有星型模型高。 ","date":"2022-12-11","objectID":"/star-snow/:2:0","tags":["数仓"],"title":"星型模型和雪花模型","uri":"/star-snow/"},{"categories":["学习"],"content":"什么时候使用雪花模型 Ralph Kimball数据仓库大师，讲述了三个例子。对于三个例子，使用雪花模型不仅仅是可接受的，而且可能是一个成功设计的关键。 一个用户维度表且数据量较大。其中，80%的事实度量表是匿名访问者，仅包含少数详细信息。20%的是可靠的注册用户，且这些注册用户有较为详细的信息，与多个维度表中的数据相连。 例如一个金融产品维度表，且这些金融产品有银行类的，保险类等等区别。因此不同种类的产品有自己一系列的特殊属性，且这些属性并非是所有产品共享的。 多个企业共用的日历维度表。但每个企业的财政周期不同，节假日不同等等。在数据仓库的环境中用雪花模型，降低储存的空间，到了具体某个主题的数据集市再用星型模型。 ","date":"2022-12-11","objectID":"/star-snow/:3:0","tags":["数仓"],"title":"星型模型和雪花模型","uri":"/star-snow/"},{"categories":["学习"],"content":"总结 通过上面的对比我们可以发现，在数据仓库建设中大多时候比较适合使用星型模型构建底层数据 Hive 表，通过大量的冗余来提升查询效率，星型模型对 OLAP 的分析引擎支持比较友好，这一点在 Kylin 中比较能体现。 而雪花模型在关系型数据库中如 MySQL，Oracle 中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。 ","date":"2022-12-11","objectID":"/star-snow/:4:0","tags":["数仓"],"title":"星型模型和雪花模型","uri":"/star-snow/"},{"categories":["学习"],"content":"HQL 转换为 MR 任务流程","date":"2022-12-11","objectID":"/sql-mr/","tags":["数仓"],"title":"HQL 转换为 MR 任务流程","uri":"/sql-mr/"},{"categories":["学习"],"content":"sql 如何转化为 MR，整体流程讲一下。 hive-sql 进入程序，利用 Antlr 框架定对 HQL 完成词法语法解析，将 HQL转换为为 AST抽象语法树 遍历 AST，抽象出查询的基本组成单元 QueryBlock查询块 遍历 QueryBlock，将其转换为 OperatorTree操作树（也就是逻辑执行计划） 使用逻辑优化器对 OperatorTree操作树 进行逻辑优化。例如合并不必要的 ReduceSinkOperator，减少 Shuffle 数据量 遍历 OperatorTree，转换为 TaskTree。也就是翻译为 MR 任务的流程，将逻辑执行计划转换为物理执行计划 使用物理优化器对 TaskTree 进行物理优化 生成最终的执行计划，提交任务到 Hadoop 集群运行 参考链接 HQL 转换为 MR 任务流程介绍 一个HQL语句是如何转换成MR任务的？ ","date":"2022-12-11","objectID":"/sql-mr/:0:0","tags":["数仓"],"title":"HQL 转换为 MR 任务流程","uri":"/sql-mr/"},{"categories":["刷题"],"content":"力扣-无重复字符的最长子串","date":"2022-12-11","objectID":"/longest-substring-without-repeating-characters/","tags":["力扣"],"title":"力扣-无重复字符的最长子串 ","uri":"/longest-substring-without-repeating-characters/"},{"categories":["刷题"],"content":"🔗 题目链接 给定一个字符串 s，请你找出其中不含有重复字符的最长子串的长度。 示例 1： 输入：s = “abcabcbb” 输出：3 解释：因为无重复字符的最长子串是 “abc”，所以其长度为 3。 示例 2： 输入：s = “bbbbb” 输出：1 解释：因为无重复字符的最长子串是 “wke”，所以其长度为 3。 请注意，你的答案必须是子串的长度，“pwke” 是一个子序列，不是子串。 示例 3： 输入：s = “pwwkew” 输出：1 解释：因为无重复字符的最长子串是 “b”，所以其长度为 1。 提示： $0 \u003c= s.length \u003c= 5 * 10^{4}$ s 由英文字母、数字、符号和空格组成 ","date":"2022-12-11","objectID":"/longest-substring-without-repeating-characters/:0:0","tags":["力扣"],"title":"力扣-无重复字符的最长子串 ","uri":"/longest-substring-without-repeating-characters/"},{"categories":["刷题"],"content":"滑动窗口 思路和算法 我们先用一个例子考虑如何在较优的时间复杂度内通过本题。1 我们不妨以示例一中的字符串 $abcabcbb$ 为例，找出从每一个字符开始的，不包含重复字符的最长子串，那么其中最长的那个字符串即为答案。对于示例一中的字符串，我们列举出这些结果，其中括号中表示选中的字符以及最长的字符串： 以 $(a)bcabcbb$ 开始的最长字符串为 $(abc)abcbb$； 以 $a(b)cabcbb$ 开始的最长字符串为 $a(bca)bcbb$； 以 $ab(c)abcbb$ 开始的最长字符串为 $ab(cab)cbb$； 以 $abc(a)bcbb$ 开始的最长字符串为 $abc(abc)bb$； 以 $abca(b)cbb$ 开始的最长字符串为 $abca(bc)bb$； 以 $abcab(c)bb$ 开始的最长字符串为 $abcab(cb)b$； 以 $abcabc(b)b$ 开始的最长字符串为 $abcabc(b)b$； 以 $abcabcb(b)$ 开始的最长字符串为 $abcabcb(b)$。 发现了什么？如果我们依次递增地枚举子串的起始位置，那么子串的结束位置也是递增的！这里的原因在于，假设我们选择字符串中的第 $k$ 个字符作为起始位置，并且得到了不包含重复字符的最长子串的结束位置为 $r_{k}$。那么当我们选择第 $k+1$ 个字符作为起始位置时，首先从 $k+1$ 到 $r_{k}$ 的字符显然是不重复的，并且由于少了原本的第 $k$ 个字符，我们可以尝试继续增大 $r_{k}$，直到右侧出现了重复字符为止。 这样一来，我们就可以使用「滑动窗口」来解决这个问题了： 我们使用两个指针表示字符串中的某个子串（或窗口）的左右边界，其中左指针代表着上文中「枚举子串的起始位置」，而右指针即为上文中的 $r_{k}$； 在每一步的操作中，我们会将左指针向右移动一格，表示我们开始枚举下一个字符作为起始位置，然后我们可以不断地向右移动右指针，但需要保证这两个指针对应的子串中没有重复的字符。在移动结束后，这个子串就对应着以左指针开始的，不包含重复字符的最长子串。我们记录下这个子串的长度； 在枚举结束后，我们找到的最长的子串的长度即为答案。 判断重复字符 在上面的流程中，我们还需要使用一种数据结构来判断是否有重复的字符，常用的数据结构为哈希集合（即 C++ 中的 std::unordered_set，Java 中的 HashSet，Python 中的 set, JavaScript 中的 Set）。在左指针向右移动的时候，我们从哈希集合中移除一个字符，在右指针向右移动的时候，我们往哈希集合中添加一个字符。 至此，我们就完美解决了本题。 def lengthOfLongestSubstring(s: str) -\u003e int: # 哈希集合，记录每个字符是否出现过 occ = set() n = len(s) # 右指针，初始值为 -1，相当于我们在字符串的左边界的左侧，还没有开始移动 rk, ans = -1, 0 for i in range(n): if i != 0: # 左指针向右移动一格，移除一个字符 occ.remove(s[i - 1]) while rk + 1 \u003c n and s[rk + 1] not in occ: # 不断地移动右指针 occ.add(s[rk + 1]) rk += 1 # 第 i 到 rk 个字符是一个极长的无重复字符子串 ans = max(ans, rk - i + 1) return ans 无重复字符的最长子串 ↩︎ ","date":"2022-12-11","objectID":"/longest-substring-without-repeating-characters/:1:0","tags":["力扣"],"title":"力扣-无重复字符的最长子串 ","uri":"/longest-substring-without-repeating-characters/"},{"categories":["学习"],"content":"理解 spark 中的 job、stage、task","date":"2022-12-10","objectID":"/stage/","tags":["数仓"],"title":"理解 spark 中的 job、stage、task","uri":"/stage/"},{"categories":["学习"],"content":"一般我们在提交s park 任务的时候，都会去其 UI 界面查看任务运行状况。其中就有 job、stage、task 的一些执行进度展示。今天，就详细说明一下这些名词术语的含义。 ","date":"2022-12-10","objectID":"/stage/:0:0","tags":["数仓"],"title":"理解 spark 中的 job、stage、task","uri":"/stage/"},{"categories":["学习"],"content":"Job spark 中的数据都是抽象为 RDD 的，它支持两种类型的算子操作：Transformation 和 Action。 Transformation 算子的代码不会真正被执行。只有当我们的程序里面遇到一个 action 算子的时候，代码才会真正的被执行。 Transformation 算子主要包括： map mapPartitions flatMap filter union groupByKey repartition cache 等 Action 算子主要包括： reduce collect show count foreach saveAsTextFile 等 当在程序中遇到一个 action 算子的时候，就会提交一个 job，执行前面的一系列操作。因此平时要注意，如果声明了数据需要 cache 或者 persist，但在 action 操作前释放掉的话，该数据实际上并没有被缓存。 通常一个任务会有多个 job，job 之间是按照串行的方式执行的。一个 job 执行完成后，才会起下一个 job。 ","date":"2022-12-10","objectID":"/stage/:1:0","tags":["数仓"],"title":"理解 spark 中的 job、stage、task","uri":"/stage/"},{"categories":["学习"],"content":"Stage 一个 job 通常包含一个或多个 stage。各个 stage 之间按照顺序执行。上面已经说过，一个 job 会有多个算子操作。这些算子都是将一个父 RDD 转换成子 RDD。这个过程中，会有两种情况：父 RDD 中的数据是否进入不同的子 RDD。 如果一个父 RDD 的数据只进入到一个子 RDD，比如 map、union 等操作，称之为 narrow dependency窄依赖。 否则，就会形成 wide dependency宽依赖，一般也称为 shuffle 依赖，比如 groupByKey 等操作。 job 中 stage 的划分就是根据 shuffle 依赖进行的。shuffle 依赖是两个 stage 的分界点。shuffle 操作一般都是任务中最耗时耗资源的部分。因为数据可能存放在 HDFS 不同的节点上，下一个 stage 的执行首先要去拉取上一个 stage的数据（shuffle read操作），保存在自己的节点上，就会增加网络通信和 IO。 按照宽窄依赖： 宽：reparation coalesce join（shuffle 参数设置位 TRUE） 窄：union filter map flatmap mappartition ","date":"2022-12-10","objectID":"/stage/:2:0","tags":["数仓"],"title":"理解 spark 中的 job、stage、task","uri":"/stage/"},{"categories":["学习"],"content":"Task 一个 spark application 提交后，陆续被分解为 job、stage，到这里其实还是一个比较粗的概念。Stage 继续往下分解，就是 Task。Task 应该是 spark 最细的执行单元了。Task 的数量其实就是 stage 的并行度。 RDD 在计算的时候，每个分区都会起一个 task，所以 rdd 的分区数目决定了总的的 task 数目。每个 Task 执行的结果就是生成了目标 RDD 的一个 partiton。在 Map 阶段 partition 数目保持不变。在 Reduce 阶段，RDD 的聚合会触发 shuffle 操作，聚合后的 RDD 的 partition 数目跟具体操作有关，例如 repartition 操作会聚合成指定分区数。 参考链接 理解spark中的job、stage、task ","date":"2022-12-10","objectID":"/stage/:3:0","tags":["数仓"],"title":"理解 spark 中的 job、stage、task","uri":"/stage/"},{"categories":["学习"],"content":"Mapreduce 工作原理","date":"2022-12-09","objectID":"/mapreduce-flow/","tags":["数仓"],"title":"Mapreduce 工作原理","uri":"/mapreduce-flow/"},{"categories":["学习"],"content":"介绍一下 Mapreduce 工作原理？ 工作流程 MapReduce 工作原理分为以下 5 个步骤 在客户端启动一个作业。 向 JobTracker 请求一个 Job ID。 将运行作业所需要的资源文件复制到 HDFS 上，包括 MapReduce 程序打包的 JAR 文件、配置文件和客户端计算所得的输入划分信息。 JobTracker 接收到作业后，将其放在一个作业队列里，等待作业调度器对其进行调度，作业调度器会根据输入划分信息为每个划分创建一个 map 任务，并将 map 任务分配给 TaskTracker 执行。 TaskTracker 每隔一段时间会给 JobTracker 发送一个心跳，告诉 JobTracker 它依然在运行，同时心跳中还携带着很多的信息，比如当前 map 任务完成的进度等信息。 以上是在客户端、JobTracker、TaskTracker 的层次来分析 MapReduce 的工作原理。 参考链接 腾讯微信部门大数据开发面试题-附答案 ","date":"2022-12-09","objectID":"/mapreduce-flow/:0:0","tags":["数仓"],"title":"Mapreduce 工作原理","uri":"/mapreduce-flow/"},{"categories":["学习"],"content":"RDD 和 DataFrame 的区别是什么","date":"2022-12-08","objectID":"/df-rdd/","tags":["数仓"],"title":"RDD 和 DataFrame 的区别是什么","uri":"/df-rdd/"},{"categories":["学习"],"content":"RDD 和 DataFrame 的区别是什么？ rdd和df RDD 是 Spark 对于分布式数据模型的抽象，DF 是带数据模式的结构化分布式数据集，类似于传统数据库中的一张表，RDD 不带数据模式或者说是泛型的。 ","date":"2022-12-08","objectID":"/df-rdd/:0:0","tags":["数仓"],"title":"RDD 和 DataFrame 的区别是什么","uri":"/df-rdd/"},{"categories":["学习"],"content":"使用 RDD 的一般场景 你需要使用 low-level 的转化操作和行动操作来控制你的数据集; 你得数据集非结构化，比如，流媒体或者文本流; 你想使用函数式编程来操作你得数据，而不是用特定领域语言( DSL )表达; 你不在乎 schema，比如，当通过名字或者列处理(或访问)数据属性不在意列式存储格式; 你放弃使用 DataFrame 和 DataSet 来优化结构化和半结构化数据集。 ","date":"2022-12-08","objectID":"/df-rdd/:1:0","tags":["数仓"],"title":"RDD 和 DataFrame 的区别是什么","uri":"/df-rdd/"},{"categories":["学习"],"content":"DataFrame 和 RDD 的优缺点 ","date":"2022-12-08","objectID":"/df-rdd/:2:0","tags":["数仓"],"title":"RDD 和 DataFrame 的区别是什么","uri":"/df-rdd/"},{"categories":["学习"],"content":"RDD 优点 编译时类型安全，开发会进行类型检查，在编译的时候及时发现错误 具有面向对象编程的风格 缺点 构建大量的 java 对象占用了大量 heap 堆空间，导致频繁的 GC 数据的序列化和反序列性能开销很大 ","date":"2022-12-08","objectID":"/df-rdd/:2:1","tags":["数仓"],"title":"RDD 和 DataFrame 的区别是什么","uri":"/df-rdd/"},{"categories":["学习"],"content":"DataFrame DataFrame 引入了 schema元信息 和 off-heap堆外 DataFrame 引入 off-heap，大量的对象构建直接使用操作系统层面上的内存，不在使用 heap 堆中的内存 DataFrame 引入了 schema 元信息(就是数据结构的描述信息)，只需要把数据的内容本身进行序列化就可以，数据结构信息可以省略掉。 缺点 编译时类型不安全，编译时不会进行类型的检查，无法在编译的时候发现错误，只有在运行的时候才会发现 不再具有面向对象编程的风格 参考链接 Spark之RDD与DataFrame的区别与理解 RDD 和 DataFrame 的区别是什么？ ","date":"2022-12-08","objectID":"/df-rdd/:2:2","tags":["数仓"],"title":"RDD 和 DataFrame 的区别是什么","uri":"/df-rdd/"},{"categories":["学习"],"content":"Spark 为什么比 MapReduce 快","date":"2022-12-08","objectID":"/spark-fast/","tags":["数仓","Spark"],"title":"Spark 为什么比 MapReduce 快","uri":"/spark-fast/"},{"categories":["学习"],"content":"Spark 为什么比 MapReduce 快? Spark 是基于内存计算，MapReduce 是基于磁盘运算，所以速度快 MapReduce 在 Shuffle 时需要花费大量时间进行排序；Spark 在 Shuffle 时则只有部分场景才需要排序。 MapReduce 的 Map Task 和 Reduce Task 都是进程级别的，每次启动都需要重新申请资源，消耗了不必要的时间；而 Spark Task 则是基于线程模型的，Spark 通过复用线程池中的线程来减少启动、关闭 task 所需要的开销。 Spark 还拥有容错机制Lineage 参考链接 腾讯微信部门大数据开发面试题-附答案 为什么Spark运行比MapReduce快 Spark速度比MapReduce快，不仅是内存计算 ","date":"2022-12-08","objectID":"/spark-fast/:0:0","tags":["数仓","Spark"],"title":"Spark 为什么比 MapReduce 快","uri":"/spark-fast/"},{"categories":["学习"],"content":"为什么要对数仓进行分层","date":"2022-12-08","objectID":"/dw-level/","tags":["数仓"],"title":"为什么要对数仓进行分层","uri":"/dw-level/"},{"categories":["学习"],"content":"根据项目中的介绍，谈谈为什么要对数仓进行分层？。 写流程 划清层次结构：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。 数据血缘追踪：简单来讲可以这样理解，我们最终给下游是直接能使用的业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。 减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。 把复杂问题简单化：将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。 屏蔽原始数据的异常：屏蔽业务的影响，不必改一次业务就需要重新接入数据。 参考链接 腾讯微信部门大数据开发面试题-附答案 ","date":"2022-12-08","objectID":"/dw-level/:0:0","tags":["数仓"],"title":"为什么要对数仓进行分层","uri":"/dw-level/"},{"categories":["学习"],"content":"Hdfs 写数据流程","date":"2022-12-08","objectID":"/hdfs-write/","tags":["数仓","Hdfs"],"title":"Hdfs 写数据流程","uri":"/hdfs-write/"},{"categories":["学习"],"content":"Hdfs 写数据流程也介绍一下。 写流程 客户端通过 Distributed FileSystem 模块向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。 NameNode 返回是否可以上传，请求第一个 Block 上传到哪几个 DataNode 服务器上。 NameNode 返回 3 个 DataNode 节点，分别为 dn1、dn2、dn3。 客户端通过 FSDataOutputStream 模块请求 dn1 上传数据，dn1 收到请求会继续调用 dn2，然后 dn2 调用 dn3，将这个通信管道建立完成。 dn1、dn2、dn3 逐级应答客户端。 客户端开始往 dn1 上传第一个 Block（先从磁盘读取数据放到一个本地内存缓存），以 Packet 为单位，dn1 收到一个 Packet 就会传给 dn2，dn2 传给 dn3；dn1 每传一个 packet 会放入一个应答队列等待应答。 当一个 Block 传输完成之后，客户端再次请求 NameNode 上传第二个 Block 的服务器。（重复上面的步骤）。 参考链接 字节二面：谈谈HDFS读写数据流程 HDFS读写流程 腾讯微信部门大数据开发面试题-附答案 ","date":"2022-12-08","objectID":"/hdfs-write/:0:0","tags":["数仓","Hdfs"],"title":"Hdfs 写数据流程","uri":"/hdfs-write/"},{"categories":["学习"],"content":"Hdfs 读数据流程","date":"2022-12-08","objectID":"/hdfs-read/","tags":["数仓","Hdfs"],"title":"Hdfs 读数据流程","uri":"/hdfs-read/"},{"categories":["学习"],"content":"那 Hdfs 读数据流程你了解吗？ 读流程 客户端通过 Distributed FileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。 挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。 DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位来做校验）。 客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。 参考链接 字节二面：谈谈HDFS读写数据流程 HDFS读写流程 腾讯微信部门大数据开发面试题-附答案 ","date":"2022-12-08","objectID":"/hdfs-read/:0:0","tags":["数仓","Hdfs"],"title":"Hdfs 读数据流程","uri":"/hdfs-read/"},{"categories":["学习"],"content":"Spark 任务执行流程?","date":"2022-12-08","objectID":"/spark-flow/","tags":["数仓","Spark"],"title":"Spark 任务执行流程?","uri":"/spark-flow/"},{"categories":["学习"],"content":"Spark 执行流程详解。 spark流程 SparkContext 向资源管理器注册并申请运行 Executor，资源管理器分配并启动 Executor Executor 发送心跳至资源管理器，保持通信 SparkContext 构建 DAG 有向无环图，将 DAG 分解成 StageTaskSet，把 Stage 发送给 TaskScheduler Executor 向 SparkContext 申请 Task，TaskScheduler 将 Task 和应用程序代码发送给 Executor 运行 Task 在 Executor 上运行，运行完毕把结果反馈给 Driver 端，释放所有资源 参考链接 Spark执行流程详解 腾讯微信部门大数据开发面试题-附答案 ","date":"2022-12-08","objectID":"/spark-flow/:0:0","tags":["数仓","Spark"],"title":"Spark 任务执行流程?","uri":"/spark-flow/"},{"categories":["学习"],"content":"ORC 和 Parquet 文件存储格式","date":"2022-12-07","objectID":"/orc/","tags":["数仓"],"title":"ORC 和 Parquet 文件存储格式","uri":"/orc/"},{"categories":["学习"],"content":"ORC 与 Parquet 的较量。 ","date":"2022-12-07","objectID":"/orc/:0:0","tags":["数仓"],"title":"ORC 和 Parquet 文件存储格式","uri":"/orc/"},{"categories":["学习"],"content":"嵌套结构支持 Parquet 能够很完美的支持嵌套式结构，而在这一点上 ORC 支持的并不好，表达起来复杂且性能和空间都损耗较大。 Parquet 非常适用于 OLAP 场景，按列存储和扫描。 ","date":"2022-12-07","objectID":"/orc/:1:0","tags":["数仓"],"title":"ORC 和 Parquet 文件存储格式","uri":"/orc/"},{"categories":["学习"],"content":"更新与 ACID 支持 ORC 格式支持 update 操作与 ACID，而 Parquet 并不支持。 ","date":"2022-12-07","objectID":"/orc/:2:0","tags":["数仓"],"title":"ORC 和 Parquet 文件存储格式","uri":"/orc/"},{"categories":["学习"],"content":"压缩与查询性能 在压缩空间与查询性能方面，Parquet 与 ORC 总体上相差不大。可能 ORC 要稍好于 Parquet。 ","date":"2022-12-07","objectID":"/orc/:3:0","tags":["数仓"],"title":"ORC 和 Parquet 文件存储格式","uri":"/orc/"},{"categories":["学习"],"content":"查询引擎支持 这方面 Parquet 可能更有优势，支持 Hive、Impala、Presto 等各种查询引擎，而 ORC 与 Hive 接触的比较紧密，而与 Impala 适配的并不好。之前我们说 Impala 不支持 ORC，直到 CDH 6.1.x 版本也就是 Impala3.x 才开始以 experimental feature 支持 ORC 格式。 ORC 文件是可切分（Split）的。因此，在 Hive 中使用 ORC 作为表的文件存储格式，不仅节省 HDFS 存储资源，查询任务的输入数据量减少，使用的 MapTask 也就减少了。 ","date":"2022-12-07","objectID":"/orc/:4:0","tags":["数仓"],"title":"ORC 和 Parquet 文件存储格式","uri":"/orc/"},{"categories":["学习"],"content":"总结 关于 Parquet 与 ORC，首先建议根据实际情况进行选择。另外，根据笔者的综合评估，如果不是一定要使用 ORC 的特性，还是建议选择 Parquet。 ","date":"2022-12-07","objectID":"/orc/:5:0","tags":["数仓"],"title":"ORC 和 Parquet 文件存储格式","uri":"/orc/"},{"categories":["学习"],"content":"参考链接 文件存储格式：ORC 与 Parquet的较量 干货 | 再来聊一聊 Parquet 列式存储格式 ","date":"2022-12-07","objectID":"/orc/:6:0","tags":["数仓"],"title":"ORC 和 Parquet 文件存储格式","uri":"/orc/"},{"categories":["学习"],"content":"详解MapReduce Shuffle与Spark Shuffle","date":"2022-12-07","objectID":"/shuffle/","tags":["数仓"],"title":"详解MapReduce Shuffle与Spark Shuffle","uri":"/shuffle/"},{"categories":["学习"],"content":"本文介绍 MapReduce Shuffle 与 Spark Shuffle 的详情和区别。 ","date":"2022-12-07","objectID":"/shuffle/:0:0","tags":["数仓"],"title":"详解MapReduce Shuffle与Spark Shuffle","uri":"/shuffle/"},{"categories":["学习"],"content":"Shuffle简介 Shuffle 的本意是洗牌、混洗的意思，把一组有规则的数据尽量打乱成无规则的数据。而在 MapReduce 中，Shuffle 更像是洗牌的逆过程，指的是将 map 端的无规则输出按指定的规则“打乱”成具有一定规则的数据，以便 reduce 端接收处理。或者说需要将各节点上同一类数据汇集到某一节点进行计算，把这些分布在不同节点的数据按照一定的规则聚集到一起的过程称为 Shuffle。 在 Shuffle 之前，也就是在 map 阶段，MapReduce 会对要处理的数据进行分片split操作，为每一个分片分配一个 MapTask 任务。接下来 map 会对每一个分片中的每一行数据进行处理得到键值对（key,value），此时得到的键值对又叫做“中间结果”。此后便进入 reduce 阶段，由此可以看出 Shuffle 阶段的作用是处理“中间结果”。 由于 Shuffle 涉及到了磁盘的读写和网络的传输，因此 Shuffle 性能的高低直接影响到了整个程序的运行效率。 ","date":"2022-12-07","objectID":"/shuffle/:1:0","tags":["数仓"],"title":"详解MapReduce Shuffle与Spark Shuffle","uri":"/shuffle/"},{"categories":["学习"],"content":"MapReduce Shuffle Hadoop 的核心思想是 MapReduce，但 Shuffle 又是 MapReduce 的核心。Shuffle 的主要工作是从 Map 结束到 Reduce 开始之间的过程。Shuffle 阶段又可以分为 Map 端的 Shuffle 和 Reduce 端的 Shuffle。 ","date":"2022-12-07","objectID":"/shuffle/:2:0","tags":["数仓"],"title":"详解MapReduce Shuffle与Spark Shuffle","uri":"/shuffle/"},{"categories":["学习"],"content":"Spark Shuffle Spark 丰富了任务类型，有些任务之间数据流转不需要通过 Shuffle，但是有些任务之间还是需要通过 Shuffle 来传递数据，比如宽依赖的 group by key 以及各种 by key 算子。宽依赖之间会划分 stage，而 Stage 之间就是 Shuffle，如下图中的 stage0，stage1 和 stage3 之间就会产生 Shuffle。 stage 在 Spark 中，负责 shuffle 过程的执行、计算和处理的组件主要就是 ShuffleManagershuffle管理器。ShuffleManager 随着 Spark 的发展有两种实现的方式，分别为 HashShuffleManager 和 SortShuffleManager，因此 spark 的 Shuffle 有 Hash Shuffle 和 Sort Shuffle 两种。 ","date":"2022-12-07","objectID":"/shuffle/:3:0","tags":["数仓"],"title":"详解MapReduce Shuffle与Spark Shuffle","uri":"/shuffle/"},{"categories":["学习"],"content":"Spark 与 MapReduce Shuffle 的异同 从整体功能上看，两者并没有大的差别。都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer。 从流程的上看，两者差别不小。MapReduce 是 sort-based，进入 combine 和 reduce 的 records 必须先 sort。这样的好处在于 combine/reduce 可以处理大规模的数据，因为其输入数据可以通过外排得到（mapper 对每段数据先做排序，reducer 的 shuffle 对排好序的每段数据做归并）。以前 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行合并，不会对数据进行提前排序。如果用户需要经过排序的数据，那么需要自己调用类似 sortByKey 的操作。 从流程实现角度来看，两者也有不少差别。MapReduce 将处理流程划分出明显的几个阶段：map, spill, merge, shuffle, sort, reduce 等。每个阶段各司其职，可以按照过程式的编程思想来逐一实现每个阶段的功能。在 Spark 中，没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation，所以 spill, merge, aggregate 等操作需要蕴含在 transformation中。 与 MapReduce 完全不一样的是，在 MapReduce 中，map task 必须将所有的数据都写入本地磁盘文件以后，才能启动 reduce 操作，来拉取数据。为什么？因为 mapreduce 要实现默认的根据 key 的排序！所以要排序，肯定得写完所有数据，才能排序，然后 reduce 来拉取。 但是 Spark 不需要，spark 默认情况下，是不会对数据进行排序的。因此 ShuffleMapTask 每写入一点数据，ResultTask 就可以拉取一点数据，然后在本地执行我们定义的聚合函数和算子，进行计算。 ","date":"2022-12-07","objectID":"/shuffle/:4:0","tags":["数仓"],"title":"详解MapReduce Shuffle与Spark Shuffle","uri":"/shuffle/"},{"categories":["学习"],"content":"大数据——Hive SQL优化","date":"2022-12-07","objectID":"/hive-youhua/","tags":["数仓"],"title":"大数据——Hive SQL优化","uri":"/hive-youhua/"},{"categories":["学习"],"content":"本文介绍一些 hive 的常见优化方案。 ","date":"2022-12-07","objectID":"/hive-youhua/:0:0","tags":["数仓"],"title":"大数据——Hive SQL优化","uri":"/hive-youhua/"},{"categories":["学习"],"content":"SELECT 字段尽可能少，数据过滤尽可能提前 SELECT ","date":"2022-12-07","objectID":"/hive-youhua/:1:0","tags":["数仓"],"title":"大数据——Hive SQL优化","uri":"/hive-youhua/"},{"categories":["学习"],"content":"能不用 JOIN 连接的就不用 join ","date":"2022-12-07","objectID":"/hive-youhua/:2:0","tags":["数仓"],"title":"大数据——Hive SQL优化","uri":"/hive-youhua/"},{"categories":["学习"],"content":"数据倾斜问题 ","date":"2022-12-07","objectID":"/hive-youhua/:3:0","tags":["数仓"],"title":"大数据——Hive SQL优化","uri":"/hive-youhua/"},{"categories":["学习"],"content":"JOIN 优化 选用 join key 分布最均匀的表作为驱动表，并且大表放在右边，小表放在左边。 ","date":"2022-12-07","objectID":"/hive-youhua/:3:1","tags":["数仓"],"title":"大数据——Hive SQL优化","uri":"/hive-youhua/"},{"categories":["学习"],"content":"排序优化 sort by 代替 order by ","date":"2022-12-07","objectID":"/hive-youhua/:3:2","tags":["数仓"],"title":"大数据——Hive SQL优化","uri":"/hive-youhua/"},{"categories":["学习"],"content":"少用count(distinct) 用 group by 代替 count(distinct) select count(*) from (select uid from testmac group by uid) t ","date":"2022-12-07","objectID":"/hive-youhua/:3:3","tags":["数仓"],"title":"大数据——Hive SQL优化","uri":"/hive-youhua/"},{"categories":["学习"],"content":"多表 join 时 key 保持一致 当对多个表进行 join 连接时，如果每个 on 子句都使用相同的连接键的话，那么只会产生一个 MapReduce job，执行效率相对快。 ","date":"2022-12-07","objectID":"/hive-youhua/:4:0","tags":["数仓"],"title":"大数据——Hive SQL优化","uri":"/hive-youhua/"},{"categories":["学习"],"content":"去除空值和无意义的值 出现空值或无意义值时，如null，空字符串、-1等，在做 join 时这些空值就会非常集中，拖累进度。因此，若不需要空值数据，就提前写 where 语句过滤掉。若需要保留，将空值 null 的记录随机改为负值： ","date":"2022-12-07","objectID":"/hive-youhua/:5:0","tags":["数仓"],"title":"大数据——Hive SQL优化","uri":"/hive-youhua/"},{"categories":["学习"],"content":"SQL-MySQL窗口函数和单行函数的使用","date":"2022-12-07","objectID":"/sql-window/","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"窗口函数也叫OLAP函数（Online Anallytical Processing联机分析处理），可以对数据进行实时分析处理。窗口函数是面试中考察的重点。窗口函数通常用来解决统计汇总、排名、TopN、连续登录天数等问题。 ","date":"2022-12-07","objectID":"/sql-window/:0:0","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"窗口函数 语法：函数名(字段名) over(partition by \u003c要分列的组\u003e order by \u003c要排序的列\u003e rows between \u003c数据范围\u003e) 数据范围：通过下面的案例来讲解数据范围如何使用。 # 取本行和前面两行 rows between 2 preceding and current row # 取本行和之前所有的行 rows between unbounded preceding and current row # 取本行和之后所有的行 rows between current row and unbounded following # 从前面三行和下面一行，总共五行 rows between 3 preceding and 1 following # 当order by后面没有rows between时，窗口规范默认是取本行和之前所有的行 # 当order by和rows between都没有时，窗口规范默认是分组下所有行（rows between unbounded preceding and unbounded following） 分类：按照窗口函数的意义大概可以分为下面 5 类，其中排序函数最为常用。 排序函数：row_number()、rank()、dense_rank() 分布函数：percent_rank()、cume_dist() 相对位置函数：lag(expr,n)、lead(expr,n)，用于返回某字段的前 n 行或后 n 行的值。expr 既可以是表达式也可以是列名。 绝对位置函数：first_value(expr)、last_value(expr)、nth_value(expr,n)，返回第一个或最后一个或第 n 个 expr 的值。 分桶函数：ntile(x) 另外，聚合函数也可以作为窗口函数使用： 聚合函数：avg()，sum()，min()，max() ","date":"2022-12-07","objectID":"/sql-window/:1:0","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"排序函数 row_number()：对每一行分配一个序号，序号连续加1，不会重复。常用于排序。 rank()：给每行分配一个序号，相同值的序号相同，序号不连续。常用于排序。 dense_rank()：给每行分配一个序号，相同值的序号相同，序号连续。常用于排序。 ","date":"2022-12-07","objectID":"/sql-window/:1:1","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"分布函数 percent_rank()：每行按照公式$(rank-1) / (rows-1)$进行计算。其中，rank 为 RANK() 函数产生的序号，rows 为当前窗口的记录总行数。 cume_dist()：分组内小于、等于当前 rank 值的行数 / 分组内总行数 ","date":"2022-12-07","objectID":"/sql-window/:1:2","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"相对位置函数 lag(expr,n)：返回位于当前行的前 n 行的值 lead(expr,n)：返回位于当前行的后 n 行的值 ","date":"2022-12-07","objectID":"/sql-window/:1:3","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"绝对位置函数 first_value(expr)：返回第一个 expr 的值。 last_value(expr)：返回最后一个 expr 的值。 nth_value(expr,n)：返回窗口中第 n 个 expr 的值。 应用场景 1 举例：求首次登录和末次登录时间 select id, log_dt, first_value(log_dt) over(partition by id order by log_dt) f_dt, last_value(log_dt) over(partition by id order by log_dt) l_dt from tb; id log_dt f_dt l_dt 1 2020-11-10 2020-11-10 2020-11-10 1 2021-01-20 2020-11-10 2021-01-20 1 2021-08-12 2020-11-10 2021-08-12 2 2021-12-05 2021-12-05 2021-12-05 2 2021-12-29 2021-12-05 2021-12-29 应用场景 2 举例：求部门中工资第二的员工 SELECT id, dept_id did, salary s, NTH_VALUE(salary,2) over(PARTITION BY dept_id ORDER BY salary DESC) s2 FROM employee; id did s s2 2 1 200 100 1 1 100 100 4 2 400 300 3 2 300 300 6 3 560 500 5 3 500 500 ","date":"2022-12-07","objectID":"/sql-window/:1:4","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"分桶函数 ntile(n)：对每个分区继续分成 n 组，每组的行数为：分区的总行数 / n。不常用。 ","date":"2022-12-07","objectID":"/sql-window/:1:5","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"日期时间函数 CURDATE() 或 CURRENT_DATE() 返回当前日期 NOW() 返回当前系统日期时间 YEAR(date) 返回年 MONTH(date) 返回月 DAY(date) 返回日 DATEDIFF(date1,date2) 返回 date1 - date2 的日期间隔 DATE_FORMAT(datetime ,fmt) 按照字符串 fmt 格式化日期 datetime 值。 %Y 4 位数字表示年份 %m 两位数字表示月份（01,02,03,…） %d 两位数字表示月中的天数(01,02…) %H 两位数字表示小时，24小时制（01,02,…） ","date":"2022-12-07","objectID":"/sql-window/:2:0","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"流程函数 IF(value,t ,f) 如果 value 是真，返回 t，否则返回 f IFNULL(value1, value2) 如果 value1 不为空，返回 value1，否则返回 value2 CASE WHEN ","date":"2022-12-07","objectID":"/sql-window/:3:0","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"数学函数 ABS(x) 返回 x 的绝对值 CEIL(x) 返回大于 x 的最小整数值 FLOOR(x) 返回小于 x 的最大整数值 MOD(x,y) 返回 x/y 的模 RAND(x) 返回 0~1 的随机值，x可以不写 ROUND(x,y) 返回参数 x 的四舍五入的有 y 位的小数的值 TRUNCATE(x,y) 返回数字 x 截断为 y 位小数的结果 SQRT(x) 返回 x 的平方根 POW(x,y) 返回 x 的 y 次方 ","date":"2022-12-07","objectID":"/sql-window/:4:0","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"字符串函数 CONCAT(S1,S2,......,Sn) 连接 S1, S2, ......, Sn 为一个字符串 CONCAT_WS(separator, S1, S2, ......, Sn) 同 CONCAT(s1, s2, ...) 函数，但是每个字符串之间要加上分隔符 separator分隔符 TRIM(s) 去掉字符串 s 开始与结尾的空格 ","date":"2022-12-07","objectID":"/sql-window/:5:0","tags":["SQL"],"title":"SQL-MySQL窗口函数和单行函数的使用","uri":"/sql-window/"},{"categories":["学习"],"content":"图文详解 MapReduce 工作流程","date":"2022-12-07","objectID":"/mapreduce/","tags":["数仓"],"title":"图文详解 MapReduce 工作流程","uri":"/mapreduce/"},{"categories":["学习"],"content":"本文会详解 MapReduce 工作流程。 MR工作流程 ","date":"2022-12-07","objectID":"/mapreduce/:0:0","tags":["数仓"],"title":"图文详解 MapReduce 工作流程","uri":"/mapreduce/"},{"categories":["学习"],"content":"MapReduce 编程模型 MapReduce 编程模型开发简单且功能强大，专门为并行处理大规模数据量而设计，接下来，通过一张图来描述 MapReduce 的工作过程，如图所示。 工作流程 ","date":"2022-12-07","objectID":"/mapreduce/:1:0","tags":["数仓"],"title":"图文详解 MapReduce 工作流程","uri":"/mapreduce/"},{"categories":["学习"],"content":"整体流程 在上图中， MapReduce5 的工作流程大致可以分为5步，具体如下: 5步 ","date":"2022-12-07","objectID":"/mapreduce/:2:0","tags":["数仓"],"title":"图文详解 MapReduce 工作流程","uri":"/mapreduce/"},{"categories":["学习"],"content":"分片、格式化数据源 输入 Map 阶段的数据源，必须经过分片和格式化操作。 分片操作：指的是将源文件划分为大小相等的小数据块(Hadoop 2.x 中默认 128MB)，也就是分片(split)，Hadoop 会为每一个分片构建一个 Map 任务，并由该任务运行自定义的 map() 函数，从而处理分片里的每一条记录; 格式化操作：将划分好的分片(split)格式化为键值对 \u003ckey,value\u003e 形式的数据，其中，key 代表偏移量，value 代表每一行内容。 ","date":"2022-12-07","objectID":"/mapreduce/:2:1","tags":["数仓"],"title":"图文详解 MapReduce 工作流程","uri":"/mapreduce/"},{"categories":["学习"],"content":"执行 MapTask 每个 Map 任务都有一个内存缓冲区缓冲区大小100MB，输入的分片split数据经过 Map 任务处理后的中间结果会写入内存缓冲区中。 如果写入的数据达到内存缓冲的阈值80MB，会启动一个线程将内存中的溢出数据写入磁盘，同时不影响 Map 中间结果继续写入缓冲区。 在溢写过程中， MapReduce 框架会对 key 进行排序，如果中间结果比较大，会形成多个溢写文件，最后的缓冲区数据也会全部溢写入磁盘形成一个溢写文件，如果是多个溢写文件，则最后合并所有的溢写文件为一个文件。 ","date":"2022-12-07","objectID":"/mapreduce/:2:2","tags":["数仓"],"title":"图文详解 MapReduce 工作流程","uri":"/mapreduce/"},{"categories":["学习"],"content":"执行 Shuffle 过程 MapReduce 工作过程中， Map 阶段处理的数据如何传递给 Reduce 阶段，这是 MapReduce 框架中关键的一个过程，这个过程叫作 Shuffle。 Shuffle 会将 MapTask 输出的处理结果数据分发给 ReduceTask，并在分发的过程中，对数据按 key 进行分区和排序。 ","date":"2022-12-07","objectID":"/mapreduce/:2:3","tags":["数仓"],"title":"图文详解 MapReduce 工作流程","uri":"/mapreduce/"},{"categories":["学习"],"content":"执行 ReduceTask 输入 ReduceTask 的数据流是 \u003ckey, {value list}\u003e 形式，用户可以自定义 reduce() 方法进行逻辑处理，最终以 \u003ckey, value\u003e 的形式输出。 ","date":"2022-12-07","objectID":"/mapreduce/:2:4","tags":["数仓"],"title":"图文详解 MapReduce 工作流程","uri":"/mapreduce/"},{"categories":["学习"],"content":"写入文件 MapReduce 框架会自动把 ReduceTask 生成的 \u003ckey, value\u003e 传入 OutputFormat 的 write 方法，实现文件的写入操作。 ","date":"2022-12-07","objectID":"/mapreduce/:2:5","tags":["数仓"],"title":"图文详解 MapReduce 工作流程","uri":"/mapreduce/"},{"categories":["学习"],"content":"MapTask MapTask Read 阶段：MapTask 通过用户编写的 RecordReader，从输入的 InputSplit 中解析出一个个 key / value 。 Map 阶段：将解析出的 key / value 交给用户编写的 Map() 函数处理，并产生一系列新的 key / value。 Collect 阶段：在用户编写的 map() 函数中，数据处理完成后，一般会调用 outputCollector.collect() 输出结果，在该函数内部，它会将生成的 key / value 分片(通过调用 partitioner)，并写入一个环形内存缓冲区中(该缓冲区默认大小是 100MB )。 Spill 阶段：即“溢写”，当缓冲区快要溢出时(默认达到缓冲区大小的 80 %)，会在本地文件系统创建一个溢出文件，将该缓冲区的数据写入这个文件。 将数据写入本地磁盘前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 写入磁盘之前，线程会根据 ReduceTask 的数量，将数据分区，一个 Reduce 任务对应一个分区的数据。 这样做的目的是为了避免有些 Reduce 任务分配到大量数据，而有些 Reduce 任务分到很少的数据，甚至没有分到数据的尴尬局面。 如果此时设置了 Combiner ，将排序后的结果进行 Combine 操作，这样做的目的是尽可能少地执行数据写入磁盘的操作。 Combine 阶段：当所有数据处理完成以后， MapTask 会对所有临时文件进行一次合并，以确保最终只会生成一个数据文件 ","date":"2022-12-07","objectID":"/mapreduce/:3:0","tags":["数仓"],"title":"图文详解 MapReduce 工作流程","uri":"/mapreduce/"},{"categories":["学习"],"content":"ReduceTask ReduceTask Copy 阶段： Reduce 会从各个 MapTask 上远程复制一片数据（每个 MapTask 传来的数据都是有序的），并针对某一片数据，如果其大小超过一定國值，则写到磁盘上，否则直接放到内存中 Merge 阶段：在远程复制数据的同时， ReduceTask 会启动两个后台线程，分别对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘文件过多。 Sort 阶段：用户编写 reduce() 方法输入数据是按 key 进行聚集的一组数据。 为了将 key 相同的数据聚在一起， Hadoop 采用了基于排序的策略。 由于各个 MapTask 已经实现对自己的处理结果进行了局部排序，因此， ReduceTask 只需对所有数据进行一次归并排序即可。 Reduce 阶段：对排序后的键值对调用 reduce() 方法，键相等的键值对调用一次 reduce()方法，每次调用会产生零个或者多个键值对，最后把这些输出的键值对写入到 HDFS 中 Write 阶段：reduce() 函数将计算结果写到 HDFS 上。 合并的过程中会产生许多的中间文件(写入磁盘了)，但 MapReduce 会让写入磁盘的数据尽可能地少，并且最后一次合并的结果并没有写入磁盘，而是直接输入到 Reduce 函数。 ","date":"2022-12-07","objectID":"/mapreduce/:4:0","tags":["数仓"],"title":"图文详解 MapReduce 工作流程","uri":"/mapreduce/"},{"categories":["学习"],"content":"SQL-英雄出场排名top3的出场次数及出场率","date":"2022-12-07","objectID":"/sql-hero-top/","tags":["SQL"],"title":"SQL-英雄出场排名top3的出场次数及出场率","uri":"/sql-hero-top/"},{"categories":["学习"],"content":"求英雄的出场排名 top3 的出场次数及出场率。names 代表英雄名字。 id names 1 aa,bb,cc,dd,ee 2 aa,bb,ff,ww,qq 3 aa,cc,rr,yy 4 aa,bb,dd,oo,pp select name, cnt, cast(cnt/all_cnt as decimal(3,1)) as ratio from ( select name, cnt, sum(cnt) over () as all_cnt, dense_rank() over (order by cnt desc) as rn from ( select name, count(1) as cnt from tmp_2022120501 lateral view explode(split(my_str, ',')) t as name group by 1 ) a ) a where rn \u003c= 3; ","date":"2022-12-07","objectID":"/sql-hero-top/:0:0","tags":["SQL"],"title":"SQL-英雄出场排名top3的出场次数及出场率","uri":"/sql-hero-top/"},{"categories":["学习"],"content":"SQL-列转行","date":"2022-12-07","objectID":"/sql-row-col/","tags":["SQL"],"title":"SQL-列转行","uri":"/sql-row-col/"},{"categories":["学习"],"content":"常用操作列转行的几种方法。 ","date":"2022-12-07","objectID":"/sql-row-col/:0:0","tags":["SQL"],"title":"SQL-列转行","uri":"/sql-row-col/"},{"categories":["学习"],"content":"数据准备 create table t1 as select '1' as id ,'mike' as name ,'22' as age,'0' as gender union all select '2' as id ,'kangkang' as name ,'19' as age,'1' as gender ","date":"2022-12-07","objectID":"/sql-row-col/:1:0","tags":["SQL"],"title":"SQL-列转行","uri":"/sql-row-col/"},{"categories":["学习"],"content":"使用 union select id,'name' as type,name as value from t1 union all select id,'age' as type,age as value from t1 union all select id,'gender' as type,gender as value from t1 order by id ","date":"2022-12-07","objectID":"/sql-row-col/:2:0","tags":["SQL"],"title":"SQL-列转行","uri":"/sql-row-col/"},{"categories":["学习"],"content":"采用concat_ws() + posexplode()方法 select id, type, value from ( select t.id, t1.pos1, t1.value1 as value, t2.pos2, t2.type2 as type from ( select id, concat_ws(',', name, age, gender) as value, array('name', 'age', 'gender') as type from t1 ) t lateral view posexplode(split(t.value, ',')) t1 as pos1, value1 lateral view posexplode(t.type) t2 as pos2, type2 ) t where t.pos1 = t.pos2 SQL简化如下： select id, b.type2 as type, a.value1 as value from t1 lateral view posexplode(split(concat_ws(',', t1.name, t1.age, t1.gender), ',')) a as pos1, value1 lateral view posexplode(array('name', 'age', 'gender')) b as pos2, type2 where a.pos1 = b.pos2 ","date":"2022-12-07","objectID":"/sql-row-col/:3:0","tags":["SQL"],"title":"SQL-列转行","uri":"/sql-row-col/"},{"categories":["学习"],"content":"采用explode()+case when方法 select id, type, case type when 'name' then name when 'age' then age when 'gender' then gender else null end as value from t1 lateral view explode(array('name', 'age', 'gender')) t2 as type ","date":"2022-12-07","objectID":"/sql-row-col/:4:0","tags":["SQL"],"title":"SQL-列转行","uri":"/sql-row-col/"},{"categories":["学习"],"content":"SQL-连续 7 天都登陆平台","date":"2022-12-07","objectID":"/sql-7d-login/","tags":["SQL"],"title":"SQL-连续 7 天都登陆平台","uri":"/sql-7d-login/"},{"categories":["学习"],"content":"找出连续 7 天都登陆平台的用户。 现有用户登陆表 user_login_table 如下： user_name 用户名 date 用户登陆时间 现在老板想知道连续 7 天都登陆平台的重要用户。 输出要求如下： user_name 用户名（连续 7 天都登陆的用户数） 思路1 首先利用偏移窗口函数 lead 求得每个用户在每个登陆时间向后偏移 7 行的登陆时间，再计算每个用户在每个登陆时间滞后 7 天的登陆时间，如果每个用户向后偏移 7 行的登陆时间正好等于滞后 7 天的时间，说明该用户连续登陆了 7 天。 select user_name from ( select user_name, log_date, lead(log_date, 7) over (partition by user_name order by log_date) log_date_7 from ( select 'A' user_name, '2020-01-01' log_date union all select 'A' user_name, '2020-01-02' log_date union all select 'A' user_name, '2020-01-03' log_date union all select 'A' user_name, '2020-01-04' log_date union all select 'A' user_name, '2020-01-05' log_date union all select 'A' user_name, '2020-01-06' log_date union all select 'A' user_name, '2020-01-07' log_date union all select 'A' user_name, '2020-01-08' log_date union all select 'A' user_name, '2020-01-19' log_date ) ) where log_date_7 is not null and date_add(log_date, 7) = log_date_7; 思路2 把用户每天登陆的日期进行排序，如果用当前天数减去序号，连续 3 天的话相同的数据就有 7 个 select user_id, count(1) cnt from ( select user_id, login_date, row_number() over(partition by user_id order by login_date) rn from tab1 ) group by user_id,date_sub(login_date, rn) having count(1) \u003e= 7; ","date":"2022-12-07","objectID":"/sql-7d-login/:0:0","tags":["SQL"],"title":"SQL-连续 7 天都登陆平台","uri":"/sql-7d-login/"},{"categories":["学习"],"content":"SQL-支付金额在前 20% 的用户","date":"2022-12-07","objectID":"/sql-20-per/","tags":["SQL"],"title":"SQL-支付金额在前 20% 的用户","uri":"/sql-20-per/"},{"categories":["学习"],"content":"找出支付金额在前 20% 的用户。 思路 利用 ntile select user_name from ( select user_name, ntile(5) over(order by sum(pay_amt) desc) as level from ( select 'A' user_name, 100 pay_amt union all select 'A' user_name, 10 pay_amt union all select 'A' user_name, 300 pay_amt union all select 'B' user_name, 100 pay_amt union all select 'B' user_name, 1090 pay_amt union all select 'C' user_name, 1030 pay_amt union all select 'D' user_name, 70 pay_amt union all select 'F' user_name, 770 pay_amt union all select 'G' user_name, 710 pay_amt union all select 'E' user_name, 800 pay_amt ) group by 1 ) where level = 1; ","date":"2022-12-07","objectID":"/sql-20-per/:0:0","tags":["SQL"],"title":"SQL-支付金额在前 20% 的用户","uri":"/sql-20-per/"},{"categories":["学习"],"content":"如何设置ReduceTask并行度","date":"2022-12-06","objectID":"/mr-reduce/","tags":["数仓"],"title":"如何设置ReduceTask并行度","uri":"/mr-reduce/"},{"categories":["学习"],"content":"如果 ReduceTask 数量过多，一个 ReduceTask 会产生一个结果文件，这样就会生成很多小文件，那么如果这些结果文件会作为下一个 Job 的输入，则会出现小文件需要进行合并的问题，而且启动和初始化 ReduceTask 需要耗费资源。1 如果 ReduceTask 数量过少，这样一个 ReduceTask 就需要处理大量的数据，并且还有可能会出现数据倾斜的问题，使得整个查询耗时长。默认情况下，Hive 分配的 reducer 个数由下列参数决定： Hadoop MapReduce 程序中，ReducerTask 个数的设定极大影响执行效率，ReducerTask 数量与输出文件的数量相关。如果 ReducerTask 数太多，会产生大量小文件，对 HDFS 造成压力。如果 ReducerTask 数太少，每个 ReducerTask 要处理很多数据，容易拖慢运行时间或者造成 OOM。这使得 Hive 怎样决定 ReducerTask 个数成为一个关键问题。遗憾的是 Hive 的估计机制很弱，不指定 ReducerTask 个数的情况下，Hive 会猜测确定一个 ReducerTask 个数，基于以下两个设定： 参数1：hive.exec.reducers.bytes.per.reducer (默认256M) 参数2：hive.exec.reducers.max (默认为1009) 参数3：mapreduce.job.reduces (默认值为-1，表示没有设置，那么就按照以上两个参数进行设置) ReduceTask 的计算公式为: $N = Math.min(参数2，总输入数据大小 / 参数1)$ 可以通过改变上述两个参数的值来控制 ReduceTask 的数量。也可以通过 set mapred.map.tasks=10; set mapreduce.job.reduces=10; 通常情况下，有必要手动指定 ReduceTask 个数。考虑到 Mapper 阶段的输出数据量通常会比输入有大幅减少，因此即使不设定 ReduceTask 个数，重设参数2还是必要的。依据经验，可以将参数2设定为 $M * (0.95 * N)$ (N 为集群中 NodeManager 个数)。一般来说，NodeManage 和 DataNode 的个数是一样的。 如何设置ReduceTask并行度 ↩︎ ","date":"2022-12-06","objectID":"/mr-reduce/:0:0","tags":["数仓"],"title":"如何设置ReduceTask并行度","uri":"/mr-reduce/"},{"categories":["学习"],"content":"MR中如何控制map的数量","date":"2022-12-06","objectID":"/mr-map/","tags":["数仓"],"title":"MR中如何控制map的数量","uri":"/mr-map/"},{"categories":["学习"],"content":"hadooop 提供了一个设置 map 个数的参数 mapred.map.tasks，我们可以通过这个参数来控制 map 的个数。但是通过这种方式设置 map 的个数，并不是每次都有效的。原因是 mapred.map.tasks 只是一个 hadoop 的参考数值，最终 map 的个数，还取决于其他的因素。1 为了方便介绍，先来看几个名词： block_size : hdfs 的文件块大小，默认为 128M，可以通过参数 hdfs.block.size 设置 total_size : 输入文件整体的大小 input_file_num : 输入文件的个数 ","date":"2022-12-06","objectID":"/mr-map/:0:0","tags":["数仓"],"title":"MR中如何控制map的数量","uri":"/mr-map/"},{"categories":["学习"],"content":"默认 map 个数 如果不进行任何设置，默认的 map 个数是和 blcok_size 相关的。 default_num = total_size / block_size ","date":"2022-12-06","objectID":"/mr-map/:1:0","tags":["数仓"],"title":"MR中如何控制map的数量","uri":"/mr-map/"},{"categories":["学习"],"content":"期望大小 可以通过参数 mapred.map.tasks 来设置期望的 map 个数，但是这个个数只有在大于 default_num 的时候，才会生效。 goal_num = mapred.map.tasks（默认为2） ","date":"2022-12-06","objectID":"/mr-map/:2:0","tags":["数仓"],"title":"MR中如何控制map的数量","uri":"/mr-map/"},{"categories":["学习"],"content":"设置处理的文件大小 可以通过 mapred.min.split.size 设置每个 task 处理的文件大小，但是这个大小只有在大于 block_size 的时候才会生效。 split_size = max(mapred.min.split.size, block_size) split_num = total_size / split_size ","date":"2022-12-06","objectID":"/mr-map/:3:0","tags":["数仓"],"title":"MR中如何控制map的数量","uri":"/mr-map/"},{"categories":["学习"],"content":"计算的map个数 compute_map_num = min(split_num, max(default_num, goal_num)) 除了这些配置以外，mapreduce 还要遵循一些原则。 mapreduce 的每一个 map 处理的数据是不能跨越文件的，也就是说 min_map_num \u003e= input_file_num。 所以，最终的 map 个数应该为： final_map_num = max(compute_map_num, input_file_num) ","date":"2022-12-06","objectID":"/mr-map/:4:0","tags":["数仓"],"title":"MR中如何控制map的数量","uri":"/mr-map/"},{"categories":["学习"],"content":"总结 经过以上的分析，在设置 map 个数的时候，可以简单的总结为以下几点： 如果想增加 map 个数，则设置 mapred.map.tasks 为一个较大的值。 如果想减小 map 个数，则设置 mapred.min.split.size 为一个较大的值。 如果输入中有很多小文件，依然想减少 map 个数，则需要将小文件 merger 为大文件，然后使用准则 2。 MR中如何控制map的数量 ↩︎ ","date":"2022-12-06","objectID":"/mr-map/:5:0","tags":["数仓"],"title":"MR中如何控制map的数量","uri":"/mr-map/"},{"categories":["刷题"],"content":"Python 线性查找","date":"2022-12-06","objectID":"/python-linear-search/","tags":["查找"],"title":"Python 线性查找 ","uri":"/python-linear-search/"},{"categories":["刷题"],"content":"线性查找指按一定的顺序检查数组中每一个元素，直到找到所要寻找的特定值为止。1 线性查找 def linearSearch(s, x): for i in range(len(s)): if s[i] == x: return i return -1 Python 线性查找 ↩︎ ","date":"2022-12-06","objectID":"/python-linear-search/:0:0","tags":["查找"],"title":"Python 线性查找 ","uri":"/python-linear-search/"},{"categories":["刷题"],"content":"Python 二分查找","date":"2022-12-06","objectID":"/python-binary-search/","tags":["查找"],"title":"Python 二分查找 ","uri":"/python-binary-search/"},{"categories":["刷题"],"content":"二分搜索是一种在有序数组中查找某一特定元素的搜索算法。1 搜索过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜索过程结束； 如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。 如果在某一步骤数组为空，则代表找不到。 这种搜索算法每一次比较都使搜索范围缩小一半。 冒泡排序 def binarySearch(s, l , r, x): if r \u003e= l: mid = (l + r) // 2 if s[mid] == x: return mid elif s[mid] \u003e x: return binarySearch(s, l, mid-1, x) else: return binarySearch(s, mid+1, r, x) else: return -1 Python 二分查找 ↩︎ ","date":"2022-12-06","objectID":"/python-binary-search/:0:0","tags":["查找"],"title":"Python 二分查找 ","uri":"/python-binary-search/"},{"categories":["刷题"],"content":"Python 冒泡排序","date":"2022-12-06","objectID":"/python-bubble-sort/","tags":["排序"],"title":"Python 冒泡排序 ","uri":"/python-bubble-sort/"},{"categories":["刷题"],"content":"冒泡排序（Bubble Sort）也是一种简单直观的排序算法。1 它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。 走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。 这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 冒泡排序 def bubbleSort(s): n = len(s) for i in range(n): for j in range(n-i-1): if s[j+1] \u003c s[j]: s[j+1], s[j] = s[j], s[j+1] return s Python 冒泡排序 ↩︎ ","date":"2022-12-06","objectID":"/python-bubble-sort/:0:0","tags":["排序"],"title":"Python 冒泡排序 ","uri":"/python-bubble-sort/"},{"categories":["刷题"],"content":"Python 插入排序","date":"2022-12-06","objectID":"/python-insertion-sort/","tags":["排序"],"title":"Python 插入排序 ","uri":"/python-insertion-sort/"},{"categories":["刷题"],"content":"插入排序（英语：Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。1 插入排序 def insertionSort(arr): for i in range(1, len(arr)): key = arr[i] j = i - 1 while j \u003e= 0 and key \u003c arr[j]: arr[j+1] = arr[j] j -= 1 arr[j+1] = key return arr Python 插入排序 ↩︎ ","date":"2022-12-06","objectID":"/python-insertion-sort/:0:0","tags":["排序"],"title":"Python 插入排序 ","uri":"/python-insertion-sort/"},{"categories":["刷题"],"content":"Python 选择排序","date":"2022-12-06","objectID":"/python-selection-sort/","tags":["排序"],"title":"Python 选择排序 ","uri":"/python-selection-sort/"},{"categories":["刷题"],"content":"选择排序（Selection sort）是一种简单直观的排序算法。它的工作原理如下。1 首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置， 然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。 以此类推，直到所有元素均排序完毕。 选择排序 def selectionSort(s): n = len(s) for i in range(n): min_idx = i for j in range(i+1, n): if s[j] \u003c s[min_idx]: min_idx = j s[i], s[min_idx] = s[min_idx], s[i] return s Python 选择排序 ↩︎ ","date":"2022-12-06","objectID":"/python-selection-sort/:0:0","tags":["排序"],"title":"Python 选择排序 ","uri":"/python-selection-sort/"},{"categories":["刷题"],"content":"力扣-单词拆分","date":"2022-12-06","objectID":"/word-break/","tags":["力扣"],"title":"力扣-单词拆分 ","uri":"/word-break/"},{"categories":["刷题"],"content":"🔗 题目链接 给你一个字符串 s 和一个字符串列表 wordDict 作为字典。请你判断是否可以利用字典中出现的单词拼接出 s。 注意：不要求字典中出现的单词全部都使用，并且字典中的单词可以重复使用。 示例 1： 输入：s = “leetcode”, wordDict = [“leet”, “code”] 输出：true 解释：返回 true 因为 “leetcode” 可以由 “leet” 和 “code” 拼接成。 示例 2： 输入：s = “applepenapple”, wordDict = [“apple”, “pen”] 输出：true 解释：返回 true 因为 “applepenapple” 可以由 “apple” “pen” “apple” 拼接成。注意，你可以重复使用字典中的单词。 示例 3： 输入：s = “catsandog”, wordDict = [“cats”, “dog”, “sand”, “and”, “cat”] 输出false 提示： $1 \u003c= s.length \u003c= 300$ $1 \u003c= wordDict.length \u003c= 1000$ $1 \u003c= wordDict[i].length \u003c= 20$ s 和 wordDict[i] 仅有小写英文字母组成 wordDict 中的所有字符串互不相同 动态规划+记忆化回溯 逐行解释 python31 ","date":"2022-12-06","objectID":"/word-break/:0:0","tags":["力扣"],"title":"力扣-单词拆分 ","uri":"/word-break/"},{"categories":["刷题"],"content":"动态规划 动态规划 初始化 $dp=[False,⋯,False]$，长度为 $n+1$。$n$ 为字符串长度。$dp[i]$ 表示 $s$ 的前 $i$ 位是否可以用 $wordDict$ 中的单词表示。 初始化 $dp[0]=True$，空字符可以被表示。 遍历字符串的所有子串，遍历开始索引 $i$，遍历区间 $[0,n)$： 遍历结束索引 $j$，遍历区间 $[i+1,n+1)$： 若 $dp[i]=True$ 且 $s[i,⋯,j)$ 在 $wordlistword$ 中：$dp[j]=True$。解释：$dp[i]=True$ 说明 $s$ 的前 $i$ 位可以用 $wordDict$ 表示，则 $s[i,⋯,j)$ 出现在 $wordDict$ 中，说明 $s$ 的前 $j$ 位可以表示。 返回 $dp[n]$ 代码 def wordBreak(s, wordDict): n = len(s) dp = [False]*(n+1) dp[0] = True for i in range(n): for j in range(i+1, n+1): if(dp[i] and (s[i:j] in wordDict)): dp[j] = True return dp[-1] 复杂度分析 时间复杂度：$O(n^{2})$ 空间复杂度：$O(n)$ 动态规划+记忆化回溯 逐行解释 python3 ↩︎ ","date":"2022-12-06","objectID":"/word-break/:1:0","tags":["力扣"],"title":"力扣-单词拆分 ","uri":"/word-break/"},{"categories":["刷题"],"content":"力扣-查询两个字符串的最长公共子串","date":"2022-12-05","objectID":"/longest/","tags":["力扣"],"title":"力扣-查询两个字符串的最长公共子串 ","uri":"/longest/"},{"categories":["刷题"],"content":"查询两个字符串的最长公共子串。 用 Python 实现查找两个字符串 a,b 中的最长公共子串1 Python——查询两个字符串的最长公共子串2 ","date":"2022-12-05","objectID":"/longest/:0:0","tags":["力扣"],"title":"力扣-查询两个字符串的最长公共子串 ","uri":"/longest/"},{"categories":["刷题"],"content":"循环查找 思路： 通过字符串1从全长开始判断是否存在于字符串2中，如果不存在则迭代至只有1位字符 通过列表来保存结果，以免出现有多个同长的最长子串情况 选择长度短的字符串作为操作字符串，以提升效率 def getLongestSameStr(str1, str2): # 判断两个字符串长短，取短的那个进行操作 if len(str1) \u003e len(str2): str1, str2 = str2, str1 f = [] for i in range(len(str1), 0, -1): for j in range(len(str1) + 1 - i): e = str1[j:j + i] if e in str2: f.append(e) # 判断当前长度下，是否存在子串 if f: break f1 = \",\".join(f) return f1 用Python实现查找两个字符串a,b中的最长公共子串 ↩︎ Python——查询两个字符串的最长公共子串 ↩︎ ","date":"2022-12-05","objectID":"/longest/:1:0","tags":["力扣"],"title":"力扣-查询两个字符串的最长公共子串 ","uri":"/longest/"},{"categories":["学习"],"content":"rdd里面groupby和reduceby有什么区别","date":"2022-12-04","objectID":"/reduceby/","tags":["数仓"],"title":"groupby和reduceby的区别","uri":"/reduceby/"},{"categories":["学习"],"content":"rdd 里面 groupby 和 reduceby 有什么区别。 reduceByKey 的泛型参数是 [V]，而 groupByKey 的泛型参数是 [CompactBuffer[V]]。这直接导致了 reduceByKey 和 groupByKey 的返回值不同，前者是 RDD[(K, V)]，而后者是 RDD[(K, Iterable[V])] 然后就是 mapSideCombine=false 了，这个 mapSideCombine 参数的默认是 true 的。这个值有什么用呢，上面也说了，这个参数的作用是控制要不要在 map端进行初步合并Combine。可以看看下面具体的例子。 groupbykey reducebykey 从功能上来说，可以发现 ReduceByKey 其实就是会在每个节点先进行一次合并的操作，而 groupByKey 没有。 这么来看 ReduceByKey 的性能会比 groupByKey 好很多，因为有些工作在节点已经处理了。 reduceByKey：按照 key 进行聚合，在 shuffle 之前有combine（预聚合）操作，返回结果是 RDD[k,v]。 groupByKey：按照 key 进行分组，直接进行 shuffle。 开发指导：reduceByKey 比 groupByKey，建议使用。但是需要注意是否会影响业务逻辑。 ","date":"2022-12-04","objectID":"/reduceby/:0:0","tags":["数仓"],"title":"groupby和reduceby的区别","uri":"/reduceby/"},{"categories":["学习"],"content":"技术选型：Kylin、Druid、ClickHouse如何选择？","date":"2022-12-04","objectID":"/olap/","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"Kylin、Druid、ClickHouse 是目前主流的 OLAP 引擎，本文尝试从数据模型和索引结构两个角度，分析这几个引擎的核心技术，并做简单对比。在阅读本文之前希望能对 Kylin、Druid、ClickHouse 有所理解。 ","date":"2022-12-04","objectID":"/olap/:0:0","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"Kylin ","date":"2022-12-04","objectID":"/olap/:1:0","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"Kylin 数据模型 Kylin 的数据模型本质上是将二维表（Hive表）转换为 Cube，然后将 Cube 存储到 HBase 表中，也就是两次转换。 第一次转换，其实就是传统数据库的 Cube 化，Cube 由 CuboId 组成，下图每个节点都被称为一个 CuboId，CuboId 表示固定列的数据数据集合，比如 “AB” 两个维度组成的 CuboId 的数据集合等价于以下 SQL 的数据集合： select A, B, sum(M), sum(N) from table group by A, B kylin 第二次转换，是将 Cube中 的数据存储到 HBase 中，转换的时候 CuboId 和维度信息序列化到 rowkey，度量列组成列簇。在转换的时候数据进行了预聚合。下图展示了 Cube 数据在 HBase 中的存储方式。 kylin ","date":"2022-12-04","objectID":"/olap/:1:1","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"Kylin索引结构 因为 Kylin 将数据存储到 HBase 中，所以 kylin 的数据索引就是 HBase 的索引。HBase 的索引是简化版本的 B+树，相比于 B+树，HFile 没有对数据文件的更新操作。 HFile 的索引是按照 rowkey 排序的聚簇索引，索引树一般为二层或者三层，索引节点比 MySQL 的 B+树大，默认是64KB。数据查找的时候通过树形结构定位到节点，节点内部数据是按照 rowkey 有序的，可以通过二分查找快速定位到目标。 kylin ","date":"2022-12-04","objectID":"/olap/:1:2","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"Kylin小结 适用于聚合查询场景； 因为数据预聚合，Kylin 可以说是最快的查询引擎（group-by 查询这样的复杂查询，可能只需要扫描 1 条数据）； Kylin 查询效率取决于是否命中 CuboId，查询波动较大； HBase 索引有点类似 MySQL 中的联合索引，维度在 rowkey 中的排序和查询维度组合对查询效率影响巨大； 所以 Kylin 建表需要业务专家参与。 ","date":"2022-12-04","objectID":"/olap/:1:3","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"Druid ","date":"2022-12-04","objectID":"/olap/:2:0","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"Druid 数据模型 Druid 数据模型比较简单，它将数据进行预聚合，只不过预聚合的方式与 Kylin 不同，Kylin 是 Cube 化，Druid 的预聚合方式是将所有维度进行 Group-by，可以参考下图： Druid ","date":"2022-12-04","objectID":"/olap/:2:1","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"Druid索引结构 Druid 索引结构使用自定义的数据结构，整体上它是一种列式存储结构，每个列独立一个逻辑文件（实际上是一个物理文件，在物理文件内部标记了每个列的start和offset）。对于维度列设计了索引，它的索引以 Bitmap 为核心。 下图为 “city” 列的索引结构： Druid 首先将该列所有的唯一值排序，并生成一个字典，然后对于每个唯一值生成一个 Bitmap，Bitmap 的长度为数据集的总行数，每个 bit 代表对应的行的数据是否是该值。Bitmap 的下标位置和行号是一一对应的，所以可以定位到度量列，Bitmap 可以说是反向索引。同时数据结构中保留了字典编码后的所有列值，其为正向的索引。 那么查询如何使用索引呢？以以下查询为例： select site, sum(pv) from xx where date = 2020-01-01 and city = 'bj' group by site city 列中二分查找 dictionary并找到 'bj' 对应的 bitmap 遍历 city 列，对于每一个字典值对应的 bitmap 与 'bj' 的 bitmap 做与操作 每个相与后的 bitmap 即为 city = 'bj' 查询条件下的 site 的一个 group 的 pv 的索引 通过索引在 pv 列中查找到相应的行，并做 agg 后续计算 ","date":"2022-12-04","objectID":"/olap/:2:2","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"Druid小结 Druid 适用于聚合查询场景但是不适合有超高基维度的场景；存储全维度 group-by 后的数据，相当于只存储了 Kylin Cube 的 Base-CuboID；每个维度都有创建索引，所以每个查询都很快，并且没有类似 Kylin 的巨大的查询效率波动。 ","date":"2022-12-04","objectID":"/olap/:2:3","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"ClickHouse ","date":"2022-12-04","objectID":"/olap/:3:0","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"Clickhouse 索引结构 此处只讨论 MergeTree 引擎。 因为 Clickhouse 数据模型就是普通二维表，这里不做介绍，只讨论索引结构。整体上 Clickhouse 的索引也是列式索引结构，每个列一个文件。 Clickhouse索引的大致思路是： 首先选取部分列作为索引列，整个数据文件的数据按照索引列有序，这点类似 MySQL 的联合索引 其次将排序后的数据每隔 8194 行选取出一行，记录其索引值和序号，注意这里的序号不是行号，序号是从零开始并递增的，Clickhouse 中序号被称作 Mark’s number 然后对于每个列（索引列和非索引列），记录 Mark’s number 与对应行的数据的 offset。 下图中以一个二维表（date, city, action）为例介绍了整个索引结构，其中（date,city）是索引列。 Clickhouse 那么查询如何使用索引呢？以以下查询为例： select count(distinct action) where date = toDate(2020-01-01) and city = 'bj' 二分查找 primary.idx 并找到对应的 mark’s number集合（即数据 block 集合） 在上一步骤中的 block 中，在 date 和 city 列中查找对应的值的行号集合，并做交集，确认行号集合 将行号转换为 mark’s number 和 offset in block（注意这里的 offset 以行为单位而不是 byte） 在 action 列中，根据 mark’s number 和 ``.mark文件确认数据block在bin文件中的offset，然后根据 offset in block` 定位到具体的列值。 后续计算 该实例中包含了对于列的正反两个方向的查找过程。 反向：查找 date = toDate(2020-01-01) and city = ‘bj’ 数据的行号； 正向：根据行号查找 action 列的值。 对于反向查找，只有在查找条件匹配最左前缀的时候，才能剪枝掉大量数据，其它时候并不高效。 ","date":"2022-12-04","objectID":"/olap/:3:1","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"Clickhouse小结 MergeTree Family 作为主要引擎系列，其中包含适合明细数据的场景和适合聚合数据的场景； Clickhouse 的索引有点类似 MySQL 的联合索引，当查询前缀元组能命中的时候效率最高，可是一旦不能命中，几乎会扫描整个表，效率波动巨大； 所以建表需要业务专家，这一点跟 Kylin 类似。 ","date":"2022-12-04","objectID":"/olap/:3:2","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["学习"],"content":"总结 Kylin、Druid 只适合聚合场景，ClickHouse 适合明细和聚合场景 聚合场景，查询效率排序：Kylin \u003e Druid \u003e ClickHouse Kylin、ClickHouse 建表都需要业务专家参与 Kylin、ClickHouse 查询效率都可能产生巨大差异 ClickHouse 在向量化方面做得的最好，Druid 少量算子支持向量化、Kylin 目前还不支持向量化计算 ","date":"2022-12-04","objectID":"/olap/:4:0","tags":["数仓"],"title":"技术选型：Kylin、Druid、ClickHouse如何选择？","uri":"/olap/"},{"categories":["刷题"],"content":"力扣-最长回文子串","date":"2022-12-03","objectID":"/longest-palindromic-substring/","tags":["力扣"],"title":"力扣-最长回文子串 ","uri":"/longest-palindromic-substring/"},{"categories":["刷题"],"content":"🔗 题目链接 给你一个字符串 s，找到 s 中最长的回文子串。 示例 1： 输入：s = “babad” 输出：“bab” 解释：“aba” 同样是符合题意的答案。 示例 2： 输入：s = “cbbd” 输出：“bb” 提示： $1 \u003c= s.length \u003c= 1000$ s 仅由数字和英文字母组成 ","date":"2022-12-03","objectID":"/longest-palindromic-substring/:0:0","tags":["力扣"],"title":"力扣-最长回文子串 ","uri":"/longest-palindromic-substring/"},{"categories":["刷题"],"content":"思路一 从中心向两端扩散的双指针1 如果回文串的长度为奇数，则它有一个中心字符；如果回文串的长度为偶数，则可以认为它有两个中心字符。 那么可以事先写一个函数，在 s 中寻找以 s[l] 和 s[r] 为中心的最长回文串（即 s[l]==s[r]，然后在不越界的前提下左移 l，右移 r），这样，如果输入相同的 l 和 r，就相当于寻找长度为奇数的回文串，如果输入相邻的 l 和 r，则相当于寻找长度为偶数的回文串： def palindrome(s, l, r): # 防止索引越界 while l \u003e= 0 and r \u003c len(s) and s[l] == s[r]: # 双指针，向两边展开，左往左，右往右 l -= 1 r += 1 # 返回以 s[l] 和 s[r] 为中心的最长回文串 return s[l+1: r] # 结束循环时l比回文串的内容多左移了一个 r多右移了一个 那么求最长回文串，就相当于是： for i in range(len(s)): 找到以 s[i] 为中心的回文串 找到以 s[i]和s[i+1] 为中心的回文串 更新答案 代码： def longestPalindrome(s: str) -\u003e str: def palindrome(s, l, r): while l \u003e= 0 and r \u003c len(s) and s[l] == s[r]: l -= 1 r += 1 return s[l+1:r] res = '' for i in range(len(s)): sub1 = palindrome(s, i, i) sub2 = palindrome(s, i, i+1) res = sub1 if len(sub1) \u003e len(res) else res res = sub2 if len(sub2) \u003e len(res) else res return res 从中心向两端扩散的双指针 ↩︎ ","date":"2022-12-03","objectID":"/longest-palindromic-substring/:1:0","tags":["力扣"],"title":"力扣-最长回文子串 ","uri":"/longest-palindromic-substring/"},{"categories":["刷题"],"content":"力扣-平方数之和","date":"2022-12-02","objectID":"/sum-of-square-numbers/","tags":["力扣"],"title":"力扣-平方数之和 ","uri":"/sum-of-square-numbers/"},{"categories":["刷题"],"content":"🔗 题目链接 给定一个非负整数 $c$，你要判断是否存在两个整数 $a$ 和 $b$，使得 $a^2 + b^2 = c$。 示例 1： 输入：c = 5 输出：true 解释：1 * 1 + 2 * 2 = 5 示例 2： 输入：c = 3 输出：false 提示： $0 \u003c= c \u003c= 2^{31} - 1$ 解题思路 为什么双指针不会错过正确答案？双指针的本质。1 看了官方题解的双指针算法，不免产生一个疑问，假设初始化时，左指针 $low = 0$，右指针 $high = sqrt(c)$。 为什么 $low^2+high^2\u003cc$ 时，要让 low++ 而不是 high++ 呢？或者说为什么让 low++ 可以保证不错过正确答案呢？ 同理，为什么 $low^2+high^2\u003ec$ 时，要让 high-- 而不是 low-- 呢？或者说为什么让 high-- 可以保证不错过正确答案呢？ 其实我们可以把双指针的过程看成在一个矩阵中搜寻的过程。举个例子，c = 18，初始化 low = 0，high = 4，那么看如下矩阵： 矩阵 矩阵沿主对角线对称 ($low\u003c=high$)，其中的元素表示 $low^2+high^2$ 的值，黄色格子表示当前的 $low^2+high^2$，绿色格子表示目标 c，low++ 相当于让黄色格子下移，high-- 则相当于让黄色格子左移。这里矩阵的性质和搜索的过程其实和搜索二维矩阵 II是一样的。每一列从上到下升序，每一行从左到右升序。 查找的过程具有如下性质： 初始化时黄色格子必定在矩阵的右上角。 每次比较 $low^2+high^2$ 可以排除矩阵的一行或一列。 如下图： 矩阵 由于以上性质，当前黄色格子的上方和右边的所有元素一定是已经被排除的，所以黄色格子在搜索过程中只有两种行为： 小于 ccc ：左边的元素都小于当前元素，只能下移，相当于 low++。此时排除的是黄色格子以及左边同行的元素，都小于 ccc ，所以不会错过正确答案。 大于 ccc ：下面的元素都大于当前元素，只能左移，相当于 high--。此时排除的是黄色格子以及下方同列的元素，都大于 ccc ，所以不会错过正确答案。 如此一来，双指针这个操作就十分自然了。 def judgeSquareSum(c: int) -\u003e bool: low, high = 0, int(c**0.5) while low \u003c= high: sumOf = low*low + high*high if sumOf == c: return True elif sumOf \u003c c: low += 1 else: high -= 1 return False 复杂度分析 矩阵的行数和列数都是 $\\sqrt{c}$，所以时间复杂度为 $O(\\sqrt{c})$。 为什么双指针不会错过正确答案？双指针的本质。 ↩︎ ","date":"2022-12-02","objectID":"/sum-of-square-numbers/:0:0","tags":["力扣"],"title":"力扣-平方数之和 ","uri":"/sum-of-square-numbers/"},{"categories":["刷题"],"content":"力扣-字符串相加","date":"2022-12-02","objectID":"/add-strings/","tags":["力扣"],"title":"力扣-字符串相加 ","uri":"/add-strings/"},{"categories":["刷题"],"content":"🔗 题目链接 给定两个字符串形式的非负整数 num1 和 num2，计算它们的和并同样以字符串形式返回。 你不能使用任何內建的用于处理大整数的库（比如 BigInteger）， 也不能直接将输入的字符串转换为整数形式。 示例 1： 输入：num1 = “11”, num2 = “123” 输出：“134” 示例 2： 输入：num1 = “456”, num2 = “77” 输出：“533” 示例 3： 输入：num1 = “0”, num2 = “0” 输出：“0” 提示： $1 \u003c= num1.length, num2.length \u003c= 10^4$ num1 和 num2 都只包含数字 0-9 num1 和 num2 都不包含任何前导零 ","date":"2022-12-02","objectID":"/add-strings/:0:0","tags":["力扣"],"title":"力扣-字符串相加 ","uri":"/add-strings/"},{"categories":["刷题"],"content":"算法流程 字符串相加 （双指针，清晰图解）1 设定 i，j 两指针分别指向 num1，num2 尾部，模拟人工加法； 计算进位： 计算 carry = tmp // 10，代表当前位相加是否产生进位； 添加当前位： 计算 tmp = n1 + n2 + carry，并将当前位 tmp % 10 添加至 res 头部； 索引溢出处理： 当指针 i 或 j 走过数字首部后，给 n1，n2 赋值为 0，相当于给 num1，num2 中长度较短的数字前面填 0，以便后续计算。 当遍历完 num1，num2 后跳出循环，并根据 carry 值决定是否在头部添加进位 1，最终返回 res 即可。 def addStrings(num1: str, num2: str) -\u003e str: res = \"\" i, j, carry = len(num1) - 1, len(num2) - 1, 0 while i \u003e= 0 or j \u003e= 0: n1 = int(num1[i]) if i \u003e= 0 else 0 n2 = int(num2[j]) if j \u003e= 0 else 0 tmp = n1 + n2 + carry carry = tmp // 10 res = str(tmp % 10) + res i, j = i - 1, j - 1 return \"1\" + res if carry else res 复杂度分析 时间复杂度 $O(max(M,N))$：其中 $M$，$N$ 为 $2$ 数字长度，按位遍历一遍数字（以较长的数字为准）； 空间复杂度 $O(1)$：指针与变量使用常数大小空间。 字符串相加 （双指针，清晰图解） ↩︎ ","date":"2022-12-02","objectID":"/add-strings/:1:0","tags":["力扣"],"title":"力扣-字符串相加 ","uri":"/add-strings/"},{"categories":["刷题"],"content":"力扣-x的平方根","date":"2022-12-02","objectID":"/sqrtx/","tags":["力扣","二分查找"],"title":"力扣-x的平方根 ","uri":"/sqrtx/"},{"categories":["刷题"],"content":"🔗 题目链接 给你一个非负整数 x ，计算并返回 x 的算术平方根。 由于返回类型是整数，结果只保留整数部分，小数部分将被舍去。 注意：不允许使用任何内置指数函数和算符，例如 pow(x, 0.5) 或者 x ** 0.5。 示例 1： 输入：x = 4 输出：2 示例 2： 输入：x = 8 输出：2 解释：8 的算术平方根是 2.82842…, 由于返回类型是整数，小数部分将被舍去。 提示： $0 \u003c= x \u003c= 2^{31} - 1$ ","date":"2022-12-02","objectID":"/sqrtx/:0:0","tags":["力扣","二分查找"],"title":"力扣-x的平方根 ","uri":"/sqrtx/"},{"categories":["刷题"],"content":"二分查找 参考链接：x 的平方根1 由于 $x$ 平方根的整数部分 $ans$ 是满足 $k^2 \\leq x$ 的最大 $k$ 值，因此我们可以对 $k$ 进行二分查找，从而得到答案。 二分查找的下界为 $0$，上界可以粗略地设定为 $x$。在二分查找的每一步中，我们只需要比较中间元素 $mid$ 的平方与 $x$ 的大小关系，并通过比较的结果调整上下界的范围。由于我们所有的运算都是整数运算，不会存在误差，因此在得到最终的答案 $ans$ 后，也就不需要再去尝试 $ans + 1$ 了。 def mySqrt(x): l, r, ans = 0, x, -1 while l \u003c= r: mid = (l + r) // 2 if mid * mid \u003c= x: ans = mid l = mid + 1 else: r = mid - 1 return ans 复杂度分析 时间复杂度：$O(log⁡x)$，即为二分查找需要的次数。 空间复杂度：$O(1)$。 x 的平方根 ↩︎ ","date":"2022-12-02","objectID":"/sqrtx/:1:0","tags":["力扣","二分查找"],"title":"力扣-x的平方根 ","uri":"/sqrtx/"},{"categories":["学习"],"content":"什么是分区分桶？","date":"2022-12-01","objectID":"/bucket/","tags":["数仓"],"title":"什么是分区分桶？","uri":"/bucket/"},{"categories":["学习"],"content":"分区和分桶面试过程中会经常考察，需要重点掌握。 ","date":"2022-12-01","objectID":"/bucket/:0:0","tags":["数仓"],"title":"什么是分区分桶？","uri":"/bucket/"},{"categories":["学习"],"content":"分区 分区 ","date":"2022-12-01","objectID":"/bucket/:1:0","tags":["数仓"],"title":"什么是分区分桶？","uri":"/bucket/"},{"categories":["学习"],"content":"什么是分区 将整个表的数据在存储时，按照 “分区键的列值” 划分成多个子目录来存储。区从形式上可以理解为文件夹。 注意：子目录名称就是分区名（分区键的列值）。 ","date":"2022-12-01","objectID":"/bucket/:1:1","tags":["数仓"],"title":"什么是分区分桶？","uri":"/bucket/"},{"categories":["学习"],"content":"为什么要分区 随着系统运行时间的增加，表的数据量会越来越大，而 Hive 查询数据的数据的时候通常使用的是“全表扫描”，这样将会导致查询效率大大的降低。 Hive 引进的分区技术，能避免 Hive 全表扫描，提升查询效率。 比如我们要收集某个大型网站的日志数据，一个网站每天的日志数据存在同一张表上，由于每天会生成大量的日志，导致数据表的内容巨大，在查询时进行全表扫描耗费的资源非常多。那其实这个情况下，我们可以按照日期对数据表进行分区，不同日期的数据存放在不同的分区，在查询时只要指定分区字段的值就可以直接从该分区查找。 ","date":"2022-12-01","objectID":"/bucket/:1:2","tags":["数仓"],"title":"什么是分区分桶？","uri":"/bucket/"},{"categories":["学习"],"content":"创建分区表 创建分区表的时候，要通过关键字 partitioned by (name string) 声明该表是分区表，并且是按照字段 name 进行分区，name 值一致的所有记录存放在一个分区中，分区属性 name 的类型是 string 类型。当然，可以依据多个列进行分区，即对某个分区的数据按照某些列继续分区。 注意 千万不要以为是对属性表中真正存在的列按照属性值的异同进行分区。比如上面的分区依据的列 name 并不真正的存在于数据表中，是我们为了方便管理添加的一个伪列，这个列的值也是我们人为规定的，不是从数据表中读取之后根据值的不同将其分区。我们并不能按照某个数据表中真实存在的列，如userid来分区。 ","date":"2022-12-01","objectID":"/bucket/:1:3","tags":["数仓"],"title":"什么是分区分桶？","uri":"/bucket/"},{"categories":["学习"],"content":"分桶 ","date":"2022-12-01","objectID":"/bucket/:2:0","tags":["数仓"],"title":"什么是分区分桶？","uri":"/bucket/"},{"categories":["学习"],"content":"什么是分桶 分桶是相对分区进行更细粒度的划分。桶是通过对指定列进行哈希计算来实现的，通过哈希值将一个列名下的数据切分为一组桶，并使每个桶对应于该列名下的一个存储文件。 注意：在 hdfs 目录上，桶是以文件的形式存在的，而不是像分区那样以文件夹的形式存在。 ","date":"2022-12-01","objectID":"/bucket/:2:1","tags":["数仓"],"title":"什么是分区分桶？","uri":"/bucket/"},{"categories":["学习"],"content":"为什么要分桶 在分区数量过于庞大以至于可能导致文件系统崩溃时，我们就需要使用分桶来解决问题了。 分区中的数据可以被进一步拆分成桶，不同于分区对列直接进行拆分，桶往往使用列的哈希值对数据打散，并分发到各个不同的桶中从而完成数据的分桶过程。 注意 hive 使用对分桶所用的值进行 hash，并用 hash 结果除以桶的个数做取余运算的方式来分桶，保证了每个桶中都有数据，但每个桶中的数据条数不一定相等。 ","date":"2022-12-01","objectID":"/bucket/:2:2","tags":["数仓"],"title":"什么是分区分桶？","uri":"/bucket/"},{"categories":["学习"],"content":"创建分桶 与分区不同的是，分区依据的不是真实数据表文件中的列，而是我们指定的伪列，但是分桶是依据数据表中真实的列而不是伪列。所以在指定分区依据的列的时候要指定列的类型，因为在数据表文件中不存在这个列，相当于新建一个列。而分桶依据的是表中已经存在的列，这个列的数据类型显然是已知的，所以不需要指定列的类型。 ","date":"2022-12-01","objectID":"/bucket/:2:3","tags":["数仓"],"title":"什么是分区分桶？","uri":"/bucket/"},{"categories":["学习"],"content":"Mysql 面试题","date":"2022-12-01","objectID":"/mysql-ques/","tags":["mysql"],"title":"Mysql 面试题","uri":"/mysql-ques/"},{"categories":["学习"],"content":"史上最全的大厂 Mysql 面试题在这里。 ","date":"2022-12-01","objectID":"/mysql-ques/:0:0","tags":["mysql"],"title":"Mysql 面试题","uri":"/mysql-ques/"},{"categories":["学习"],"content":"MySQL 中 myisam 与 innodb的区别 ","date":"2022-12-01","objectID":"/mysql-ques/:1:0","tags":["mysql"],"title":"Mysql 面试题","uri":"/mysql-ques/"},{"categories":["学习"],"content":"问 5 点不同 InnoDB 支持事物，而 MyISAM 不支持事物 InnoDB 支持行级锁，而 MyISAM 支持表级锁 InnoDB 支持 MVCC, 而 MyISAM 不支持 InnoDB 支持外键，而 MyISAM 不支持 InnoDB 不支持全文索引，而 MyISAM 支持。 ","date":"2022-12-01","objectID":"/mysql-ques/:1:1","tags":["mysql"],"title":"Mysql 面试题","uri":"/mysql-ques/"},{"categories":["学习"],"content":"innodb 引擎的 4 大特性 插入缓冲insert buffer 二次写double write 自适应哈希索引ahi 预读read ahead ","date":"2022-12-01","objectID":"/mysql-ques/:1:2","tags":["mysql"],"title":"Mysql 面试题","uri":"/mysql-ques/"},{"categories":["学习"],"content":"2 者 select count(*) 哪个更快，为什么 myisam 更快，因为 myisam 内部维护了一个计数器，可以直接调取。 ","date":"2022-12-01","objectID":"/mysql-ques/:1:3","tags":["mysql"],"title":"Mysql 面试题","uri":"/mysql-ques/"},{"categories":["学习"],"content":"大数据之 Reduce Join","date":"2022-12-01","objectID":"/reducejoin/","tags":["数仓"],"title":"大数据之 Reduce Join","uri":"/reducejoin/"},{"categories":["学习"],"content":"介绍 hive 里的 Reduce Join 的一些原理和使用技巧。 ","date":"2022-12-01","objectID":"/reducejoin/:0:0","tags":["数仓"],"title":"大数据之 Reduce Join","uri":"/reducejoin/"},{"categories":["学习"],"content":"应用场景 适合于大表 Join 大表 ","date":"2022-12-01","objectID":"/reducejoin/:1:0","tags":["数仓"],"title":"大数据之 Reduce Join","uri":"/reducejoin/"},{"categories":["学习"],"content":"原理 原理 将两张表的数据在 shuffle 阶段利用 shuffle 的分组来将数据按照关联字段进行合并 必须经过 shuffle，利用 Shuffle 过程中的分组来实现关联 ","date":"2022-12-01","objectID":"/reducejoin/:2:0","tags":["数仓"],"title":"大数据之 Reduce Join","uri":"/reducejoin/"},{"categories":["学习"],"content":"过程 Map阶段：构建（key(tag),value）,key 这里后面的数字是 tag，后面在 reduce 阶段用来区分来自于那个表的数据，对 key 求 hashcode 设为 hivekey； Shuffle阶段：如果 key 在不同机器上，会通过网络传输把 hivekey 相同的数据汇集到一台机器； Reduce阶段：把 tag=1 的内容（value），都加到 tag=0 的后面，合并输出。 reduce join原理 ","date":"2022-12-01","objectID":"/reducejoin/:3:0","tags":["数仓"],"title":"大数据之 Reduce Join","uri":"/reducejoin/"},{"categories":["学习"],"content":"使用 Hive 会自动判断是否满足 Map Join，如果不满足 Map Join，则自动执行 Reduce Join。 ","date":"2022-12-01","objectID":"/reducejoin/:4:0","tags":["数仓"],"title":"大数据之 Reduce Join","uri":"/reducejoin/"},{"categories":["学习"],"content":"大表join小表，独钟爱mapjoin","date":"2022-12-01","objectID":"/mapjoin/","tags":["数仓"],"title":"大表join小表，独钟爱mapjoin","uri":"/mapjoin/"},{"categories":["学习"],"content":"在 Hive 调优里面，经常会问到一个很小的表和一个大表进行 join，如何优化。 Shuffle 阶段代价非常昂贵，因为它需要排序和合并。减少 Shuffle 和 Reduce 阶段的代价可以提高任务性能。 MapJoin 通常用于一个很小的表和一个大表进行 join 的场景，具体小表有多小，由参数 hive.mapjoin.smalltable.filesize 来决定，该参数表示小表的总大小，默认值为 25000000 字节，即 25M。 Hive0.7 之前，需要使用 hint 提示 /*+ mapjoin(table) */ 才会执行 MapJoin,否则执行 Common Join，但在 0.7 版本之后，默认自动会转换 Map Join，由参数 hive.auto.convert.join 来控制，默认为true。 假设 a 表为一张大表，b 为小表，并且 hive.auto.convert.join=true，那么 Hive 在执行时候会自动转化为 MapJoin。 MapJoin 简单说就是 在 Map 阶段将小表数据从 HDFS 上读取到内存中的哈希表中，读完后将内存中的哈希表序列化为哈希表文件 在下一阶段，当 MapReduce 任务启动时，会将这个哈希表文件上传到 Hadoop 分布式缓存中，该缓存会将这些文件发送到每个 Mapper 的本地磁盘上。 因此，所有 Mapper 都可以将此持久化的哈希表文件加载回内存，并像之前一样进行 Join。 顺序扫描大表完成 Join。减少昂贵的 shuffle 操作及 reduce 操作 MapJoin分为两个阶段： 通过 MapReduce Local Task，将小表读入内存，生成 HashTableFiles 上传至 Distributed Cache 中，这里会 HashTableFiles 进行压缩。 MapReduce Job 在 Map 阶段，每个 Mapper 从 Distributed Cache 读取 HashTableFiles 到内存中，顺序扫描大表，在 Map 阶段直接进行 Join，将数据传递给下一个 MapReduce 任务 mapjoin 在 MapReduce 任务中，第一步就是创建一个 MapReduce 本地任务，然后该 map / reduce 任务从 HDFS 读取小表的数据，然后把数据保存到内存中的哈希表，然后再存储到一个哈希表文件。接着，当 MapReduce join 任务启动的时候，它会把哈希表文件移到 Hadoop 分布式内存中，并把哈希表文件存储到每个 mapper 的本地磁盘上。所有的 mapper 都能把这个哈希表文件加载到内存，然后在 map 阶段做 join 操作。1 Hive map Join ↩︎ ","date":"2022-12-01","objectID":"/mapjoin/:0:0","tags":["数仓"],"title":"大表join小表，独钟爱mapjoin","uri":"/mapjoin/"},{"categories":["学习"],"content":"order by,sort by,distribute by,cluster by的区别","date":"2022-12-01","objectID":"/orderby/","tags":["数仓"],"title":"order by,sort by,distribute by,cluster by的区别","uri":"/orderby/"},{"categories":["学习"],"content":"深入探究 order by,sort by,distribute by,cluster by 的区别，并用数据征服你。 ","date":"2022-12-01","objectID":"/orderby/:0:0","tags":["数仓"],"title":"order by,sort by,distribute by,cluster by的区别","uri":"/orderby/"},{"categories":["学习"],"content":"准备工作 ","date":"2022-12-01","objectID":"/orderby/:1:0","tags":["数仓"],"title":"order by,sort by,distribute by,cluster by的区别","uri":"/orderby/"},{"categories":["学习"],"content":"建表 drop table if exists wedw_dw.province_city_info; create table wedw_dw.province_city_info( proinve_name string COMMENT '省份名' ,city_name string COMMENT '市名' ,pc_cnt bigint COMMENT '人口数(单位：万)' ,in_come decimal(16,2) COMMENT '收入(单位：亿)' ) row format delimited fields terminated by ','; ","date":"2022-12-01","objectID":"/orderby/:1:1","tags":["数仓"],"title":"order by,sort by,distribute by,cluster by的区别","uri":"/orderby/"},{"categories":["学习"],"content":"数据准备 数据准备 ","date":"2022-12-01","objectID":"/orderby/:1:2","tags":["数仓"],"title":"order by,sort by,distribute by,cluster by的区别","uri":"/orderby/"},{"categories":["学习"],"content":"order by 升序: asc 降序: desc 默认升序 order by 会对输入做全局排序，因此只有一个 reducer(多个 reducer 无法保证全局有序)，然而只有一个 Reducer 会导致当输入规模较大时，消耗较长的计算时间。 select * from wedw_dw.province_city_info order by in_come desc; order by ","date":"2022-12-01","objectID":"/orderby/:2:0","tags":["数仓"],"title":"order by,sort by,distribute by,cluster by的区别","uri":"/orderby/"},{"categories":["学习"],"content":"sort by sort by 不是全局排序，其在数据进入 reducer 前完成排序，因此，如果用 sort by 进行排序并且设置 mapped.reduce.tasks \u003e 1，则 sort by 只会保证每个 reducer的输出有序，并不保证全局有序。(全排序实现:先用 sortby 保证每个 reducer 输出有序，然后在进行 order by 归并下前面所有的 reducer 输出进行单个 reducer排序，实现全局有序。) 在 sort by 之前我们还有配置属性: //配置ruduce数量 set mapreduce.job.reduces=4; select * from wedw_dw.province_city_info sort by in_come desc; sort by ","date":"2022-12-01","objectID":"/orderby/:3:0","tags":["数仓"],"title":"order by,sort by,distribute by,cluster by的区别","uri":"/orderby/"},{"categories":["学习"],"content":"distribute by distribute by 是控制在 map 端如何拆分数据给 reduce 端的。hive 会根据 distribute by 后面列，对应 reduce 的个数进行分发，默认是采用 hash 算法。 sort by 为每个 reduce 产生一个排序文件。在有些情况下，你需要控制某个特定行应该到哪个 reducer，这通常是为了进行后续的聚集操作。 distribute by 刚好可以做这件事。因此，distribute by 经常和 sort by 配合使用。 distribute by 的分区规则是根据分区字段的 hash 码与 reduce 的个数进行模除后，余数相同的分到一个区。1 Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前。 select * from wedw_dw.province_city_info distribute by proinve_name; distribute by distribute by 经常和 sort by 配合使用。 distribute by 和 sort by 配合 ","date":"2022-12-01","objectID":"/orderby/:4:0","tags":["数仓"],"title":"order by,sort by,distribute by,cluster by的区别","uri":"/orderby/"},{"categories":["学习"],"content":"cluster by 当 distribute by 和 sort by 所指定的字段相同时，即可以使用 cluster by。 select * from wedw_dw.province_city_info distribute by in_come sort by in_come asc; 所指定的字段相同 注意 cluster by 指定的列只能是升序，不能指定 asc 和 desc。 select * from wedw_dw.province_city_info cluster by in_come; cluster by order by, sort by, distribute by, cluster by 区别 ↩︎ ","date":"2022-12-01","objectID":"/orderby/:5:0","tags":["数仓"],"title":"order by,sort by,distribute by,cluster by的区别","uri":"/orderby/"},{"categories":["刷题"],"content":"dub老师给滴题目准备","date":"2022-11-30","objectID":"/dub-exercise/","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"头条大佬 dubhe 老师给的题目准备，好好刷一下。 ","date":"2022-11-30","objectID":"/dub-exercise/:0:0","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"SQL 类型 ","date":"2022-11-30","objectID":"/dub-exercise/:1:0","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"成绩排名 学生成绩表 student,course,score，记录每个学生每门课程（共语文、数学、英语 3 门课）的成绩，请用 SQL 找出每门课程成绩排名前 5 的学生。 select a.course, a.student from ( select scoure, student, row_number() over (partition by course order by socre desc) rn from tab1 ) as a where a.rn \u003c= 5; ","date":"2022-11-30","objectID":"/dub-exercise/:1:1","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"平均分和总分最高 table1: student_id,class table2: student_id,course,score 求各个班级，每一门课程的平均分 求各个班级，总分最高的学生 select a.class, b.course, avg(b.score) as score_avg from table1 as a left join table2 as b on a.student_id = b.student_id group by 1,2 select a.class, a.student_id, a.score_sum from ( select a.class, a.student_id, a.score_sum, row_number() over (partition by a.class order by a.score_sum desc) as rn from ( select a.class, a.student_id, sum(b.score) as score_sum from table1 as a left join table2 as b on a.student_id = b.student_id group by 1,2 ) as a ) as a where a.rn = 1 ","date":"2022-11-30","objectID":"/dub-exercise/:1:2","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"ip 访问次数前 5 给定 hive 表 t1，包含三列，user_id、ip、time：表示 user_id 在时间 time 访问过一次 ip。请利用 hive-sql 计算每个 ip 访问次数排名前 5 的 user_id。 select a.ip, a.user_id, a.cnt from ( select a.ip, a.user_id, a.cnt, row_number() over (partition by a.ip order by a.cnt desc) as rn from ( select ip, user_id, count(time) as cnt from t1 group by 1,2 ) as a ) as a where a.rn \u003c= 5 ","date":"2022-11-30","objectID":"/dub-exercise/:1:3","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"数据清洗 客户端每隔几分钟都会上报一条记录，形式如 uid、time，我们做数据清洗的时候要求：如果 10 分钟内上报多条的话，就只保留最早的一条。请问怎么用 SQL 实现。 // 将一个时间维表(分钟维度存储按照 10 分钟进行 N 等分) with time_tmp as ( select time_format, // 格式 12:10 ntile(24*60/10) over (order by time) as time_rn from dim_time ) select a.uid, a.time from ( select a.uid, a.time, row_number() over (partition by b.time_rn order by a.time) rn from table1 as a left join time_tmp as b on concat(hour(a.time), ':', minute(a.time)) = b.time_format ) as a where a.rn = 1 ","date":"2022-11-30","objectID":"/dub-exercise/:1:4","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"充值额最大的账号 假设有一个用户充值日志表 A，字段如下 字段 类型 含义 dist_id int 区组id account string 账号 money int 充值金额 create_time string 订单时间 求解：统计某一天各区组下充值额最大的账号以及金额。 select a.dist_id, a.account, a.money from ( select a.dist_id, a.account, a.money, row_number() over (partition by a.dist_id order by a.money desc) rn from tab1 as a where date(a.create_time) = '2022-10-01' ) as a where a.rn = 1 ","date":"2022-11-30","objectID":"/dub-exercise/:1:5","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"观看视频 根据要求写出对应的 sql 语句 表A：用户浏览视频日志 user_behavior:date,user_id,video_id,start_time,end_time 表B：视频信息 video_info:video_id,video_duration 表C：用户信息 user_info:user_id,gender 问题： 某一天（如20200310），观看不同视频个数最多的前 5 名 user_id 观看超过 50 个不用视频的女性用户中，完整观看率最高的 10 个 user_id select a.user_id, count(distinct a.video_id) as cnt from tab_a as a where a.date = '20200310' order by 2 desc limit 5 select a.user_id, a.video_finish_cnt / a.video_cnt as finish_rate from ( select a.user_id, count(distinct a.video_id) over (partition by a.user_id) as video_cnt, count(distinct case when unix_timestamp(a.end_time) - unix_timestamp(a.start_time) \u003e= b.video_duration then a.video_id end) over (partition by a.user_id) as video_finish_cnt, row_number() over (partition by a.user_id order by a.start_time) rn from tab_a as a left join tab_b as b on a.video_id = b.video_id left join tab_c as c on a.user_id = c.user_id where a.date = '20200310' and c.gender = '女' ) as a where a.video_cnt \u003e= 50 and a.rn = 1 order by 2 desc limit 10 ","date":"2022-11-30","objectID":"/dub-exercise/:1:6","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"页面 session 时长计算 用户在浏览页面时，前端以 1 次/s 的频率上报后端，假如连续 30s 后端未收到上报，可以认为一次访问 session 已经结束，使用 hivesql/sparksql 求每次 session 的信息（只有一条记录时任务是 1s）。 示例： 表名为：page_upload_info，单次上报数据结构如下： user_id-\u003e用户id，create_ts-\u003e上报时间戳（单位：秒），假如有下数据（数据未排序）： user_id-\u003e1,create_ts-\u003e10000 user_id-\u003e1,create_ts-\u003e10010 user_id-\u003e1,create_ts-\u003e10003 user_id-\u003e1,create_ts-\u003e10070 user_id-\u003e2,create_ts-\u003e10000 user_id-\u003e2,create_ts-\u003e10003 解答：应返回 3 条结果，分别是： user_id-\u003e1,session-\u003e10s,start_ts-\u003e10000,end_ts-\u003e10010 user_id-\u003e1,session-\u003e1s,start_ts-\u003e10070,end_ts-\u003e10070 user_id-\u003e2,session-\u003e3s,start_ts-\u003e10000,end_ts-\u003e10003 select a.user_id, case when a.end_ts = a.create_ts then 1 else a.end_ts - a.create_ts end as session, a.start_ts, a.end_ts from ( select a.user_id, a.f_group, min(a.create_ts) as start_ts, max(a.create_ts) as end_ts from ( select a.user_id, a.create_ts, sum(f_flag) over (partition a.user_id order by a.create_ts desc) f_group from ( select a.user_id, a.create_ts, case when a.diff_create_ts \u003e= 30 or a.diff_create_ts is null then 1 else 0 end as f_flag from ( select a.user_id, a.create_ts, lead(a.create_ts, 1, null) over (partition by a.user_id order by a.create_ts) - a.create_ts as diff_create_ts from page_upload_info as a ) a ) a ) a group by 1,2 ) a ","date":"2022-11-30","objectID":"/dub-exercise/:1:7","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"留存，和连续活跃 1、用户活跃模型表：user_daily 以 imp_date、user_id 为主键，一个用户 1 天只出现一次，出现即表示当日登陆字段： 字段： imp_date 时间，user_id 用户id，is_new 是否新用户，0-老用户，1-新用户 2、红包领取日志：money_flow 字段： imp_date 时间，report_time 红包领取时间戳，用户id，add_money 领取金额 题目1：最近 1 个月每日未领红包用户的次日留存率和 7 日留存 题目2：最近 1 个月，每日 DAU 中，3 天连续活跃用户的占比 select a.imp_date, count(case when b.user_id is not null then a.user_id end)/count(a.user_id) as f_remain_1d_rate, count(case when c.user_id is not null then a.user_id end)/count(a.user_id) as f_remain_7d_rate from ( select a.imp_date, a.user_id from user_daily as a left join money_flow as b on a.imp_date = b.imp_date and a.user_id = b.user_id where date_diff(current_date, a.imp_date) \u003c= 30 and date_diff(current_date, b.imp_date) \u003c= 30 and b.user_id is null ) a left join user_daily b on date_diff(b.imp_date, a.imp_date) = 1 and a.user_id = b.user_id left join user_daily c on date_diff(c.imp_date, a.imp_date) = 7 and a.user_id = c.user_id group by 1 select a.imp_date, count(case when b.user_id is not null and c.user_id is not null then a.user_id end)/count(a.user_id) as f_3d_continue_rate from user_daily as a left join user_daily as b on date_diff(a.imp_date, b.imp_date) = 1 and a.user_id = b.user_id left join user_daily as c on date_diff(a.imp_date, c.imp_date) = 2 and a.user_id = c.user_id where date_diff(current_date, a.imp_date) \u003c= 30 group by 1 ","date":"2022-11-30","objectID":"/dub-exercise/:1:8","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"算法题 ","date":"2022-11-30","objectID":"/dub-exercise/:2:0","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"x的平方根 🔗 x的平方根 ","date":"2022-11-30","objectID":"/dub-exercise/:2:1","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"字符串相加 🔗 字符串相加 ","date":"2022-11-30","objectID":"/dub-exercise/:2:2","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"平方数之和 🔗 字符串相加 ","date":"2022-11-30","objectID":"/dub-exercise/:2:3","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"最长回文子串 🔗 最长回文子串 ","date":"2022-11-30","objectID":"/dub-exercise/:2:4","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"查询两个字符串的最长公共子串 🔗 查询两个字符串的最长公共子串 ","date":"2022-11-30","objectID":"/dub-exercise/:2:5","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"单词拆分 🔗 单词拆分 ","date":"2022-11-30","objectID":"/dub-exercise/:2:6","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"编程类 ","date":"2022-11-30","objectID":"/dub-exercise/:3:0","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["刷题"],"content":"统计kv 有一份数据有 n 行，每一行是一个由逗号、冒号间隔的字符串（例如 “a:10,b,20,c:30”），逗号拆分后每个都是字符串类型的 “key:value”。现需要统计每个 key 的 value 求和，可尝试用 python 或 hive/sql 或 spark 实现。 (1) 若 python 实现，则输入为一个list：lis1 = ['a:10,b:20,c:30', 'a:100,dog:200', 'hi:40,b:10', 'dog:20'] (2) 若 hive/sql 实现，则输入可设为一个表 A，且仅一列数据。 info a:10,b:20,c:30 a:100,dog:200 hi:40,b:10 dog:20 …… (3) 若 saprk 实现，则输入可设为一个 rdd/dateframe 最终求得：a: 110, b: 30, c: 30, dog: 220, hi: 40。 答： lis1 = ['a:10,b:20,c:30', 'a:100,dog:200', 'hi:40,b:10', 'dog:20'] ret_dict = {} for item in lis1: item_list = item.split(',') for i in item_list: k = i.split(':')[0] v = int(i.split(':')[1]) if k in ret_dict: ret_dict[k] += v else: ret_dict[k] = v print(ret_dict) select concat_ws(',', collect_list(concat(k,':',v))) as ret from ( select k, sum(v) as v from ( select split(value1, ':')[0] as k, cast(split(value1, ':')[1] as int) as v from tmp_2022120501 t lateral view explode(split(t.my_str, ',')) t1 as value1 ) as a group by 1 ) as a ","date":"2022-11-30","objectID":"/dub-exercise/:3:1","tags":["面试"],"title":"dub老师给滴题目准备","uri":"/dub-exercise/"},{"categories":["学习"],"content":"【数仓】事实表设计","date":"2022-11-30","objectID":"/fact-tab/","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"今天学习第 11 章事实表设计。 ","date":"2022-11-30","objectID":"/fact-tab/:0:0","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"事实表基础 ","date":"2022-11-30","objectID":"/fact-tab/:1:0","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"事实表特性 粒度：一条记录表达的业务细节称为粒度。 维度属性组合所表示的细节程度； 所表示的具体业务含义。 作为度量业务过程的事实，一般为整型或浮点型数值，类型： 可加性事实：可以在任意维度汇总； 半可加性事实：只能按照特定维度汇总，比如库存，可以按地点、商品汇总，不能按时间汇总，没意义。 不可加性事实：比如比率型。 与维度表相比，事实表细长，行增加快。 维度属性也可以存储到事实表中，称为退化维度，也可以用于对事实表过滤查询、聚合。 类型： 事务事实表：描述业务过程，保存最原子的数据 周期快照事实表：具有规律性、可预见的时间间隔记录事实 累计快照事实表：表述过程开始和结束之间的关键步骤事件，覆盖整个生命周期，具有多个日期字段，随着过程变化修改。 ","date":"2022-11-30","objectID":"/fact-tab/:1:1","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"事实表设计原则 尽可能包含所有与业务过程相关的事实 只选择与业务过程相关的事实：例如下单这个业务过程，不应包含支付金额（这是支付业务过程）。 分解不可加性事实为可加的组件。例如订单优惠率，分解为原价、优惠金额。 在选择维度和事实之前必须先声明粒度：粒度确定事实表中一行所表示业务的细节层次，粒度越细越好，从原子粒度开始。 在同一个事实表中不能有多种不同粒度的事实。 事实的单位要保持一致。 对事实的 null 值要处理。建议用零值填充。 使用退化维度提高事实表的易用性。 ","date":"2022-11-30","objectID":"/fact-tab/:1:2","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"事实表设计方法 选择业务过程及确定事实表类型。业务过程通常使用行为动词表示，例如创建订单、付款、发货、确认收货。如果选择了所有四个业务过程，还需要包含四个业务过程的累积快照事实表。 声明粒度。尽量选原子粒度。比如用户一个订单买了多种商品，每个子订单对应一种商品，那么应该选择子订单。 确定维度。比如事实表粒度为子订单，维度有买家、卖家、商品、收货人、业务类型、订单时间等。 确定事实。事实通过“过程的度量是什么“来确定。比如订单付款事务事实表，事实有支付金额、邮费、优惠金额等。 冗余维度。传统的星形模型做法是维度单独存放在维度表，通过事实表的外键获取维度。但考虑到下游的使用效率，会在事实表中冗余常用维度，便于过滤查询、控制聚合层次、排序等。比如订单付款事务事实表中冗余常用维度字段以及商品类目、卖家店铺等维度。 ","date":"2022-11-30","objectID":"/fact-tab/:1:3","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"事务事实表 ","date":"2022-11-30","objectID":"/fact-tab/:2:0","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"设计过程 定义：任何事件都可以被理解为事务。例如创建订单、付款等。 设计过程： 选择业务过程 确定粒度 确定维度 确定事实 冗余维度 设计过程 ","date":"2022-11-30","objectID":"/fact-tab/:2:1","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"单事务事实表 针对每个业务过程设计一个事实表。 ","date":"2022-11-30","objectID":"/fact-tab/:2:2","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"多事务事实表 将不同的事实放到同一个事实表中。两种实现方式： 不同业务过程使用不同事实字段存放； 不同业务过程使用相同事实字段存放，但增加一个业务过程标签。 ","date":"2022-11-30","objectID":"/fact-tab/:2:3","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"两种事实表对比 业务过程：单事务事实表一个业务过程一个事实表，多事务事实表同一个事实表反映多个业务过程。 粒度和维度：不同业务粒度相同且有相似维度时才能使用多事务事实表。 事实：单事务事实表处理事时更灵活方便。 下游业务使用：单事务事实表下游用户更容易理解。 计算存储成本：多事务事实表更优。 两种事实表对比 ","date":"2022-11-30","objectID":"/fact-tab/:2:4","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"父子事实的处理方式 例如用户下单多个商品。交易事务事实表的粒度选择子订单（一个订单一种商品）。 ","date":"2022-11-30","objectID":"/fact-tab/:2:5","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"事实的设计原则 事实完整性 事实一致性 事实可加性 ","date":"2022-11-30","objectID":"/fact-tab/:2:6","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"周期快照事实表 确定的事件间隔内对实体的度量进行抽样。 ","date":"2022-11-30","objectID":"/fact-tab/:3:0","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"特性 用快照采样状态：快照事实表以预定的间隔采样状态度量。 快照粒度：多维声明，即采样周期以及什么将被采样。 密度与稀疏性：快照事实表是稠密的，无论当天是否有业务过程发生，都会记录一行。 半可加性：不能根据时间维度获取有意义的汇总结果，但可以求平均值。 ","date":"2022-11-30","objectID":"/fact-tab/:3:1","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"主要事项 事务与快照成对设计 附加事实：可以附加上一个周期的状态度量，便于分析使用 周期到日期度量：多种统计周期，例如财年至今的下单金额、自然年至今的下单金额 ","date":"2022-11-30","objectID":"/fact-tab/:3:2","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"累积快照事实表 用来统计下单到支付的时长、支付到发货的时长、下单到确认收货的时长等。 ","date":"2022-11-30","objectID":"/fact-tab/:4:0","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"设计过程 同事务事实表一样：选择业务过程、确定粒度、确定维度、确定事实、退化维度。 ","date":"2022-11-30","objectID":"/fact-tab/:4:1","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"特点 数据不断更新 多业务过程日期 ","date":"2022-11-30","objectID":"/fact-tab/:4:2","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"特殊处理 非线性过程：例如下单→支付→发货→确认收货，不一定都是这样的过程，可能是下单→关闭订单。 多源过程：来自不同的系统或表。 业务过程取舍：简化一些业务过程。 ","date":"2022-11-30","objectID":"/fact-tab/:4:3","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"物理实现 全量表。一般是日期分区表 全量表的变化形式。用于事实表数据量大时。比如交易订单设置 200 天作为订单产生到消亡的最大间隔，设计最近 200 天的订单累积快照事实表。 以业务实体的结束时间分区。每天分区是存放当天结束的数据，未结束的放在一个极大的时间下面比如 3000-12-31。 ","date":"2022-11-30","objectID":"/fact-tab/:4:4","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"三种事实表的比较 三种事实表的比较 ","date":"2022-11-30","objectID":"/fact-tab/:5:0","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"无事实的事实表 没有明确事实，但可以用来支持业务过程的度量。例如： 记录事件发生：用户浏览了页面，每次浏览事实为 1； 条件、资格、范围类，例如促销范围。 ","date":"2022-11-30","objectID":"/fact-tab/:6:0","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"聚集型事实表 把频繁使用的公用数据聚集汇总进行沉淀，被称为“公共汇总层“。例如卖家近 N 天的交易汇总表。 ","date":"2022-11-30","objectID":"/fact-tab/:7:0","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"聚集的基本原则 一致性 避免单一表设计 聚集粒度可不同 ","date":"2022-11-30","objectID":"/fact-tab/:7:1","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["学习"],"content":"聚集的基本步骤 确定聚集维度 确定一致性上钻 确定聚集事实 ","date":"2022-11-30","objectID":"/fact-tab/:7:2","tags":["数仓"],"title":"【数仓】事实表设计","uri":"/fact-tab/"},{"categories":["刷题"],"content":"力扣-两数之和","date":"2022-11-29","objectID":"/two-sum/","tags":["力扣"],"title":"力扣-两数之和","uri":"/two-sum/"},{"categories":["刷题"],"content":"🔗 题目链接 给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出和为目标值 target 的那两个整数，并返回它们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。 你可以按任意顺序返回答案。 示例 1： 输入：nums = [2,7,11,15], target = 9 输出：[0,1] 解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。 示例 2： 输入：nums = [3,2,4], target = 6 输出：[1,2] 示例 2： 输入：nums = [3,3], target = 6 输出：[0,1] 参考链接：两数之和1 ","date":"2022-11-29","objectID":"/two-sum/:0:0","tags":["力扣"],"title":"力扣-两数之和","uri":"/two-sum/"},{"categories":["刷题"],"content":"方法一：暴力枚举 思路及算法 最容易想到的方法是枚举数组中的每一个数 x，寻找数组中是否存在 target - x。 当我们使用遍历整个数组的方式寻找 target - x 时，需要注意到每一个位于 x 之前的元素都已经和 x 匹配过，因此不需要再进行匹配。而每一个元素不能被使用两次，所以我们只需要在 x 后面的元素中寻找 target - x。 代码 def twoSum(nums: List[int], target: int) -\u003e List[int]: n = len(nums) for i in range(n): for j in range(i + 1, n): if nums[i] + nums[j] == target: return [i, j] return [] 复杂度分析 时间复杂度：$O(N^2)$，其中 $N$ 是数组中的元素数量。最坏情况下数组中任意两个数都要被匹配一次。 空间复杂度：$O(1)$。 ","date":"2022-11-29","objectID":"/two-sum/:1:0","tags":["力扣"],"title":"力扣-两数之和","uri":"/two-sum/"},{"categories":["刷题"],"content":"方法二：哈希表 思路及算法 注意到方法一的时间复杂度较高的原因是寻找 target - x 的时间复杂度过高。因此，我们需要一种更优秀的方法，能够快速寻找数组中是否存在目标元素。如果存在，我们需要找出它的索引。 使用哈希表，可以将寻找 target - x 的时间复杂度降低到从 $O(N)$ 降低到 $O(1)$。 这样我们创建一个哈希表，对于每一个 x，我们首先查询哈希表中是否存在 target - x，然后将 x 插入到哈希表中，即可保证不会让 x 和自己匹配。 代码 def twoSum(nums: List[int], target: int) -\u003e List[int]: hashtable = dict() for i, num in enumerate(nums): if target - num in hashtable: return [hashtable[target - num], i] hashtable[nums[i]] = i return [] 复杂度分析 时间复杂度：$O(N)$，其中 $N$ 是数组中的元素数量。对于每一个元素 x，我们可以 $O(1)$ 地寻找 target - x。 空间复杂度：$O(N)$，其中 $N$ 是数组中的元素数量。主要为哈希表的开销。 参考链接：两数之和 ↩︎ ","date":"2022-11-29","objectID":"/two-sum/:2:0","tags":["力扣"],"title":"力扣-两数之和","uri":"/two-sum/"},{"categories":["刷题"],"content":"力扣-数组中重复的数据","date":"2022-11-29","objectID":"/duplicates/","tags":["力扣"],"title":"力扣-数组中重复的数据","uri":"/duplicates/"},{"categories":["刷题"],"content":"🔗 题目链接 给你一个长度为 n 的整数数组 nums ，其中 nums 的所有整数都在范围 [1, n] 内，且每个整数出现一次或两次。请你找出所有出现两次的整数，并以数组形式返回。 你必须设计并实现一个时间复杂度为 O(n) 且仅使用常量额外空间的算法解决此问题。 示例 1： 输入：nums = [4,3,2,7,8,2,3,1] 输出：[2,3] 示例 2： 输入：nums = [1,1,2] 输出：[1] 示例 3： 输入：nums = [1] 输出：[] 参考链接：数组中重复的数据1 ","date":"2022-11-29","objectID":"/duplicates/:0:0","tags":["力扣"],"title":"力扣-数组中重复的数据","uri":"/duplicates/"},{"categories":["刷题"],"content":"方法一：将元素交换到对应的位置 思路与算法 由于给定的 $n$ 个数都在 $[1,n]$ 的范围内，如果有数字出现了两次，就意味着 $[1,n]$ 中有数字没有出现过。 因此，我们可以尝试将每一个数放在对应的位置。由于数组的下标范围是 $[0,n−1]$，我们需要将数 $i$ 放在数组中下标为 $i−1$ 的位置： 如果 $i$ 恰好出现了一次，那么将 $i$ 放在数组中下标为 $i−1$ 的位置即可； 如果 $i$ 出现了两次，那么我们希望其中的一个 $i$ 放在数组下标中为 $i−1$ 的位置，另一个 $i$ 放置在任意「不冲突」的位置 $j$。也就是说，数 $j+1$ 没有在数组中出现过。 这样一来，如果我们按照上述的规则放置每一个数，那么我们只需要对数组进行一次遍历。当遍历到位置 $i$ 时，如果 $nums[i]−1 \\neq i$，说明 $nums[i]$ 出现了两次（另一次出现在位置 $num[i]−1$），我们就可以将 $num[i]$ 放入答案。 放置的方法也很直观：我们对数组进行一次遍历。当遍历到位置 $i$ 时，我们知道 $nums[i]$ 应该被放在位置 $nums[i]−1$。因此我们交换 $num[i]$ 和 $nums[nums[i]−1]$ 即可，直到待交换的两个元素相等为止。 代码 def findDuplicates(nums: List[int]) -\u003e List[int]: for i in range(len(nums)): while nums[i] != nums[nums[i] - 1]: nums[nums[i] - 1], nums[i] = nums[i], nums[nums[i] - 1] return [num for i, num in enumerate(nums) if num - 1 != i] 复杂度分析 时间复杂度：$O(n)$。每一次交换操作会使得至少一个元素被交换到对应的正确位置，因此交换的次数为 $O(n)$，总时间复杂度为 $O(n)$。 空间复杂度：$O(1)$。返回值不计入空间复杂度。 ","date":"2022-11-29","objectID":"/duplicates/:1:0","tags":["力扣"],"title":"力扣-数组中重复的数据","uri":"/duplicates/"},{"categories":["刷题"],"content":"方法二：使用正负号作为标记 思路与算法 我们也可以给 $nums[i]$ 加上「负号」表示数 $i+1$ 已经出现过一次。具体地，我们首先对数组进行一次遍历。当遍历到位置 $i$ 时，我们考虑 $nums[nums[i]−1]$ 的正负性： 如果 $nums[nums[i]−1]$ 是正数，说明 $nums[i]$ 还没有出现过，我们将 $nums[nums[i]−1]$ 加上负号； 如果 $nums[nums[i]−1]$ 是负数，说明 $nums[i]$ 已经出现过一次，我们将 $nums[i]$ 放入答案。 细节 由于 $nums[i]$ 本身可能已经为负数，因此在将 $nums[i]$ 作为下标或者放入答案时，需要取绝对值。 代码 def findDuplicates(nums: List[int]) -\u003e List[int]: ans = [] for x in nums: x = abs(x) if nums[x - 1] \u003e 0: nums[x - 1] = -nums[x - 1] else: ans.append(x) return ans 复杂度分析 时间复杂度：$O(n)$。我们只需要对数组 $nums$ 进行一次遍历。 空间复杂度：$O(1)$。返回值不计入空间复杂度。 参考链接：数组中重复的数据 ↩︎ ","date":"2022-11-29","objectID":"/duplicates/:2:0","tags":["力扣"],"title":"力扣-数组中重复的数据","uri":"/duplicates/"},{"categories":["学习"],"content":"Inmon 范式建模和 Kimball 维度建模","date":"2022-11-29","objectID":"/kimball/","tags":["维度建模"],"title":"Inmon 范式建模和 Kimball 维度建模","uri":"/kimball/"},{"categories":["学习"],"content":"本文开始先简单理解两种建模的核心思想，然后根据一个具体的例子，分别使用这两种建模方式进行建模，大家便会一目了然！ 😏。 Inmon：《数据仓库》 Kimball：《数仓工具箱》 范式建模 自上向下(数据的流向)，“上\"即数据的上游，“下\"即数据的下游 分散异构的数据源—\u003e数据仓库—\u003e数据集市 以数据源头为导向 将数据抽取为实体-关系模型 并不强调事实表和维度表 维度建模 自下向上，数据集市—\u003e数据仓库—\u003e分散异构的数据源 以最终任务为导向 按照目标拆分需求，抽取为事实-维度模型，数据源经过ETL转化为事实表和维度表导入数据集市 以星型模型或雪花模型构建维度数据仓库 数据集市是数据仓库中的一个逻辑上的主题域 建模实践 以电商系统为例，有4张表，分别是： 用户信息表 城市信息表 用户等级表 用户订单表 ","date":"2022-11-29","objectID":"/kimball/:0:0","tags":["维度建模"],"title":"Inmon 范式建模和 Kimball 维度建模","uri":"/kimball/"},{"categories":["学习"],"content":"Inmon建模 将数据抽取为实体-关系(ER)模型 用户实体表(ETL已过滤掉注销用户) 支付成功订单实体表 城市信息实体表 订单与用户关系表 用户与城市信息关系表 用户与用户等级关系表 数据没有冗余，符合三范式设计规范 ","date":"2022-11-29","objectID":"/kimball/:1:0","tags":["维度建模"],"title":"Inmon 范式建模和 Kimball 维度建模","uri":"/kimball/"},{"categories":["学习"],"content":"Kimball建模 将数据抽取为事实表和维度表 在 Kimball 维度建模中，不需要单独维护数据关系表，因为关系已经冗余在事实表和维度表中 支付成功订单事实表 用户维度表 城市信息维度表 用户等级维度表 以上是雪花模型，特点是维度表可以拥有其他维度表 ","date":"2022-11-29","objectID":"/kimball/:2:0","tags":["维度建模"],"title":"Inmon 范式建模和 Kimball 维度建模","uri":"/kimball/"},{"categories":["学习"],"content":"范式建模 优点： 没有数据冗余，保证了数据一致性 数据解耦，方便维护 缺点： 表的数量多 查询时多表联查降低查询性能 ","date":"2022-11-29","objectID":"/kimball/:3:0","tags":["维度建模"],"title":"Inmon 范式建模和 Kimball 维度建模","uri":"/kimball/"},{"categories":["学习"],"content":"维度建模 优点： 模型简单，面向分析 增加数据冗余，开发周期短，能快速迭代 缺点： 数据大量冗余，预处理阶段开销大 后期维护麻烦 维度设计不好的话不能保证数据口径一致性 ☺️ 维度就是看待问题的角度，量度就是事实表中数据类型的字段 如果把省份当作一个单独维度，城市当作一个单独维度，计算城市的人口数量。 这时省份和城市都是单独的维度，它们之间没有了关系，会出现： 广东省 杭州市 1500 浙江省 广州市 1200 在范式建模中，强调实体-关系模型，省份和城市之间一定存在归属关系，所以范式建模能保证口径的一致性，而维度建模不能！ ","date":"2022-11-29","objectID":"/kimball/:4:0","tags":["维度建模"],"title":"Inmon 范式建模和 Kimball 维度建模","uri":"/kimball/"},{"categories":["学习"],"content":"建模方式对比 特性 Kimball Inmon 开发周期 短 长 维护难度 大 小 数据要求 针对具体业务 站在企业角度 缓慢变化维 是 否 数据模型 维度建模、星型模型、雪花模型 实体-关系模型，准三范式设计 ","date":"2022-11-29","objectID":"/kimball/:5:0","tags":["维度建模"],"title":"Inmon 范式建模和 Kimball 维度建模","uri":"/kimball/"},{"categories":["学习"],"content":"三范式","date":"2022-11-29","objectID":"/3nf/","tags":["面试","三范式"],"title":"三范式","uri":"/3nf/"},{"categories":["学习"],"content":"三范式都有哪些？举例说明具体含义 😏。 第一范式：原子性(不可分割) 第二范式：消除部分依赖 第三范式：消除传递依赖 举例：存在一张学生关系模型表，按照三范式去改进该模型，如下： 三范式 根据第一范式：字段 contact 联系方式还可划分为邮箱和联系电话，改进如下： 三范式 根据第二范式：当需要录入学生张三另一课程 856 的新成绩，除了把学生号 stu_id，课程号 cou_no，成绩 grade，还需填张三的其他基本信息，你觉得该工作十分冗余。 你思考发现：成绩 grade 依赖于学生号 stu_id 和课程号 cou_no 才能唯一确定，而张三基本信息只依赖于学生号 stu_id，于是你把该表分拆为两个表，改进如下： 三范式 通过消除部分依赖，你只需要录入学生号 stu_id，课程号 cou_no，成绩 grade 即可。 根据第三范式：又在录入同一个系的学生成绩时，发现系主任都是一样，即：学生号 stu_id 确定则系 stu_dept 确定，系 stu_dept 则系主任 dean 确定。你思考发现：系主任 dean 传递依赖于学生号 stu_id，再次改进了关系模型，分拆为三个表描述，如下： 三范式 于是又减少了录入李主任的工作。 ","date":"2022-11-29","objectID":"/3nf/:0:0","tags":["面试","三范式"],"title":"三范式","uri":"/3nf/"},{"categories":["刷题"],"content":"学习之联通沃音乐","date":"2022-11-16","objectID":"/study-of-liantong/","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"本文主要摘联通沃音乐的一些题目，以便后续查阅复习 😋。 ","date":"2022-11-16","objectID":"/study-of-liantong/:0:0","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"shell 题目 ","date":"2022-11-16","objectID":"/study-of-liantong/:1:0","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 1 题 如何把文件 a.txt 赋权为755，另外解释 755 是什么权限？ 答案 chmod 755 a.txt 具体的权限是由数字来表示的 读取的权限等于 4，用 r 表示； 写入的权限等于 2，用 w 表示； 执行的权限等于 1，用 x 表示； 以 755 为例： 1-3 位 7 等于 4+2+1，rwx，文件所有者具有读取、写入、执行权限； 4-6 位 5 等于 4+1+0，r-x，同组用户具有读取、执行权限但没有写入权限； 7-9 位 5，同上，也是 r-x，其他用户具有读取、执行权限但没有写入权限。 ","date":"2022-11-16","objectID":"/study-of-liantong/:1:1","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 2 题 如何把文件 a.txt 重命名为 b.txt 答案 mv a.txt b.txt ","date":"2022-11-16","objectID":"/study-of-liantong/:1:2","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 3 题 如何查询包含 a.sh 的所有进程，并进行批量杀进程？ 答案 使用 awk 批量杀进程的命令 ps -ef | grep a.sh | grep -v grep | awk '{print \"kill -9 \"$2}'|sh 说明： #列出了当前主机中运行的进程中包含a.sh关键字的进程 ps -ef | grep a.sh | grep -v grep #列出了要kill掉这些进程的命令，并将之打印在了屏幕上 ps -ef | grep a.sh | grep -v grep | awk '{print \"kill -9 \"$2}' #后面加上|sh后，则执行这些命令，进而杀掉了这些进程 ps -ef | grep a.sh | grep -v grep | awk '{print \"kill -9 \"$2}' | sh ","date":"2022-11-16","objectID":"/study-of-liantong/:1:3","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 4 题 请列举 vi 的 6 种命名及其用途 i: 在光标前插内内容 x: 删除光标后面的字符 yy: 复制整行 :w 保存 :q 退出 :q! 强制退出 :w! 强制保存 ","date":"2022-11-16","objectID":"/study-of-liantong/:1:4","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 5 题 文件 a.txt 只有一列数据，请进行排序剔重得到 b.txt？ a.txt b.txt 12 12 45 35 48 =\u003e 45 35 48 85 85 45 35 答案 sort -u a.txt \u003e b.txt 默认是升序，降序的话加上 -r ","date":"2022-11-16","objectID":"/study-of-liantong/:1:5","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 6 题 请写一个 shell 循环得到 1 到 100 的相加之和。 答案 vim sum1_100.sh #!/bin/bash sum = 0 for i in {1..100} do let sum = sum + i done $echo $sum chmod +x sum1_100.sh sh sum1_100.sh 或者 ./sum1_100.sh ","date":"2022-11-16","objectID":"/study-of-liantong/:1:6","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"数据库题 ","date":"2022-11-16","objectID":"/study-of-liantong/:2:0","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 7 题 以表 a 作为主表，外连接表 b，得出表 c。 数据表 答案 select a.id, a.Score1, coalesce(b.Score2, '') as Score2 from tab1 a left join tab2 b on a.id = b.id ","date":"2022-11-16","objectID":"/study-of-liantong/:2:1","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 8 题 参加一个活动，表 a 记录了活动的年龄及该年龄的人数，现需要求出表 b 年龄分段的人数。 数据表 答案 select case when age \u003e= 0 and age \u003c 10 then '[0,10)' when age \u003e= 10 and age \u003c 20 then '[10,20)' when age \u003e= 20 and age \u003c 30 then '[20,30)' when age \u003e= 30 then '大于30' end as Age, sum(cnt) as Cnt from tab1 a group by 1 ","date":"2022-11-16","objectID":"/study-of-liantong/:2:2","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 9 题 表 a 有学生 id 及其成绩，现做排名得出表 b 数据表 答案 select a.id, a.Score, rank() over (order by Score desc) pm from tab1 a order by id ","date":"2022-11-16","objectID":"/study-of-liantong/:2:3","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 10 题 每天 1000 万条的数据（有时间（具体到秒），字段2，字段3，字段4……）从前端写入数据库，如何设置表结构，可以快速的查询到 2017年3月11日20点31分到32分的所有数据？ 答案 思路是把时间进行拆解，按照天+小时+分钟的格式，然后对时间字段加上索引，这样查询的速度就会提升。 CREATE TABLE `tab1` ( `f_id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增id', `f_date` varchar(64) NOT NULL DEFAULT '' COMMENT '日期', `f_hour` varchar(64) NOT NULL DEFAULT '' COMMENT '小时', `f_min` varchar(64) NOT NULL DEFAULT '' COMMENT '分钟', 字段2，字段3.... `f_update_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY ( `f_id` ), KEY `all_index` (`f_date`, `f_hour`, `f_min`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8 COMMENT = '注释' ","date":"2022-11-16","objectID":"/study-of-liantong/:2:4","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 11 题 如下图：表 a 是营业厅，所属区，地市，省销售金额，现得出表 b 区、地市和全省汇总 数据表 答案 select City, Area, sum(amt) as amt from tab1 group by 1,2 union all select '全省' as City, '' as Area, sum(amt) as amt from tab1 group by 1,2 union all select City as City, '全市' as Area, sum(amt) as amt from tab1 group by 1,2 ","date":"2022-11-16","objectID":"/study-of-liantong/:2:5","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 12 题 表 a 字段是以日期作为分区表，有日期、用户id、产品id，使用的时间详细时间，每天有 50 亿条数据，即 31 天总数据为 1550 亿。需要每天得到一个表 b，以 b 表的日期、产品id为联合汇总，算出 1 号到当前的日期的 pv 和 uv。但 Hadoop 集群只能计算 100 亿数据，如何通过逻辑方式计算请写出具体 sql。答案需要考虑集群的计算能力，提示每天新增的用户大概 3 千万。 数据表 答案 insert overwrite table tab_with_pv partition(static_dat='计算日') select static_dat, product_id, count(user_id) pv from tab1 where static_dat = '计算日' group by 1,2 pv 的计算 select static_dat, product_id, sum(pv) pv from tab_with_pv where substr(static_dat,1,6) = '计算月' and static_dat \u003c= '计算日' group by 1,2 tab_with_user 这个表需要加上自依赖 insert overwrite table tab_with_user partition(static_dat='计算日') select distinct static_dat, product_id, user_id from tab_with_user where static_dat = '计算日' union select distinct static_dat, product_id, user_id from tab_with_user where static_dat = '计算日前一日' and substr(static_dat,1,6) = '计算月' uv 的计算 select static_dat, product_id, count(distinct user_id) uv from tab_with_user where static_dat = '计算日' 最后把 pv 和 uv 的数据拼接在一起即可。 ","date":"2022-11-16","objectID":"/study-of-liantong/:2:6","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"JAVA题目 ","date":"2022-11-16","objectID":"/study-of-liantong/:3:0","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 13 题 用代码或伪代码实现一个 java 限流工具类（说明：调用关系为我方系统调用对方系统，我方系统调用频次不能大于对方系统的限制，如：阿里云限制我方每秒调用不能大于1000次）。 答案 public class RateLimiterSimpleWindow { // 阈值 private static Integer QPS = 2; // 时间窗口（毫秒） private static long TIME_WINDOWS = 1000; // 计数器 private static AtomicInteger REQ_COUNT = new AtomicInteger(); private static long START_TIME = System.currentTimeMillis(); public synchronized static boolean tryAcquire() { if ((System.currentTimeMillis() - START_TIME) \u003e TIME_WINDOWS) { REQ_COUNT.set(0); START_TIME = System.currentTimeMillis(); } return REQ_COUNT.incrementAndGet() \u003c= QPS; } public static void main(String[] args) throws InterruptedException { for (int i = 0; i \u003c 10; i++) { Thread.sleep(250); LocalTime now = LocalTime.now(); if (!tryAcquire()) { System.out.println(now + \" 被限流\"); } else { System.out.println(now + \" 做点什么\"); } } ","date":"2022-11-16","objectID":"/study-of-liantong/:3:1","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 14 题 假设在 5 台 linux 服务器上我们部署了 5 个应用实例（外部访问默认通过 ngix 负载），该应用涉及到文件的上传和下载，你会如何设计，保证用户上传文件和下载文件时不会出现异常问题，同时也需要保证存储数据访问的安全性（假设不能使用对象存储服务）。 ","date":"2022-11-16","objectID":"/study-of-liantong/:3:2","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"Python题目 ","date":"2022-11-16","objectID":"/study-of-liantong/:4:0","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["刷题"],"content":"第 15 题 现有一个 dataframe 对象 df，如下： name age sex score zhangsan 10 50 lisi 女 60 12 男 90 chenyi 11 女 chenhai 9 64 请分别写出相关代码： （1）将性别 sex 为空的使用男来替代；将 name 为空的记录删除；将 age、score 列为空的分别使用改列平均值替代。 df[\"sex\"].fillna(\"男\", inplace = True) # 将性别 sex 为空的使用男来替代 df[\"name\"].dropna() # 将 name 为空的记录删除 age_mean = df[\"age\"].mean() score_mean = df[\"score\"].mean() df[\"age\"].fillna(age_mean, inplace = True) df[\"score\"].fillna(score_mean, inplace = True) （2）完成 2.1 数据后，对性别进行分组汇总，得出每个性别的 score 汇总得分。 df.groupby(\"sex\").agg({\"score\":\"sum\"}) （3）找出分数大于平均分的记录，并输出 name 和 score df[df[\"score\"] \u003e df[\"score\"].mean()][[\"name\",\"score\"]] ","date":"2022-11-16","objectID":"/study-of-liantong/:4:1","tags":["面试"],"title":"学习之联通沃音乐","uri":"/study-of-liantong/"},{"categories":["学习"],"content":"学习之基础知识篇","date":"2022-11-08","objectID":"/study-of-interview/","tags":["面试","基础"],"title":"学习之基础知识篇","uri":"/study-of-interview/"},{"categories":["学习"],"content":"本文主要摘抄一些面试过程可能会遇到的一些基础知识问题，以便后续查阅复习。 ","date":"2022-11-08","objectID":"/study-of-interview/:0:0","tags":["面试","基础"],"title":"学习之基础知识篇","uri":"/study-of-interview/"},{"categories":["学习"],"content":"Linux操作系统常用命令 使用 shell 命令查看路径下的文件大小和属性 答案 ls -lrth 使用s hell 命令找出路径下带有关键字 “test” 的 go 语言文件 答案 find . -name \"*.go\"|xargs grep \"test\" 查看服务器的进程资源使用情况 答案 top 使用 shell 命令查看服务监听 80 端口的进程 id 答案 netstat -anp|grep EST|grep 80 ","date":"2022-11-08","objectID":"/study-of-interview/:1:0","tags":["面试","基础"],"title":"学习之基础知识篇","uri":"/study-of-interview/"},{"categories":["学习"],"content":"数仓之SQL篇","date":"2022-11-07","objectID":"/study-of-sql/","tags":["数仓","SQL"],"title":"数仓之SQL篇","uri":"/study-of-sql/"},{"categories":["学习"],"content":"本文主要摘抄数仓中的一些 SQL 问题，包括一些常用的语法和练习题，以便后续查阅复习。 ","date":"2022-11-07","objectID":"/study-of-sql/:0:0","tags":["数仓","SQL"],"title":"数仓之SQL篇","uri":"/study-of-sql/"},{"categories":["学习"],"content":"平均活跃天数和月活人数 用户在牛客试卷作答区作答记录存储在表 exam_record 中，内容如下： exam_record表（uid 用户 ID, exam_id 试卷 ID, start_time 开始作答时间, submit_time 交卷时间, score 得分） id uid exam_id start_time submit_time score 1 1001 9001 2021-07-02 09:01:01 2021-07-02 09:21:01 80 2 1002 9001 2021-09-05 19:01:01 2021-09-05 19:40:01 81 3 1002 9002 2021-09-02 12:01:01 (NULL) (NULL) 4 1002 9003 2021-09-01 12:01:01 (NULL) (NULL) 5 1002 9001 2021-07-02 19:01:01 2021-07-02 19:30:01 82 6 1002 9002 2021-07-05 18:01:01 2021-07-05 18:59:02 90 7 1003 9002 2021-07-06 12:01:01 (NULL) (NULL) 8 1003 9003 2021-09-07 10:01:01 2021-09-07 10:31:01 86 9 1004 9003 2021-09-06 12:01:01 (NULL) (NULL) 10 1002 9003 2021-09-01 12:01:01 2021-09-01 12:31:01 81 11 1005 9001 2021-09-01 12:01:01 2021-09-01 12:31:01 88 12 1006 9002 2021-09-02 12:11:01 2021-09-02 12:31:01 89 13 1007 9002 2020-09-02 12:11:01 2020-09-02 12:31:01 89 请计算 2021 年每个月里试卷作答区用户平均月活跃天数 avg_active_days 和月度活跃人数 mau，上面数据的示例输出如下： month avg_active_days mau 202107 1.50 2 202109 1.25 4 思路 对于用户平均活跃天数，则需要得到用户的总活跃天数和不同的用户数。不同的用户数和月活的求解方式一样，总活跃天数则需要同时读用户和日期进行去重，因为如果同一天有多个用户活跃，则最终的总活跃天数中是会计算多天的。 select date_format(submit_time,'%Y%m') month, round(count(distinct uid,date_format(submit_time,'%Y%m%d'))/count(distinct uid),2) avg_active_days, count(distinct uid) mau from exam_record where year(submit_time)=2021 group by month; ","date":"2022-11-07","objectID":"/study-of-sql/:1:0","tags":["数仓","SQL"],"title":"数仓之SQL篇","uri":"/study-of-sql/"},{"categories":["学习"],"content":"每天的日活数及新用户占比 用户行为日志表 tb_user_log（uid-用户ID, artical_id-文章ID, in_time-进入时间, out_time-离开时间, sign_in-是否签到） id uid artical_id in_time out_time sign_cin 1 101 9001 2021-10-31 10:00:00 2021-10-31 10:00:09 0 2 102 9001 2021-10-31 10:00:00 2021-10-31 10:00:09 0 3 101 0 2021-11-01 10:00:00 2021-11-01 10:00:42 1 4 102 9001 2021-11-01 10:00:00 2021-11-01 10:00:09 0 5 108 9001 2021-11-01 10:00:01 2021-11-01 10:00:50 0 6 108 9001 2021-11-02 10:00:01 2021-11-02 10:00:50 0 7 104 9001 2021-11-02 10:00:28 2021-11-02 10:00:50 0 8 106 9001 2021-11-02 10:00:28 2021-11-02 10:00:50 0 9 108 9001 2021-11-03 10:00:01 2021-11-03 10:00:50 0 10 109 9002 2021-11-03 11:00:55 2021-11-03 11:00:59 0 11 104 9003 2021-11-03 11:00:45 2021-11-03 11:00:55 0 12 105 9003 2021-11-03 11:00:53 2021-11-03 11:00:59 0 13 106 9003 2021-11-03 11:00:45 2021-11-03 11:00:55 0 问题：统计每天的日活数及新用户占比 注： 新用户占比 = 当天的新用户数 ÷ 当天活跃用户数（日活数）。 如果in_time-进入时间和out_time-离开时间跨天了，在两天里都记为该用户活跃过。 新用户占比保留2位小数，结果按日期升序排序。 输出示例： 示例数据的输出结果如下 dt dau uv_new_ratio 2021-10-30 2 1.00 2021-11-01 3 0.33 2021-11-02 3 0.67 2021-11-03 5 0.40 思路1 日活就是每天访问的不同用户数，所以我们首先要得到一张登录表，登录表记录了每天登录的用户，并按天对用户进行了去重，也就是下面的 t1。而要统计新用户的占比，我们就需要识别每天登录用户中哪些用户是新用户（即第一次登录）。一个可行的思路是，使用窗口函数对每个用户的登录日期进行排序得到下面的 t2。统计的时候进行判断，如果统计当天该用户的序号为 1，则表示用户今天是第一次登录，即为新用户。 with t1 as ( # 用户登录表，记录了用户 id 和登录时间，对每天的登录用户进行了去重 select uid, date(in_time) dt from tb_user_log union # union 实现去重，union all 不去重 select uid, date(out_time) dt from tb_user_log ), t2 as ( # 对每个用户的登录日期进行排序，注册日期的序号是 1 select uid,dt, row_number() over(partition by uid order by dt) rn from t1 ) # 获得答案 select dt, count(uid) dau, round(sum(if(rn=1, 1, 0))/count(uid),2) uv_new_ration from t2 group by dt order by dt; 思路2 同样的思路得到用户登录表 t1。每个用户的注册日期肯定在登录表中是最小的，因此用 min 函数可以得到用户登录表即下面的 t2。最后在求解答案的时候，用 t1 left join t2，关联的字段是 uid 以及日期，由于使用了 left join，t1 中每个用户所有的登录日期都得到了保留，count 计数即可得到 dau，而 t2 表中不是当天注册的用户 uid 和 reg_dt 都为 null，同样使用 count 计数就能得到当天的新用户。 with t1 as ( # 用户登录表，记录了用户 id 和登录时间，对每天的登录用户进行了去重 select uid, date(in_time) dt from tb_user_log union select uid, date(out_time) dt from tb_user_log ), t2 as ( # 得到用户注册表 select uid, min(dt) reg_dt from t1 group by uid ) select dt, count(t1.uid) dau, round(count(t2.uid)/count(t1.uid),2) uv_new_ration from t1 left join t2 on t1.uid = t2.uid and t1.dt = t2.reg_dt group by dt order by dt; ","date":"2022-11-07","objectID":"/study-of-sql/:2:0","tags":["数仓","SQL"],"title":"数仓之SQL篇","uri":"/study-of-sql/"},{"categories":["学习"],"content":"用户留存分析 n日留存率 = 第 n 天还在登录的用户数/新增的用户数，假如某日新增了 100 个用户，第二天登录了 50 个，则次日留存率为 50/100 = 50%，第三天登录了 30 个，则第二日留存率为 30/100 = 30%，以此类推，第 7 天登录了 10 个用户，则 7 日留存率就是 10/100 = 10%。 首先，我们需要计算出每个user的首次登录日期，也就是下面的代码。 SELECT user_id, MIN(log_date) AS first_log_date FROM user_log GROUP BY user_id 然后，我们和 user_log 表进行关联之后再进行日期差的计算，这就得到了某一天登录离第一次登录有多长时间。 --用户留存计算 SELECT first_log_date, SUM(CASE WHEN t3.diff_days = 0 THEN 1 ELSE 0 END) AS day_0, SUM(CASE WHEN t3.diff_days = 1 THEN 1 ELSE 0 END) AS day_1, SUM(CASE WHEN t3.diff_days = 2 THEN 1 ELSE 0 END) AS day_2, SUM(CASE WHEN t3.diff_days = 3 THEN 1 ELSE 0 END) AS day_3, SUM(CASE WHEN t3.diff_days = 4 THEN 1 ELSE 0 END) AS day_4, SUM(CASE WHEN t3.diff_days = 5 THEN 1 ELSE 0 END) AS day_5, SUM(CASE WHEN t3.diff_days = 6 THEN 1 ELSE 0 END) AS day_6, SUM(CASE WHEN t3.diff_days = 7 THEN 1 ELSE 0 END) AS day_7 FROM ( SELECT t1.user_id, t2.first_log_date, DATEDIFF(DAY, t2.first_log_date, t1.log_date) AS diff_days FROM user_log AS t1 LEFT JOIN ( SELECT user_id, MIN(log_date) AS first_log_date FROM user_log GROUP BY user_id ) as t2 ON t1.user_id = t2.user_id ) AS t3 GROUP BY first_log_date ","date":"2022-11-07","objectID":"/study-of-sql/:3:0","tags":["数仓","SQL"],"title":"数仓之SQL篇","uri":"/study-of-sql/"},{"categories":["学习"],"content":"数仓之学无止境","date":"2022-11-03","objectID":"/study-of-all-life/","tags":["数仓"],"title":"数仓之学无止境","uri":"/study-of-all-life/"},{"categories":["学习"],"content":"本文主要摘抄数仓面试中的一些常见问题，以便后续查阅复习。 ","date":"2022-11-03","objectID":"/study-of-all-life/:0:0","tags":["数仓"],"title":"数仓之学无止境","uri":"/study-of-all-life/"},{"categories":["学习"],"content":"数仓是什么？ 数据仓库是一个面向主题的、集成的、相对稳定的、反映历史变化的数据集合，用于支持管理决策。 ","date":"2022-11-03","objectID":"/study-of-all-life/:1:0","tags":["数仓"],"title":"数仓之学无止境","uri":"/study-of-all-life/"},{"categories":["学习"],"content":"Hive 是什么？ 是基于 Hadoop 的一个数据仓库工具； 可以将结构化的数据映射为一张数据库表； 并提供 HQL(Hive SQL) 查询功能； 底层数据是存储在 HDFS 上； Hive 的本质是将 SQL 语句转换为 MapReduce、Tez 或者 spark 等任务执行； 适用于离线的批量数据计算。 ","date":"2022-11-03","objectID":"/study-of-all-life/:2:0","tags":["数仓"],"title":"数仓之学无止境","uri":"/study-of-all-life/"},{"categories":["学习"],"content":"ETL 是哪三个单词的缩写 Extraction 提取 Transformation 转换 Loading 加载 ","date":"2022-11-03","objectID":"/study-of-all-life/:3:0","tags":["数仓"],"title":"数仓之学无止境","uri":"/study-of-all-life/"},{"categories":["学习"],"content":"知道笛卡尔积吗？ 根据笛卡尔积的定义，JOIN 的两表中的任意一行都会形成一组关系对，如果 A 表有 N 条记录，B 表有 M 条记录，A X B 会生产 N*M 条数据。 1 可以通过 CROSS JOIN 来实现笛卡尔积 select * from A cross join b 如果不支持 CROSS JOIN 的情况下，可以采用 select * from A join B on 1 = 1 的方式实现。 第二种方法可能在语法检测阶段就报错不支持，可以转成如下语法 select * from (select * , '1' as flag from A) t1 on (select *, '1' as flag from B) t2 where t1.flag = t2.flag ","date":"2022-11-03","objectID":"/study-of-all-life/:4:0","tags":["数仓"],"title":"数仓之学无止境","uri":"/study-of-all-life/"},{"categories":["学习"],"content":"shuffle 用来干什么？ 为了让相同的 key 都到一个 reduce 中进行处理，reduce 要去每个 map 中拉取数据，all-to-all 的过程，跨分区聚集相同的 key。 ","date":"2022-11-03","objectID":"/study-of-all-life/:5:0","tags":["数仓"],"title":"数仓之学无止境","uri":"/study-of-all-life/"},{"categories":["学习"],"content":"数据倾斜的场景与解决方法 ","date":"2022-11-03","objectID":"/study-of-all-life/:6:0","tags":["数仓"],"title":"数仓之学无止境","uri":"/study-of-all-life/"},{"categories":["学习"],"content":"内部表和外部表 ","date":"2022-11-03","objectID":"/study-of-all-life/:7:0","tags":["数仓"],"title":"数仓之学无止境","uri":"/study-of-all-life/"},{"categories":["学习"],"content":"离线数仓最大的挑战是什么，如何克服的，是否沉淀方法论 ","date":"2022-11-03","objectID":"/study-of-all-life/:8:0","tags":["数仓"],"title":"数仓之学无止境","uri":"/study-of-all-life/"},{"categories":["学习"],"content":"实时数仓如何保障数据质量，有哪些手段和方式 参考链接：大数据SQL如何实现笛卡尔积 ↩︎ ","date":"2022-11-03","objectID":"/study-of-all-life/:9:0","tags":["数仓"],"title":"数仓之学无止境","uri":"/study-of-all-life/"},{"categories":["诗歌"],"content":"诗歌之荟萃","date":"2022-11-03","objectID":"/poetry-of-others/","tags":["张若虚"],"title":"诗歌之荟萃","uri":"/poetry-of-others/"},{"categories":["诗歌"],"content":"本文主要汇总一些诗人的著名诗篇，这些作者留下的诗篇不多，但是都是非常经典的，比如张若虚的春江花月夜。 ","date":"2022-11-03","objectID":"/poetry-of-others/:0:0","tags":["张若虚"],"title":"诗歌之荟萃","uri":"/poetry-of-others/"},{"categories":["诗歌"],"content":"张若虚 ","date":"2022-11-03","objectID":"/poetry-of-others/:1:0","tags":["张若虚"],"title":"诗歌之荟萃","uri":"/poetry-of-others/"},{"categories":["诗歌"],"content":"春江花月夜 春江潮水连海平，海上明月共潮生。 滟滟随波千万里，何处春江无月明！ 江流宛转绕芳甸，月照花林皆似霰； 空里流霜不觉飞，汀上白沙看不见。 江天一色无纤尘，皎皎空中孤月轮。 江畔何人初见月？江月何年初照人？ 人生代代无穷已，江月年年望相似。 不知江月待何人，但见长江送流水。 白云一片去悠悠，青枫浦上不胜愁。 谁家今夜扁舟子？何处相思明月楼？ 可怜楼上月裴回，应照离人妆镜台。 玉户帘中卷不去，捣衣砧上拂还来。 此时相望不相闻，愿逐月华流照君。 鸿雁长飞光不度，鱼龙潜跃水成文。 昨夜闲潭梦落花，可怜春半不还家。 江水流春去欲尽，江潭落月复西斜。 斜月沉沉藏海雾，碣石潇湘无限路。 不知乘月几人归，落月摇情满江树。 ","date":"2022-11-03","objectID":"/poetry-of-others/:1:1","tags":["张若虚"],"title":"诗歌之荟萃","uri":"/poetry-of-others/"},{"categories":["诗歌"],"content":"代答闺梦还 关塞年华早，楼台别望违。 试衫著暖气，开镜觅春晖。 燕入窥罗幕，蜂来上画衣。 情催桃李艳，心寄管弦飞。 妆洗朝相待，风花暝不归。 梦魂何处入，寂寂掩重扉。 ","date":"2022-11-03","objectID":"/poetry-of-others/:1:2","tags":["张若虚"],"title":"诗歌之荟萃","uri":"/poetry-of-others/"},{"categories":["诗歌"],"content":"诗歌之李商隐","date":"2022-11-02","objectID":"/poetry-of-lishangyin/","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"李商隐（约813年～约858年），字义山，号玉谿生，怀州河内（今河南省沁阳市）人。晚唐著名诗人，和杜牧合称“小李杜”。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:0:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"夜雨寄北 君问归期未有期，巴山夜雨涨秋池。 何当共剪西窗烛，却话巴山夜雨时。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:1:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"无题·相见时难别亦难 相见时难别亦难，东风无力百花残。 春蚕到死丝方尽，蜡炬成灰泪始干。 晓镜但愁云鬓改，夜吟应觉月光寒。 蓬山此去无多路，青鸟殷勤为探看。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:2:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"锦瑟 锦瑟无端五十弦，一弦一柱思华年。 庄生晓梦迷蝴蝶，望帝春心托杜鹃。 沧海月明珠有泪，蓝田日暖玉生烟。 此情可待成追忆？只是当时已惘然。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:3:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"嫦娥 云母屏风烛影深，长河渐落晓星沉。 嫦娥应悔偷灵药，碧海青天夜夜心。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:4:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"登乐游原 向晚意不适，驱车登古原。 夕阳无限好，只是近黄昏。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:5:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"贾生 宣室求贤访逐臣，贾生才调更无伦。 可怜夜半虚前席，不问苍生问鬼神。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:6:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"宿骆氏亭寄怀崔雍崔衮 竹坞无尘水槛清，相思迢递隔重城。 秋阴不散霜飞晚，留得枯荷听雨声。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:7:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"无题二首 昨夜星辰昨夜风，画楼西畔桂堂东。 身无彩凤双飞翼，心有灵犀一点通。 隔座送钩春酒暖，分曹射覆蜡灯红。 嗟余听鼓应官去，走马兰台类转蓬。 闻道阊门萼绿华，昔年相望抵天涯。 岂知一夜秦楼客，偷看吴王苑内花。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:8:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"安定城楼 迢递高城百尺楼，绿杨枝外尽汀洲。 贾生年少虚垂泪，王粲春来更远游。 永忆江湖归白发，欲回天地入扁舟。 不知腐鼠成滋味，猜意鹓雏竟未休。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:9:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"无题·八岁偷照镜 八岁偷照镜，长眉已能画。 十岁去踏青，芙蓉作裙衩。 十二学弹筝，银甲不曾卸。 十四藏六亲，悬知猜想犹未嫁。 十五泣春风，背面秋千下。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:10:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["诗歌"],"content":"夕阳楼 花明柳暗绕天愁，上尽重城更上楼。 欲问孤鸿向何处，不知身世自悠悠。 ","date":"2022-11-02","objectID":"/poetry-of-lishangyin/:11:0","tags":["李商隐"],"title":"诗歌之李商隐","uri":"/poetry-of-lishangyin/"},{"categories":["股市"],"content":"股市2022Q4记录","date":"2022-10-30","objectID":"/stock-of-2022q4/","tags":["投资"],"title":"股市2022Q4记录","uri":"/stock-of-2022q4/"},{"categories":["股市"],"content":" 碳中和基金的每日总结 日期 Step1 Step2 Step3 Step4 20221101 高开 拉升 下来了一半 然后冲高 20221102 低开 拉升 冲高 震荡 20221103 低开 拉升 震荡 跳水 20221106 高开 拉升 走强 横盘 20221107 平开 拉升 跳水 震荡 20221108 低开 拉升 跳水 小V 20221109 高开 下跌 跳水 横盘 20221110 低开 下跌 回落 横盘 20221111 高开 下跌 小V 横盘 20221113 低开 下跌 跳水 跳水 ","date":"2022-10-30","objectID":"/stock-of-2022q4/:0:0","tags":["投资"],"title":"股市2022Q4记录","uri":"/stock-of-2022q4/"},{"categories":["股市"],"content":"2022.10.14 医疗ETF 日丰转债 今天医疗赛道涨到天花板去了，ETF都涨了 10 个点 尾盘入了日丰转债，成本 131.479，希望下周一高开，赚一点 周天召开 20 大，希望出利好消息，下周开启起飞模式 ","date":"2022-10-30","objectID":"/stock-of-2022q4/:1:0","tags":["投资"],"title":"股市2022Q4记录","uri":"/stock-of-2022q4/"},{"categories":["股市"],"content":"2022.11.01 供销社概念 碳中和今天回血 3.36 个点 ","date":"2022-10-30","objectID":"/stock-of-2022q4/:2:0","tags":["投资"],"title":"股市2022Q4记录","uri":"/stock-of-2022q4/"},{"categories":["股市"],"content":"2022.11.02 腾讯和联通合作成立了一个公司 回暖 3000 点 ","date":"2022-10-30","objectID":"/stock-of-2022q4/:3:0","tags":["投资"],"title":"股市2022Q4记录","uri":"/stock-of-2022q4/"},{"categories":["股市"],"content":"2022.11.03 美联储加息，A股大跳水 高开低走的一天 ","date":"2022-10-30","objectID":"/stock-of-2022q4/:4:0","tags":["投资"],"title":"股市2022Q4记录","uri":"/stock-of-2022q4/"},{"categories":["股市"],"content":"2022.11.07 连续涨了好几天，希望明天大跌啊 在川投转债撸了 200 多元 ","date":"2022-10-30","objectID":"/stock-of-2022q4/:5:0","tags":["投资"],"title":"股市2022Q4记录","uri":"/stock-of-2022q4/"},{"categories":["股市"],"content":"2022.11.08 入了嘉诚转债，希望早点解套 ","date":"2022-10-30","objectID":"/stock-of-2022q4/:6:0","tags":["投资"],"title":"股市2022Q4记录","uri":"/stock-of-2022q4/"},{"categories":["股市"],"content":"2022.11.09 嘉诚转债 嘉诚转债被深套 1.9 个点，下次冲高的话还是先出一部分吧 今天行情不太好，大盘在回调 ","date":"2022-10-30","objectID":"/stock-of-2022q4/:7:0","tags":["投资"],"title":"股市2022Q4记录","uri":"/stock-of-2022q4/"},{"categories":["股市"],"content":"2022.11.14 今天惨兮兮，碳中和放量大跌，血亏 3 个点 对嘉诚无语了，早盘还冲高了，后面直接跳水 最近医药有点猛 ","date":"2022-10-30","objectID":"/stock-of-2022q4/:8:0","tags":["投资"],"title":"股市2022Q4记录","uri":"/stock-of-2022q4/"},{"categories":["学习"],"content":"学习刷题Part1","date":"2022-10-30","objectID":"/exercise-1/","tags":["牛客","刷题"],"title":"学习刷题Part1","uri":"/exercise-1/"},{"categories":["学习"],"content":"比特和波特 关于比特和波特，下面说法正确的是( )。 (A) 比特和波特施一个概念 (B) 比特是码元传输速率单位 (C) 波特是信息量单位 (D) 在某些情况下，“比特/秒”和“波特”在数值上是相等的 参考答案 ","date":"2022-10-30","objectID":"/exercise-1/:1:0","tags":["牛客","刷题"],"title":"学习刷题Part1","uri":"/exercise-1/"},{"categories":["学习"],"content":"比特和波特 正确答案: D 比特 比特率是信息量传送速率单位，即每秒传输二 进制代码位数。bit/s 波特 波特率是码元传输速率单位，他说明单位时间 传输了多少个码元 ","date":"2022-10-30","objectID":"/exercise-1/:2:0","tags":["牛客","刷题"],"title":"学习刷题Part1","uri":"/exercise-1/"},{"categories":["诗歌"],"content":"诗歌之李白篇","date":"2022-10-29","objectID":"/poetry-of-libai/","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"李白（701年2月28日—762年12月），字太白，号青莲居士，又号“谪仙人”，唐代伟大的浪漫主义诗人，被后人誉为“诗仙”，与杜甫并称为“李杜”。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:0:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"黄鹤楼送孟浩然之广陵 故人西辞黄鹤楼，烟花三月下扬州。 孤帆远影碧空尽，唯见长江天际流。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:1:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"望庐山瀑布 日照香炉生紫烟，遥看瀑布挂前川。 飞流直下三千尺，疑是银河落九天。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:2:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"静夜思 床前明月光，疑是地上霜。 举头望明月，低头思故乡。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:3:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"早发白帝城 朝辞白帝彩云间，千里江陵一日还。 两岸猿声啼不住，轻舟已过万重山。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:4:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"独坐敬亭山 众鸟高飞尽，孤云独去闲。 相看两不厌，只有敬亭山。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:5:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"赠汪伦 李白乘舟将欲行，忽闻岸上踏歌声。 桃花潭水深千尺，不及汪伦送我情。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:6:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"清平调·其一 云想衣裳花想容，春风拂槛露华浓。 若非群玉山头见，会向瑶台月下逢。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:7:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"清平调·其二 一枝秾艳露凝香，云雨巫山枉断肠。 借问汉宫谁得似，可怜飞燕倚新妆。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:8:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"清平调·其三 名花倾国两相欢，常得君王带笑看。 解释春风无限恨，沉香亭北倚阑干。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:9:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"渡荆门送别 渡远荆门外，来从楚国游。 山随平野尽，江入大荒流。 月下飞天镜，云生结海楼。 仍怜故乡水，万里送行舟。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:10:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"将进酒·君不见 君不见，黄河之水天上来，奔流到海不复回。 君不见，高堂明镜悲白发，朝如青丝暮成雪。 人生得意须尽欢，莫使金樽空对月。 天生我材必有用，千金散尽还复来。 烹羊宰牛且为乐，会须一饮三百杯。 岑夫子，丹丘生，将进酒，杯莫停。 与君歌一曲，请君为我倾耳听。 钟鼓馔玉不足贵，但愿长醉不愿醒。 古来圣贤皆寂寞，惟有饮者留其名。 陈王昔时宴平乐，斗酒十千恣欢谑。 主人何为言少钱，径须沽取对君酌。 五花马，千金裘， 呼儿将出换美酒，与尔同销万古愁。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:11:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"望天门山 天门中断楚江开，碧水东流至此回。 两岸青山相对出，孤帆一片日边来。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:12:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"春夜洛城闻笛 谁家玉笛暗飞声，散入春风满洛城。 此夜曲中闻折柳，何人不起故园情。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:13:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"闻王昌龄左迁龙标遥有此寄 杨花落尽子规啼，闻道龙标过五溪。 我寄愁心与明月，随君直到夜郎西。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:14:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"峨眉山月歌 峨眉山月半轮秋，影入平羌江水流。 夜发清溪向三峡，思君不见下渝州。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:15:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"宣州谢朓楼饯别校书叔云 弃我去者，昨日之日不可留； 乱我心者，今日之日多烦忧。 长风万里送秋雁，对此可以酣高楼。 蓬莱文章建安骨，中间小谢又清发。 俱怀逸兴壮思飞，欲上青天览明月。 抽刀断水水更流，举杯消愁愁更愁。 人生在世不称意，明朝散发弄扁舟。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:16:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"行路难·其一 金樽清酒斗十千，玉盘珍羞直万钱。 停杯投箸不能食，拔剑四顾心茫然。 欲渡黄河冰塞川，将登太行雪满山。 闲来垂钓碧溪上，忽复乘舟梦日边。 行路难，行路难，多歧路，今安在？ 长风破浪会有时，直挂云帆济沧海。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:17:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"夜宿山寺 危楼高百尺，手可摘星辰。 不敢高声语，恐惊天上人。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:18:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"秋风词 秋风清，秋月明， 落叶聚还散，寒鸦栖复惊。 相思相见知何日？此时此夜难为情！ ","date":"2022-10-29","objectID":"/poetry-of-libai/:19:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"赠孟浩然 吾爱孟夫子，风流天下闻。 红颜弃轩冕，白首卧松云。 醉月频中圣，迷花不事君。 高山安可仰，徒此揖清芬。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:20:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"},{"categories":["诗歌"],"content":"听蜀僧濬弹琴 蜀僧抱绿绮，西下峨眉峰。 为我一挥手，如听万壑松。 客心洗流水，馀响入霜钟。 不觉碧山暮，秋云暗几重。 ","date":"2022-10-29","objectID":"/poetry-of-libai/:21:0","tags":["李白"],"title":"诗歌之李白篇","uri":"/poetry-of-libai/"}]